{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mercury Composable for Node.js Good news! We have merged our enterprise extension (\"Event Script\") into the Mercury event-driven programming foundation codebase from version 4.2 onwards. It is a comprehensive toolkit to write composable applications including microservices and serverless. The specification for this technology is documented under US Patent application 18/459,307. The source code is provided as is under the Apache 2.0 license. The project is available in both Java and Node.js languages. For Java, please visit Mercury Composable for Java For Node.js, please browse Mercury Composable for Node and Composable-example January 2025 Optimized for Human Composability methodology provides a clear path from Domain Driven Design (DDD), Event Driven Architecture (EDA) to application software design and implementation, connecting product managers, domain knowledge owners, architects and engineers together to deliver high quality products. Optimized for AI Composable methodology reduces the problem space for AI code assistant because each function is self-contained, independent and I/O is immutable. In addition, the Event Script is a Domain Specific Language (DSL) that can be understood by AI agent with some fine-tuning, thus making the whole ecosystem AI friendly. Getting Started A composable application is designed in 3 steps: Describe your use case as an event flow diagram Create a configuration file to represent the event flow Write a user story for each user function To get started, please visit Chapter 1, Developer Guide and Methodology . We will illustrate the methodology with a composable application example which is available in this repo: Composable-example Conquer Complexity: Embrace Composable Design Introduction Software development is an ongoing battle against complexity. Over time, codebases can become tangled and unwieldy, hindering innovation and maintenance. This article introduces composable design patterns, a powerful approach to build applications that are modular, maintainable, and scalable. The Perils of Spaghetti Code We have all encountered it: code that resembles a plate of spaghetti \u2013 tangled dependencies, hidden logic, and a general sense of dread when approaching modifications. These codebases are difficult to test, debug, and update. Composable design patterns offer a solution. Evolution of Design Patterns Software development methodologies have evolved alongside hardware advancements. In the early days, developers prized efficiency, writing code from scratch due to limited libraries. The rise of frameworks brought structure and boilerplate code, but also introduced potential rigidity. Functional Programming and Event-Driven Architecture Functional programming, with its emphasis on pure functions and immutable data, paved the way for composable design. This approach encourages building applications as chains of well-defined functions, each with a clear input and output. Event-driven architecture complements this approach by using events to trigger functions. This loose coupling promotes modularity and scalability. The Power of Composable Design At its core, composable design emphasizes two principles: Self-Contained Functions : Each function is a well-defined unit, handling its own logic and transformations with minimal dependencies. Event Choreography : Functions communicate through events, allowing for loose coupling and independent execution. Benefits of Composable Design Enhanced Maintainability : Isolated functions are easier to understand, test, and modify. Improved Reusability : Self-contained functions can be easily reused across different parts of your application. Superior Performance : Loose coupling reduces bottlenecks and encourages asynchronous execution. Streamlined Testing : Well-defined functions facilitate unit testing and isolate potential issues. Simplified Debugging : Independent functions make it easier to pinpoint the source of errors. Technology Agnostic : You may use your preferred frameworks and tools to write composable code, allowing for easier future adaptations. Implementing Composable Design While seemingly simple, implementing composable design can involve some initial complexity. Here's a breakdown of the approach: Function Design : Each function serves a specific purpose, with clearly defined inputs and outputs. Event Communication : Functions communicate through well-defined events, avoiding direct dependencies. Choreography : An event manager, with a state machine and event flow configuration, sequences and triggers functions based on events. Conclusion Composable design patterns offer a powerful paradigm for building maintainable, scalable, and future-proof applications. By embracing the principles of self-contained functions and event-driven communication, you can conquer complexity and write code that is a joy to work with.","title":"Home"},{"location":"#mercury-composable-for-nodejs","text":"Good news! We have merged our enterprise extension (\"Event Script\") into the Mercury event-driven programming foundation codebase from version 4.2 onwards. It is a comprehensive toolkit to write composable applications including microservices and serverless. The specification for this technology is documented under US Patent application 18/459,307. The source code is provided as is under the Apache 2.0 license. The project is available in both Java and Node.js languages. For Java, please visit Mercury Composable for Java For Node.js, please browse Mercury Composable for Node and Composable-example January 2025","title":"Mercury Composable for Node.js"},{"location":"#optimized-for-human","text":"Composability methodology provides a clear path from Domain Driven Design (DDD), Event Driven Architecture (EDA) to application software design and implementation, connecting product managers, domain knowledge owners, architects and engineers together to deliver high quality products.","title":"Optimized for Human"},{"location":"#optimized-for-ai","text":"Composable methodology reduces the problem space for AI code assistant because each function is self-contained, independent and I/O is immutable. In addition, the Event Script is a Domain Specific Language (DSL) that can be understood by AI agent with some fine-tuning, thus making the whole ecosystem AI friendly.","title":"Optimized for AI"},{"location":"#getting-started","text":"A composable application is designed in 3 steps: Describe your use case as an event flow diagram Create a configuration file to represent the event flow Write a user story for each user function To get started, please visit Chapter 1, Developer Guide and Methodology . We will illustrate the methodology with a composable application example which is available in this repo: Composable-example","title":"Getting Started"},{"location":"#conquer-complexity-embrace-composable-design","text":"","title":"Conquer Complexity: Embrace Composable Design"},{"location":"#introduction","text":"Software development is an ongoing battle against complexity. Over time, codebases can become tangled and unwieldy, hindering innovation and maintenance. This article introduces composable design patterns, a powerful approach to build applications that are modular, maintainable, and scalable.","title":"Introduction"},{"location":"#the-perils-of-spaghetti-code","text":"We have all encountered it: code that resembles a plate of spaghetti \u2013 tangled dependencies, hidden logic, and a general sense of dread when approaching modifications. These codebases are difficult to test, debug, and update. Composable design patterns offer a solution.","title":"The Perils of Spaghetti Code"},{"location":"#evolution-of-design-patterns","text":"Software development methodologies have evolved alongside hardware advancements. In the early days, developers prized efficiency, writing code from scratch due to limited libraries. The rise of frameworks brought structure and boilerplate code, but also introduced potential rigidity.","title":"Evolution of Design Patterns"},{"location":"#functional-programming-and-event-driven-architecture","text":"Functional programming, with its emphasis on pure functions and immutable data, paved the way for composable design. This approach encourages building applications as chains of well-defined functions, each with a clear input and output. Event-driven architecture complements this approach by using events to trigger functions. This loose coupling promotes modularity and scalability.","title":"Functional Programming and Event-Driven Architecture"},{"location":"#the-power-of-composable-design","text":"At its core, composable design emphasizes two principles: Self-Contained Functions : Each function is a well-defined unit, handling its own logic and transformations with minimal dependencies. Event Choreography : Functions communicate through events, allowing for loose coupling and independent execution.","title":"The Power of Composable Design"},{"location":"#benefits-of-composable-design","text":"Enhanced Maintainability : Isolated functions are easier to understand, test, and modify. Improved Reusability : Self-contained functions can be easily reused across different parts of your application. Superior Performance : Loose coupling reduces bottlenecks and encourages asynchronous execution. Streamlined Testing : Well-defined functions facilitate unit testing and isolate potential issues. Simplified Debugging : Independent functions make it easier to pinpoint the source of errors. Technology Agnostic : You may use your preferred frameworks and tools to write composable code, allowing for easier future adaptations.","title":"Benefits of Composable Design"},{"location":"#implementing-composable-design","text":"While seemingly simple, implementing composable design can involve some initial complexity. Here's a breakdown of the approach: Function Design : Each function serves a specific purpose, with clearly defined inputs and outputs. Event Communication : Functions communicate through well-defined events, avoiding direct dependencies. Choreography : An event manager, with a state machine and event flow configuration, sequences and triggers functions based on events.","title":"Implementing Composable Design"},{"location":"#conclusion","text":"Composable design patterns offer a powerful paradigm for building maintainable, scalable, and future-proof applications. By embracing the principles of self-contained functions and event-driven communication, you can conquer complexity and write code that is a joy to work with.","title":"Conclusion"},{"location":"CHANGELOG/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Note : Some version numbers may be skipped to align feature set with the Java version. Version 4.3.18, 8/26/2025 Added Validation logic to filter out CR/LF for headers, cookies and session info when creating an AsyncHttpRequest from a map. For example, when using the \"AsyncHttpClient by configuration\" method, the AsyncHttpRequest is created by a map of key-values. The additional validation prevents creating headers and cookies with CR/LF accidentially. Support runtime string substitution using a model variable. StringBuilder class for easy concatenation of strings and other primitives into a single string. Removed N/A Changed Streamlined subflow routing in TaskExecutor of the event-script-engine. Changed subflow RPC call to asynchronous callback for performance optimization. Version 4.3.14, 8/19/2025 Added To reduce ambiguity in event script configuration, created an alias for model.parent. namespace as model.root. The alias is implemented using a memory reference for lowest memory and processing overheads. Removed N/A Changed CompileFlows and TaskExecutor classes are updated to support the model.root. namespace alias. Version 4.3.6, 8/13/2025 Added Support json file from classpath and local file system in input data mapping Removed N/A Changed N/A Version 4.3.5, 7/14/2025 Added N/A Removed N/A Changed Rename \"test\" folder to \"tests\" Version 4.3.4, 7/5/2025 Added util.portReady(host, port) API to test if an external server is available Minimalist Kafka Flow adapter for inbound and outbound messages Kafka emulator for unit tests \"Pseudo annotation\" for unit tests Removed N/A Changed TypeScriptClassScanner updated to support preloader generation for the test folder Version 4.3.3, 6/26/2025 Added Support file \"append\" mode in output data mapping Dynamic fork-n-join feature for parallel processing of a list of elements by multiple instances of the same task Removed N/A Changed Improve CompileFlow error message. The new error message will tell where the error comes from. Version 4.3.1, 6/24/2025 Added Worked example and template to encapsulate worker thread as a composable function Removed N/A Changed N/A Version 4.3.0, 6/21/2025 Added N/A Removed N/A Changed Comprehensive refactoring applies to both JavaScript and TypeScript in the whole project to comply with SonarQube's complexity recommendation of 15. Complex code blocks are decomposed into multiple smaller methods to improve readability. Singleton constructors are updated with the JavaScript's native double question mark to check for null and undefined value. Regression tests validated. Version 4.2.46, 6/2/2025 Added N/A Removed The \"npm run pull\" command is no longer required Changed Improved negate type mapping logic Version 4.2.45, 5/31/2025 Added \"Empty array index\" syntax in data mapping to append an element to an array in a dataset Allow text constant in data mapping to contain any characters including the mapping signature \"->\" Removed N/A Changed Bugfix for certain edge cases in detecting a non-exist key in a MultiLevelMap Version 4.2.41, 5/17/2025 Added dynamic model variable as index to address an array element (support LHS and RHS mapping) supports library resource file discovery in the configuration management system updated documentation of configuration management in Appendix-I of developer guide Removed N/A Changed N/A Version 4.2.40, 5/10/2025 Added support of \"flows\" in the \"modules.autostart\" feature \"length\" type matching feature \"dynamic model variables in for-loop\" - model variable in comparator's left and/or right hand sides Removed N/A Changed Simplified configuration management system to support using the \"test/resources\" folder to override the \"src/resources\" folder during unit tests. Version 4.2.39, 5/3/2025 Added Automatically start main composable modules and libaries: When ComposableLoader initializes, it sends start commands to configured \"main application\" or \"library\" modules. Removed N/A Changed N/A Version 4.2.38, 5/2/2025 Added N/A Removed examples subproject Changed update vitest.config.ts Version 4.2.37, 4/29/2025 Added vitest Removed jest Changed minor adjustment in unit tests for vitest syntax Version 4.2.36, 4/28/2025 Added Express static file handler Removed REST automation static file handler Custom MIME type configuration Changed N/A Version 4.2.35, 4/25/2025 Added Support \"matrix parameters\" and \"hash parameters\" in HTTP request URI in platform-core Exported the FlowExecutor class in index.ts Removed N/A Changed Bugfix for pipeline that contains only one task Rename variable 'flow' to 'template' in FlowInstance Apply path traversal avoidance using the Utility's getDecodedUri(path) method to each incoming HTTP request Version 4.2.28, 4/17/2025 Added Support 'Classpath' parameter in LHS of output data mapping Removed HTML escape characters in URI path handling in AsyncHttpClient Changed N/A Version 4.2.27, 3/31/2025 Added getError() method added in EventEnvelope to return encoded error message. This is required for distributed trace processing and proper error handling of subflows. Generic resilience handler with alternative path and backoff features Removed N/A Changed Delete file when mapping a null value from the LHS to the RHS that is defined as a file, thus allowing clearing of temporary data files in a flow. Version 4.2.23, 3/12/2025 Added N/A Removed N/A Changed For security, the parent state machine (namespace \"model.parent\") is a protected resource. It can only be shared by the primary flow and all sub-flow instances that are instantiated from it. Version 4.2.22, 3/11/2025 Added N/A Removed Dependency for \"execa\" is not required Changed All sub-flows instantiated from a primary flow can access the same parent state machine using the \"model.parent\" namespace Version 4.2.21, 3/9/2025 Added Support flow and function for external state machine Parent state machine for sub-flow Validation rules to reject access to the whole model or parent namespace Removed N/A Changed N/A Version 4.2.18, 2/21/2025 Added simple type matching feature is extended with a new string 'concat' method default REST endpoints for /api/event and actuator services Removed N/A Changed Sort REST endpoints for orderly loading Drop \"async.http.request\" RPC traces to reduce observability noise Version 4.2.17, 2/20/2025 Added Support scanning of TypeScript source file and compiled JavaScript files with class scanners (TypeScriptClassScanner and JavaScriptClassScanner) Removed N/A Changed preloader.js and developer guide updated Version 4.2.16, 2/18/2025 Added N/A Removed N/A Changed Improved class scanner and loader Filter out event metadata to propagate as HTTP response headers Version 4.2.15, 2/15/2025 Added Extract version from package.json to override info.app.version in application.yml Fork-n-Join parallel RPC request API in PostOffice Removed N/A Changed Updated Developer Guide's Chapter 5 to describe publishing mercury-composable core library to enterprise npm artifactory Version 4.2.14, 2/14/2025 Added Log application initialization time Two additional actuator endpoints (/info/routes and /env) Removed N/A Changed use different route names for various actuator services to avoid hardcode of URLs bugfix for Singleton pattern Version 4.2.13, 2/13/2025 Added Actuator REST endpoints are now configurable in rest.yaml Removed N/A Changed Update actuator services to serve REST requests directly Version 4.2.10, 2/11/2025 Added N/A Removed N/A Changed Update Actuator function and REST automation's static HTML file handler to address 2 security vulnerabilities reported by a Snyk scan. Version 4.2.9, 2/9/2025 Added uuid v4 generator in the \"simple type matching\" feature event annotation feature tagging feature in the EventEnvelope Removed The \"extra\" field has been retired from EventEnvelope Changed Filter out protected metadata from RPC response to user functions (my_route, my_instance, my_trace_id, my_trace_path) Version 4.2.7, 2/4/2025 Added N/A Removed N/A Changed endFlow method of TaskExecutor sends event to distributed trace instead of logging Version 4.2.6, 2/3/2025 Added Added log.always feature in logger Removed N/A Changed Update distributed trace function to log in \"always\" mode Version 4.2.5, 2/2/2025 Added Add 3-part syntax for Event Script's data mapping processing. Supports the following data mapping syntax: LHS -> RHS LHS -> model.variable -> RHS Removed N/A Changed Make input event immutable to PostOffice's send and request API Consistent temporary stream folder name for Java and Node.js under /tmp/composable Version 4.2.3, 1/28/2025 Added Support of negate operator of a model value in event script added to the \"simple type matching\" feature Removed N/A Changed N/A Version 4.2.2, 1/22/2025 Added N/A Removed N/A Changed For consistency with the Composable Java version, do not use pretty JSON print when log.format=text Version 4.2.1, 1/21/2025 Added N/A Removed N/A Changed reconfigure logger to json or compact format early when app starts Version 4.2.0, 1/20/2025 This is a milestone release for consistent features and behaviors between Java and Node.js versions Added Composable methodology in developer guide Event Script engine for event choreography Composable example application Removed N/A Changed N/A Version 4.1.1, 12/22/2024 Added Composable class scanner for the source folder Added \"web.component.scan\" parameter to support scanning of dependency libaries Removed N/A Changed N/A Version 4.1.0, 12/20/2024 Added AppConfig will resolve key-values from system properties and environment variables at startup Removed Eliminate preload.yaml configuration file Changed Streamlined configuration management Updated preload annotation for developer to define concurrency Version 4.0.1, 12/16/2024 Added Support parsing of multiple environment variables and base system properties for a single key-value in Config Reader. Removed N/A Changed Improved environment variable parsing logic and detection of config loops. Compatibility with Unix, Mac and Windows OS Version 4.0.0, 12/9/2024 Upgraded to sync with Mercury-Composable for the foundation event-driven and Event-over-HTTP design. Tested with Node.js version 22.12.0 (LTS). Backward compatible to version 20.18.1 (LTS). Event-over-HTTP compatibility tests conducted with Mercury-Composable version 4.0.32. Added N/A Removed N/A Changed Refactored Event-over-HTTP to use standardized HTTP headers X-Stream-Id and X-Ttl Updated OSS dependencies to latest version Configured for EsLint version 9.16.0 Version 3.0.0, 6/10/2023 Ported composable core features from Mercury 3.0 Java version Added Unit and end-to-end tests for Mercury 3.0 Node.js and for the example app project. For backward compatibility, added optional \"setupMiddleware\" method in the rest-automation module. Removed Threshold feature in REST automation Changed N/A Version 1.0.0, 5/30/2022 Added Minimal viable product Removed N/A Changed N/A","title":"Release notes"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Note : Some version numbers may be skipped to align feature set with the Java version.","title":"Changelog"},{"location":"CHANGELOG/#version-4318-8262025","text":"","title":"Version 4.3.18, 8/26/2025"},{"location":"CHANGELOG/#added","text":"Validation logic to filter out CR/LF for headers, cookies and session info when creating an AsyncHttpRequest from a map. For example, when using the \"AsyncHttpClient by configuration\" method, the AsyncHttpRequest is created by a map of key-values. The additional validation prevents creating headers and cookies with CR/LF accidentially. Support runtime string substitution using a model variable. StringBuilder class for easy concatenation of strings and other primitives into a single string.","title":"Added"},{"location":"CHANGELOG/#removed","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed","text":"Streamlined subflow routing in TaskExecutor of the event-script-engine. Changed subflow RPC call to asynchronous callback for performance optimization.","title":"Changed"},{"location":"CHANGELOG/#version-4314-8192025","text":"","title":"Version 4.3.14, 8/19/2025"},{"location":"CHANGELOG/#added_1","text":"To reduce ambiguity in event script configuration, created an alias for model.parent. namespace as model.root. The alias is implemented using a memory reference for lowest memory and processing overheads.","title":"Added"},{"location":"CHANGELOG/#removed_1","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_1","text":"CompileFlows and TaskExecutor classes are updated to support the model.root. namespace alias.","title":"Changed"},{"location":"CHANGELOG/#version-436-8132025","text":"","title":"Version 4.3.6, 8/13/2025"},{"location":"CHANGELOG/#added_2","text":"Support json file from classpath and local file system in input data mapping","title":"Added"},{"location":"CHANGELOG/#removed_2","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_2","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-435-7142025","text":"","title":"Version 4.3.5, 7/14/2025"},{"location":"CHANGELOG/#added_3","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_3","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_3","text":"Rename \"test\" folder to \"tests\"","title":"Changed"},{"location":"CHANGELOG/#version-434-752025","text":"","title":"Version 4.3.4, 7/5/2025"},{"location":"CHANGELOG/#added_4","text":"util.portReady(host, port) API to test if an external server is available Minimalist Kafka Flow adapter for inbound and outbound messages Kafka emulator for unit tests \"Pseudo annotation\" for unit tests","title":"Added"},{"location":"CHANGELOG/#removed_4","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_4","text":"TypeScriptClassScanner updated to support preloader generation for the test folder","title":"Changed"},{"location":"CHANGELOG/#version-433-6262025","text":"","title":"Version 4.3.3, 6/26/2025"},{"location":"CHANGELOG/#added_5","text":"Support file \"append\" mode in output data mapping Dynamic fork-n-join feature for parallel processing of a list of elements by multiple instances of the same task","title":"Added"},{"location":"CHANGELOG/#removed_5","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_5","text":"Improve CompileFlow error message. The new error message will tell where the error comes from.","title":"Changed"},{"location":"CHANGELOG/#version-431-6242025","text":"","title":"Version 4.3.1, 6/24/2025"},{"location":"CHANGELOG/#added_6","text":"Worked example and template to encapsulate worker thread as a composable function","title":"Added"},{"location":"CHANGELOG/#removed_6","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_6","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-430-6212025","text":"","title":"Version 4.3.0, 6/21/2025"},{"location":"CHANGELOG/#added_7","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_7","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_7","text":"Comprehensive refactoring applies to both JavaScript and TypeScript in the whole project to comply with SonarQube's complexity recommendation of 15. Complex code blocks are decomposed into multiple smaller methods to improve readability. Singleton constructors are updated with the JavaScript's native double question mark to check for null and undefined value. Regression tests validated.","title":"Changed"},{"location":"CHANGELOG/#version-4246-622025","text":"","title":"Version 4.2.46, 6/2/2025"},{"location":"CHANGELOG/#added_8","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_8","text":"The \"npm run pull\" command is no longer required","title":"Removed"},{"location":"CHANGELOG/#changed_8","text":"Improved negate type mapping logic","title":"Changed"},{"location":"CHANGELOG/#version-4245-5312025","text":"","title":"Version 4.2.45, 5/31/2025"},{"location":"CHANGELOG/#added_9","text":"\"Empty array index\" syntax in data mapping to append an element to an array in a dataset Allow text constant in data mapping to contain any characters including the mapping signature \"->\"","title":"Added"},{"location":"CHANGELOG/#removed_9","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_9","text":"Bugfix for certain edge cases in detecting a non-exist key in a MultiLevelMap","title":"Changed"},{"location":"CHANGELOG/#version-4241-5172025","text":"","title":"Version 4.2.41, 5/17/2025"},{"location":"CHANGELOG/#added_10","text":"dynamic model variable as index to address an array element (support LHS and RHS mapping) supports library resource file discovery in the configuration management system updated documentation of configuration management in Appendix-I of developer guide","title":"Added"},{"location":"CHANGELOG/#removed_10","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_10","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4240-5102025","text":"","title":"Version 4.2.40, 5/10/2025"},{"location":"CHANGELOG/#added_11","text":"support of \"flows\" in the \"modules.autostart\" feature \"length\" type matching feature \"dynamic model variables in for-loop\" - model variable in comparator's left and/or right hand sides","title":"Added"},{"location":"CHANGELOG/#removed_11","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_11","text":"Simplified configuration management system to support using the \"test/resources\" folder to override the \"src/resources\" folder during unit tests.","title":"Changed"},{"location":"CHANGELOG/#version-4239-532025","text":"","title":"Version 4.2.39, 5/3/2025"},{"location":"CHANGELOG/#added_12","text":"Automatically start main composable modules and libaries: When ComposableLoader initializes, it sends start commands to configured \"main application\" or \"library\" modules.","title":"Added"},{"location":"CHANGELOG/#removed_12","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_12","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4238-522025","text":"","title":"Version 4.2.38, 5/2/2025"},{"location":"CHANGELOG/#added_13","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_13","text":"examples subproject","title":"Removed"},{"location":"CHANGELOG/#changed_13","text":"update vitest.config.ts","title":"Changed"},{"location":"CHANGELOG/#version-4237-4292025","text":"","title":"Version 4.2.37, 4/29/2025"},{"location":"CHANGELOG/#added_14","text":"vitest","title":"Added"},{"location":"CHANGELOG/#removed_14","text":"jest","title":"Removed"},{"location":"CHANGELOG/#changed_14","text":"minor adjustment in unit tests for vitest syntax","title":"Changed"},{"location":"CHANGELOG/#version-4236-4282025","text":"","title":"Version 4.2.36, 4/28/2025"},{"location":"CHANGELOG/#added_15","text":"Express static file handler","title":"Added"},{"location":"CHANGELOG/#removed_15","text":"REST automation static file handler Custom MIME type configuration","title":"Removed"},{"location":"CHANGELOG/#changed_15","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4235-4252025","text":"","title":"Version 4.2.35, 4/25/2025"},{"location":"CHANGELOG/#added_16","text":"Support \"matrix parameters\" and \"hash parameters\" in HTTP request URI in platform-core Exported the FlowExecutor class in index.ts","title":"Added"},{"location":"CHANGELOG/#removed_16","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_16","text":"Bugfix for pipeline that contains only one task Rename variable 'flow' to 'template' in FlowInstance Apply path traversal avoidance using the Utility's getDecodedUri(path) method to each incoming HTTP request","title":"Changed"},{"location":"CHANGELOG/#version-4228-4172025","text":"","title":"Version 4.2.28, 4/17/2025"},{"location":"CHANGELOG/#added_17","text":"Support 'Classpath' parameter in LHS of output data mapping","title":"Added"},{"location":"CHANGELOG/#removed_17","text":"HTML escape characters in URI path handling in AsyncHttpClient","title":"Removed"},{"location":"CHANGELOG/#changed_17","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4227-3312025","text":"","title":"Version 4.2.27, 3/31/2025"},{"location":"CHANGELOG/#added_18","text":"getError() method added in EventEnvelope to return encoded error message. This is required for distributed trace processing and proper error handling of subflows. Generic resilience handler with alternative path and backoff features","title":"Added"},{"location":"CHANGELOG/#removed_18","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_18","text":"Delete file when mapping a null value from the LHS to the RHS that is defined as a file, thus allowing clearing of temporary data files in a flow.","title":"Changed"},{"location":"CHANGELOG/#version-4223-3122025","text":"","title":"Version 4.2.23, 3/12/2025"},{"location":"CHANGELOG/#added_19","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_19","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_19","text":"For security, the parent state machine (namespace \"model.parent\") is a protected resource. It can only be shared by the primary flow and all sub-flow instances that are instantiated from it.","title":"Changed"},{"location":"CHANGELOG/#version-4222-3112025","text":"","title":"Version 4.2.22, 3/11/2025"},{"location":"CHANGELOG/#added_20","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_20","text":"Dependency for \"execa\" is not required","title":"Removed"},{"location":"CHANGELOG/#changed_20","text":"All sub-flows instantiated from a primary flow can access the same parent state machine using the \"model.parent\" namespace","title":"Changed"},{"location":"CHANGELOG/#version-4221-392025","text":"","title":"Version 4.2.21, 3/9/2025"},{"location":"CHANGELOG/#added_21","text":"Support flow and function for external state machine Parent state machine for sub-flow Validation rules to reject access to the whole model or parent namespace","title":"Added"},{"location":"CHANGELOG/#removed_21","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_21","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4218-2212025","text":"","title":"Version 4.2.18, 2/21/2025"},{"location":"CHANGELOG/#added_22","text":"simple type matching feature is extended with a new string 'concat' method default REST endpoints for /api/event and actuator services","title":"Added"},{"location":"CHANGELOG/#removed_22","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_22","text":"Sort REST endpoints for orderly loading Drop \"async.http.request\" RPC traces to reduce observability noise","title":"Changed"},{"location":"CHANGELOG/#version-4217-2202025","text":"","title":"Version 4.2.17, 2/20/2025"},{"location":"CHANGELOG/#added_23","text":"Support scanning of TypeScript source file and compiled JavaScript files with class scanners (TypeScriptClassScanner and JavaScriptClassScanner)","title":"Added"},{"location":"CHANGELOG/#removed_23","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_23","text":"preloader.js and developer guide updated","title":"Changed"},{"location":"CHANGELOG/#version-4216-2182025","text":"","title":"Version 4.2.16, 2/18/2025"},{"location":"CHANGELOG/#added_24","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_24","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_24","text":"Improved class scanner and loader Filter out event metadata to propagate as HTTP response headers","title":"Changed"},{"location":"CHANGELOG/#version-4215-2152025","text":"","title":"Version 4.2.15, 2/15/2025"},{"location":"CHANGELOG/#added_25","text":"Extract version from package.json to override info.app.version in application.yml Fork-n-Join parallel RPC request API in PostOffice","title":"Added"},{"location":"CHANGELOG/#removed_25","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_25","text":"Updated Developer Guide's Chapter 5 to describe publishing mercury-composable core library to enterprise npm artifactory","title":"Changed"},{"location":"CHANGELOG/#version-4214-2142025","text":"","title":"Version 4.2.14, 2/14/2025"},{"location":"CHANGELOG/#added_26","text":"Log application initialization time Two additional actuator endpoints (/info/routes and /env)","title":"Added"},{"location":"CHANGELOG/#removed_26","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_26","text":"use different route names for various actuator services to avoid hardcode of URLs bugfix for Singleton pattern","title":"Changed"},{"location":"CHANGELOG/#version-4213-2132025","text":"","title":"Version 4.2.13, 2/13/2025"},{"location":"CHANGELOG/#added_27","text":"Actuator REST endpoints are now configurable in rest.yaml","title":"Added"},{"location":"CHANGELOG/#removed_27","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_27","text":"Update actuator services to serve REST requests directly","title":"Changed"},{"location":"CHANGELOG/#version-4210-2112025","text":"","title":"Version 4.2.10, 2/11/2025"},{"location":"CHANGELOG/#added_28","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_28","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_28","text":"Update Actuator function and REST automation's static HTML file handler to address 2 security vulnerabilities reported by a Snyk scan.","title":"Changed"},{"location":"CHANGELOG/#version-429-292025","text":"","title":"Version 4.2.9, 2/9/2025"},{"location":"CHANGELOG/#added_29","text":"uuid v4 generator in the \"simple type matching\" feature event annotation feature tagging feature in the EventEnvelope","title":"Added"},{"location":"CHANGELOG/#removed_29","text":"The \"extra\" field has been retired from EventEnvelope","title":"Removed"},{"location":"CHANGELOG/#changed_29","text":"Filter out protected metadata from RPC response to user functions (my_route, my_instance, my_trace_id, my_trace_path)","title":"Changed"},{"location":"CHANGELOG/#version-427-242025","text":"","title":"Version 4.2.7, 2/4/2025"},{"location":"CHANGELOG/#added_30","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_30","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_30","text":"endFlow method of TaskExecutor sends event to distributed trace instead of logging","title":"Changed"},{"location":"CHANGELOG/#version-426-232025","text":"","title":"Version 4.2.6, 2/3/2025"},{"location":"CHANGELOG/#added_31","text":"Added log.always feature in logger","title":"Added"},{"location":"CHANGELOG/#removed_31","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_31","text":"Update distributed trace function to log in \"always\" mode","title":"Changed"},{"location":"CHANGELOG/#version-425-222025","text":"","title":"Version 4.2.5, 2/2/2025"},{"location":"CHANGELOG/#added_32","text":"Add 3-part syntax for Event Script's data mapping processing. Supports the following data mapping syntax: LHS -> RHS LHS -> model.variable -> RHS","title":"Added"},{"location":"CHANGELOG/#removed_32","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_32","text":"Make input event immutable to PostOffice's send and request API Consistent temporary stream folder name for Java and Node.js under /tmp/composable","title":"Changed"},{"location":"CHANGELOG/#version-423-1282025","text":"","title":"Version 4.2.3, 1/28/2025"},{"location":"CHANGELOG/#added_33","text":"Support of negate operator of a model value in event script added to the \"simple type matching\" feature","title":"Added"},{"location":"CHANGELOG/#removed_33","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_33","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-422-1222025","text":"","title":"Version 4.2.2, 1/22/2025"},{"location":"CHANGELOG/#added_34","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_34","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_34","text":"For consistency with the Composable Java version, do not use pretty JSON print when log.format=text","title":"Changed"},{"location":"CHANGELOG/#version-421-1212025","text":"","title":"Version 4.2.1, 1/21/2025"},{"location":"CHANGELOG/#added_35","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_35","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_35","text":"reconfigure logger to json or compact format early when app starts","title":"Changed"},{"location":"CHANGELOG/#version-420-1202025","text":"This is a milestone release for consistent features and behaviors between Java and Node.js versions","title":"Version 4.2.0, 1/20/2025"},{"location":"CHANGELOG/#added_36","text":"Composable methodology in developer guide Event Script engine for event choreography Composable example application","title":"Added"},{"location":"CHANGELOG/#removed_36","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_36","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-411-12222024","text":"","title":"Version 4.1.1, 12/22/2024"},{"location":"CHANGELOG/#added_37","text":"Composable class scanner for the source folder Added \"web.component.scan\" parameter to support scanning of dependency libaries","title":"Added"},{"location":"CHANGELOG/#removed_37","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_37","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-410-12202024","text":"","title":"Version 4.1.0, 12/20/2024"},{"location":"CHANGELOG/#added_38","text":"AppConfig will resolve key-values from system properties and environment variables at startup","title":"Added"},{"location":"CHANGELOG/#removed_38","text":"Eliminate preload.yaml configuration file","title":"Removed"},{"location":"CHANGELOG/#changed_38","text":"Streamlined configuration management Updated preload annotation for developer to define concurrency","title":"Changed"},{"location":"CHANGELOG/#version-401-12162024","text":"","title":"Version 4.0.1, 12/16/2024"},{"location":"CHANGELOG/#added_39","text":"Support parsing of multiple environment variables and base system properties for a single key-value in Config Reader.","title":"Added"},{"location":"CHANGELOG/#removed_39","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_39","text":"Improved environment variable parsing logic and detection of config loops. Compatibility with Unix, Mac and Windows OS","title":"Changed"},{"location":"CHANGELOG/#version-400-1292024","text":"Upgraded to sync with Mercury-Composable for the foundation event-driven and Event-over-HTTP design. Tested with Node.js version 22.12.0 (LTS). Backward compatible to version 20.18.1 (LTS). Event-over-HTTP compatibility tests conducted with Mercury-Composable version 4.0.32.","title":"Version 4.0.0, 12/9/2024"},{"location":"CHANGELOG/#added_40","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_40","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_40","text":"Refactored Event-over-HTTP to use standardized HTTP headers X-Stream-Id and X-Ttl Updated OSS dependencies to latest version Configured for EsLint version 9.16.0","title":"Changed"},{"location":"CHANGELOG/#version-300-6102023","text":"Ported composable core features from Mercury 3.0 Java version","title":"Version 3.0.0, 6/10/2023"},{"location":"CHANGELOG/#added_41","text":"Unit and end-to-end tests for Mercury 3.0 Node.js and for the example app project. For backward compatibility, added optional \"setupMiddleware\" method in the rest-automation module.","title":"Added"},{"location":"CHANGELOG/#removed_41","text":"Threshold feature in REST automation","title":"Removed"},{"location":"CHANGELOG/#changed_41","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-100-5302022","text":"","title":"Version 1.0.0, 5/30/2022"},{"location":"CHANGELOG/#added_42","text":"Minimal viable product","title":"Added"},{"location":"CHANGELOG/#removed_42","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_42","text":"N/A","title":"Changed"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting Kevin Bader (the current project maintainer). All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting Kevin Bader (the current project maintainer). All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contributing to the Mercury framework Thanks for taking the time to contribute! The following is a set of guidelines for contributing to Mercury and its packages, which are hosted in the Accenture Organization on GitHub. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request. Code of Conduct This project and everyone participating in it is governed by our Code of Conduct . By participating, you are expected to uphold this code. Please report unacceptable behavior to Kevin Bader, who is the current project maintainer. What should I know before I get started? We follow the standard GitHub workflow . Before submitting a Pull Request: Please write tests. Make sure you run all tests and check for warnings. Think about whether it makes sense to document the change in some way. For smaller, internal changes, inline documentation might be sufficient, while more visible ones might warrant a change to the developer's guide or the README . Update CHANGELOG.md file with your current change in form of [Type of change e.g. Config, Kafka, .etc] with a short description of what it is all about and a link to issue or pull request, and choose a suitable section (i.e., changed, added, fixed, removed, deprecated). Design Decisions When we make a significant decision in how to write code, or how to maintain the project and what we can or cannot support, we will document it using Architecture Decision Records (ADR) . Take a look at the design notes for existing ADRs. If you have a question around how we do things, check to see if it is documented there. If it is not documented there, please ask us - chances are you're not the only one wondering. Of course, also feel free to challenge the decisions by starting a discussion on the mailing list.","title":"Contribution"},{"location":"CONTRIBUTING/#contributing-to-the-mercury-framework","text":"Thanks for taking the time to contribute! The following is a set of guidelines for contributing to Mercury and its packages, which are hosted in the Accenture Organization on GitHub. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.","title":"Contributing to the Mercury framework"},{"location":"CONTRIBUTING/#code-of-conduct","text":"This project and everyone participating in it is governed by our Code of Conduct . By participating, you are expected to uphold this code. Please report unacceptable behavior to Kevin Bader, who is the current project maintainer.","title":"Code of Conduct"},{"location":"CONTRIBUTING/#what-should-i-know-before-i-get-started","text":"We follow the standard GitHub workflow . Before submitting a Pull Request: Please write tests. Make sure you run all tests and check for warnings. Think about whether it makes sense to document the change in some way. For smaller, internal changes, inline documentation might be sufficient, while more visible ones might warrant a change to the developer's guide or the README . Update CHANGELOG.md file with your current change in form of [Type of change e.g. Config, Kafka, .etc] with a short description of what it is all about and a link to issue or pull request, and choose a suitable section (i.e., changed, added, fixed, removed, deprecated).","title":"What should I know before I get started?"},{"location":"CONTRIBUTING/#design-decisions","text":"When we make a significant decision in how to write code, or how to maintain the project and what we can or cannot support, we will document it using Architecture Decision Records (ADR) . Take a look at the design notes for existing ADRs. If you have a question around how we do things, check to see if it is documented there. If it is not documented there, please ask us - chances are you're not the only one wondering. Of course, also feel free to challenge the decisions by starting a discussion on the mailing list.","title":"Design Decisions"},{"location":"INCLUSIVITY/","text":"TECHNOLOGY INCLUSIVE LANGUAGE GUIDEBOOK As an organization, Accenture believes in building an inclusive workplace and contributing to a world where equality thrives. Certain terms or expressions can unintentionally harm, perpetuate damaging stereotypes, and insult people. Inclusive language avoids bias, slang terms, and word choices which express derision of groups of people based on race, gender, sexuality, or socioeconomic status. The Accenture North America Technology team created this guidebook to provide Accenture employees with a view into inclusive language and guidance for working to avoid its use\u2014helping to ensure that we communicate with respect, dignity and fairness. How to use this guide? Accenture has over 514,000 employees from diverse backgrounds, who perform consulting and delivery work for an equally diverse set of clients and partners. When communicating with your colleagues and representing Accenture, consider the connotation, however unintended, of certain terms in your written and verbal communication. The guidelines are intended to help you recognize non-inclusive words and understand potential meanings that these words might convey. Our goal with these recommendations is not to require you to use specific words, but to ask you to take a moment to consider how your audience may be affected by the language you choose. Inclusive Categories Non-inclusive term Replacement Explanation Race, Ethnicity & National Origin master primary client source leader Using the terms \u201cmaster/slave\u201d in this context inappropriately normalizes and minimizes the very large magnitude that slavery and its effects have had in our history. slave secondary replica follower blacklist deny list block list The term \u201cblacklist\u201d was first used in the early 1600s to describe a list of those who were under suspicion and thus not to be trusted, whereas \u201cwhitelist\u201d referred to those considered acceptable. Accenture does not want to promote the association of \u201cblack\u201d and negative, nor the connotation of \u201cwhite\u201d being the inverse, or positive. whitelist allow list approved list native original core feature Referring to \u201cnative\u201d vs \u201cnon-native\u201d to describe technology platforms carries overtones of minimizing the impact of colonialism on native people, and thus minimizes the negative associations the terminology has in the latter context. non-native non-original non-core feature Gender & Sexuality man-hours work-hours business-hours When people read the words \u2018man\u2019 or \u2018he,\u2019 people often picture males only. Usage of the male terminology subtly suggests that only males can perform certain work or hold certain jobs. Gender-neutral terms include the whole audience, and thus using terms such as \u201cbusiness executive\u201d instead of \u201cbusinessman,\u201d or informally, \u201cfolks\u201d instead of \u201cguys\u201d is preferable because it is inclusive. man-days work-days business-days Ability Status & (Dis)abilities sanity check insanity check confidence check quality check rationality check Using the \u201cHuman Engagement, People First\u2019 approach, putting people - all people - at the center is important. Denoting ability status in the context of inferior or problematic work implies that people with mental illnesses are inferior, wrong, or incorrect. dummy variables indicator variables Violence STONITH, kill, hit conclude cease discontinue Using the \u201cHuman Engagement, People First\u2019 approach, putting people - all people - at the center is important. Denoting ability status in the context of inferior or problematic work implies that people with mental illnesses are inferior, wrong, or incorrect. one throat to choke single point of contact primary contact This guidebook is a living document and will be updated as terminology evolves. We encourage our users to provide feedback on the effectiveness of this document and we welcome additional suggestions. Contact us at Technology_ProjectElevate@accenture.com .","title":"Inclusivity"},{"location":"INCLUSIVITY/#technology-inclusive-language-guidebook","text":"As an organization, Accenture believes in building an inclusive workplace and contributing to a world where equality thrives. Certain terms or expressions can unintentionally harm, perpetuate damaging stereotypes, and insult people. Inclusive language avoids bias, slang terms, and word choices which express derision of groups of people based on race, gender, sexuality, or socioeconomic status. The Accenture North America Technology team created this guidebook to provide Accenture employees with a view into inclusive language and guidance for working to avoid its use\u2014helping to ensure that we communicate with respect, dignity and fairness. How to use this guide? Accenture has over 514,000 employees from diverse backgrounds, who perform consulting and delivery work for an equally diverse set of clients and partners. When communicating with your colleagues and representing Accenture, consider the connotation, however unintended, of certain terms in your written and verbal communication. The guidelines are intended to help you recognize non-inclusive words and understand potential meanings that these words might convey. Our goal with these recommendations is not to require you to use specific words, but to ask you to take a moment to consider how your audience may be affected by the language you choose. Inclusive Categories Non-inclusive term Replacement Explanation Race, Ethnicity & National Origin master primary client source leader Using the terms \u201cmaster/slave\u201d in this context inappropriately normalizes and minimizes the very large magnitude that slavery and its effects have had in our history. slave secondary replica follower blacklist deny list block list The term \u201cblacklist\u201d was first used in the early 1600s to describe a list of those who were under suspicion and thus not to be trusted, whereas \u201cwhitelist\u201d referred to those considered acceptable. Accenture does not want to promote the association of \u201cblack\u201d and negative, nor the connotation of \u201cwhite\u201d being the inverse, or positive. whitelist allow list approved list native original core feature Referring to \u201cnative\u201d vs \u201cnon-native\u201d to describe technology platforms carries overtones of minimizing the impact of colonialism on native people, and thus minimizes the negative associations the terminology has in the latter context. non-native non-original non-core feature Gender & Sexuality man-hours work-hours business-hours When people read the words \u2018man\u2019 or \u2018he,\u2019 people often picture males only. Usage of the male terminology subtly suggests that only males can perform certain work or hold certain jobs. Gender-neutral terms include the whole audience, and thus using terms such as \u201cbusiness executive\u201d instead of \u201cbusinessman,\u201d or informally, \u201cfolks\u201d instead of \u201cguys\u201d is preferable because it is inclusive. man-days work-days business-days Ability Status & (Dis)abilities sanity check insanity check confidence check quality check rationality check Using the \u201cHuman Engagement, People First\u2019 approach, putting people - all people - at the center is important. Denoting ability status in the context of inferior or problematic work implies that people with mental illnesses are inferior, wrong, or incorrect. dummy variables indicator variables Violence STONITH, kill, hit conclude cease discontinue Using the \u201cHuman Engagement, People First\u2019 approach, putting people - all people - at the center is important. Denoting ability status in the context of inferior or problematic work implies that people with mental illnesses are inferior, wrong, or incorrect. one throat to choke single point of contact primary contact This guidebook is a living document and will be updated as terminology evolves. We encourage our users to provide feedback on the effectiveness of this document and we welcome additional suggestions. Contact us at Technology_ProjectElevate@accenture.com .","title":"TECHNOLOGY INCLUSIVE LANGUAGE GUIDEBOOK"},{"location":"arch-decisions/DESIGN-NOTES/","text":"Design notes Composable application Modern applications are sophisticated. Navigating multiple layers of application logic, utilities and libraries make code complex and difficult to read. To make code readable and module, we advocate the composable application design pattern. Each function in a composable application is a building block of functionality. It is self-contained, stateless and independent of the rest of the application. You can write code using the first principle of \"input-process-output\". Fully event driven Mercury is both a development methodology and a toolkit. It articulates the use of events between functions instead of tight coupling using direct method calls. In Node.js, this is particular important because it ensures that each function yields to the event loop without blocking the rest of the application, resulting in higher performance and throughout. Reactive design The system encapsulates the standard Node.js EventEmitter with a \"manager and worker\" pattern. Each worker of a function will process incoming event orderly. This allows the developer the flexibility to implement singleton pattern and parallel processing easily. Native Node.js stream and ObjectStream I/O It integrates natively with the standard Node.js stream library. For higher digital decoupling, the system provides a set of ObjectStream I/O API so that producer can write to a stream before a consumer is ready. To reduce memory footprint, the system uses the temporary local file system at \"/tmp/composable/node/temp-streams\" to hold data blocks of a stream. The temporary data blocks are cleared automatically when a stream is read or closed. Configuration management The system supports a base configuration (application.yml) and the developer can use additional configuration files with the \"ConfigReader\" API. It follows a structured configuration approach similar to Java's Spring Boot. Compatibility with browsers The core engine does not have dependency on the local file system. This provides a path to support Composable design in a browser application in future iterations.","title":"Design notes"},{"location":"arch-decisions/DESIGN-NOTES/#design-notes","text":"","title":"Design notes"},{"location":"arch-decisions/DESIGN-NOTES/#composable-application","text":"Modern applications are sophisticated. Navigating multiple layers of application logic, utilities and libraries make code complex and difficult to read. To make code readable and module, we advocate the composable application design pattern. Each function in a composable application is a building block of functionality. It is self-contained, stateless and independent of the rest of the application. You can write code using the first principle of \"input-process-output\".","title":"Composable application"},{"location":"arch-decisions/DESIGN-NOTES/#fully-event-driven","text":"Mercury is both a development methodology and a toolkit. It articulates the use of events between functions instead of tight coupling using direct method calls. In Node.js, this is particular important because it ensures that each function yields to the event loop without blocking the rest of the application, resulting in higher performance and throughout.","title":"Fully event driven"},{"location":"arch-decisions/DESIGN-NOTES/#reactive-design","text":"The system encapsulates the standard Node.js EventEmitter with a \"manager and worker\" pattern. Each worker of a function will process incoming event orderly. This allows the developer the flexibility to implement singleton pattern and parallel processing easily.","title":"Reactive design"},{"location":"arch-decisions/DESIGN-NOTES/#native-nodejs-stream-and-objectstream-io","text":"It integrates natively with the standard Node.js stream library. For higher digital decoupling, the system provides a set of ObjectStream I/O API so that producer can write to a stream before a consumer is ready. To reduce memory footprint, the system uses the temporary local file system at \"/tmp/composable/node/temp-streams\" to hold data blocks of a stream. The temporary data blocks are cleared automatically when a stream is read or closed.","title":"Native Node.js stream and ObjectStream I/O"},{"location":"arch-decisions/DESIGN-NOTES/#configuration-management","text":"The system supports a base configuration (application.yml) and the developer can use additional configuration files with the \"ConfigReader\" API. It follows a structured configuration approach similar to Java's Spring Boot.","title":"Configuration management"},{"location":"arch-decisions/DESIGN-NOTES/#compatibility-with-browsers","text":"The core engine does not have dependency on the local file system. This provides a path to support Composable design in a browser application in future iterations.","title":"Compatibility with browsers"},{"location":"guides/APPENDIX-I/","text":"Application configuration The application base configuration can be defined in the application.yml file. The following parameters are reserved by the system. You can add your application parameters in the main application configuration file ( application.yml ) or apply additional configuration files using the ConfigReader API. Key Value (example) Required application.name Application name Yes info.app.version major.minor.build (e.g. 1.0.0) Yes info.app.description Something about your application Yes server.port e.g. 8083 Yes static.html.folder e.g. classpath:/public or file:/tmp/html Yes web.component.scan a comma separated list of composable library Yes yaml.rest.automation Default value is classpath:/rest.yaml Optional yaml.custom.content.types Optional config file Optional custom.content.types List of content type mappings Optional log.format text, compact or json. default=text Optional log.level Default: 'info' Optional health.dependencies e.g. 'database.health' Optional modules.autostart list of composable functions to start Optional max.model.array.size max size of a dynamic model variable as index (Default 1000) Optional Configuration management The application.yml config file must be placed in the \"src/resources\" folder in your project. For unit test, it can be placed in the \"tests/resources\" folder. The configuration management system will discover configuration files with the following order of precedence: dist/resources tests/resources src/resources (library-1)/dist/resources (library-2)/dist/resources (library-n)/dist/resources For example, if a config file is not found in the tests/resources folder in a unit test, it will search the \"src/resources\" folder. If still not found, it will search the list of libraries for their resources folders. The resource file path must be prefixed with the keyword classpath: . This discovery mechanism applies to all types of files including config files. If your application needs to use a resource file, you can programmatically look up the file like this: const filePath = config.resolveResourceFilePath('classpath:/private/interesting.txt'); // filePath will be resolved as a fully qualified file path // if not found, a null value will be returned Note : While the search order for libraries is defined by the web.component.scan parameter, it is always a good idea to use unique filenames for resource files in a library to avoid unintended configuration errors. Static HTML contents You can place static HTML files (e.g. the HTML bundle for a UI program) in the \"resources/public\" folder or in the local file system using the \"static.html.folder\" parameter. Static HTML contents are served by the built-in Express static file handler. Custom content types If you use custom content types in your application, you may add the following section in the application.yml configuration file. For example, custom.content.types: - 'application/vnd.my.org-v2.0+json -> application/json' - 'application/vnd.my.org-v2.1+xml -> application/xml' In the \"custom.content.types\" section, you can configure a list of content-type mappings. The left-hand-side is the custom content-type and the right-hand-side is a standard content-type. The content-type mapping tells the system to treat the custom content type as if it is the standard content type. In the above example, the HTTP payload with the custom content type \"application/vnd.my.org-v2.0+json\" is treated as a regular JSON content. If you want to put the custom content types in a separate configuration file, please put them in a file named custom-content-types.yml under the resources folder and add this entry in application.yml: yaml.custom.content.types: 'classpath:/custom-content-types.yml' Transient data store The system uses a temp folder in \"/tmp/composable/node/temp-streams\" to hold temporary data blocks for streaming I/O. Reserved route names The following route names are reserved by the system. Route Purpose Modules distributed.tracing Distributed tracing logger platform-core temporary.inbox Event listener for RPC platform-core event.api.service Event API handler platform-core object.stream.manager Object stream event handler platform-core async.http.request HTTP request event handler REST automation async.http.response HTTP response event handler REST automation info.actuator.service admin endpoint for /info REST automation routes.actuator.service admin endpoint for /info/routes REST automation env.actuator.service admin endpoint for /env REST automation health.actuator.service admin endpoint for /health REST automation liveness.actuator.service admin endpoint for /livenessprobe REST automation rest.automation.housekeeper REST automation housekeeper REST automation event.script.manager Instantiate new event flow instance event-script task.executor Perform event choreography event-script http.flow.adapter Built-in flow adapter event-script no.op no-operation placeholder function event-script Reserved HTTP header names Header Purpose X-Stream-Id Temporal route name for streaming content X-TTL Time to live in milliseconds for a streaming content X-Async This header, if set to true, indicates it is a drop-n-forget request X-Trace-Id This allows the system to propagate trace ID Chapter-8 Home Appendix-II Custom Flow Adapter Table of Contents Async HTTP client","title":"Appendix-I"},{"location":"guides/APPENDIX-I/#application-configuration","text":"The application base configuration can be defined in the application.yml file. The following parameters are reserved by the system. You can add your application parameters in the main application configuration file ( application.yml ) or apply additional configuration files using the ConfigReader API. Key Value (example) Required application.name Application name Yes info.app.version major.minor.build (e.g. 1.0.0) Yes info.app.description Something about your application Yes server.port e.g. 8083 Yes static.html.folder e.g. classpath:/public or file:/tmp/html Yes web.component.scan a comma separated list of composable library Yes yaml.rest.automation Default value is classpath:/rest.yaml Optional yaml.custom.content.types Optional config file Optional custom.content.types List of content type mappings Optional log.format text, compact or json. default=text Optional log.level Default: 'info' Optional health.dependencies e.g. 'database.health' Optional modules.autostart list of composable functions to start Optional max.model.array.size max size of a dynamic model variable as index (Default 1000) Optional","title":"Application configuration"},{"location":"guides/APPENDIX-I/#configuration-management","text":"The application.yml config file must be placed in the \"src/resources\" folder in your project. For unit test, it can be placed in the \"tests/resources\" folder. The configuration management system will discover configuration files with the following order of precedence: dist/resources tests/resources src/resources (library-1)/dist/resources (library-2)/dist/resources (library-n)/dist/resources For example, if a config file is not found in the tests/resources folder in a unit test, it will search the \"src/resources\" folder. If still not found, it will search the list of libraries for their resources folders. The resource file path must be prefixed with the keyword classpath: . This discovery mechanism applies to all types of files including config files. If your application needs to use a resource file, you can programmatically look up the file like this: const filePath = config.resolveResourceFilePath('classpath:/private/interesting.txt'); // filePath will be resolved as a fully qualified file path // if not found, a null value will be returned Note : While the search order for libraries is defined by the web.component.scan parameter, it is always a good idea to use unique filenames for resource files in a library to avoid unintended configuration errors.","title":"Configuration management"},{"location":"guides/APPENDIX-I/#static-html-contents","text":"You can place static HTML files (e.g. the HTML bundle for a UI program) in the \"resources/public\" folder or in the local file system using the \"static.html.folder\" parameter. Static HTML contents are served by the built-in Express static file handler.","title":"Static HTML contents"},{"location":"guides/APPENDIX-I/#custom-content-types","text":"If you use custom content types in your application, you may add the following section in the application.yml configuration file. For example, custom.content.types: - 'application/vnd.my.org-v2.0+json -> application/json' - 'application/vnd.my.org-v2.1+xml -> application/xml' In the \"custom.content.types\" section, you can configure a list of content-type mappings. The left-hand-side is the custom content-type and the right-hand-side is a standard content-type. The content-type mapping tells the system to treat the custom content type as if it is the standard content type. In the above example, the HTTP payload with the custom content type \"application/vnd.my.org-v2.0+json\" is treated as a regular JSON content. If you want to put the custom content types in a separate configuration file, please put them in a file named custom-content-types.yml under the resources folder and add this entry in application.yml: yaml.custom.content.types: 'classpath:/custom-content-types.yml'","title":"Custom content types"},{"location":"guides/APPENDIX-I/#transient-data-store","text":"The system uses a temp folder in \"/tmp/composable/node/temp-streams\" to hold temporary data blocks for streaming I/O.","title":"Transient data store"},{"location":"guides/APPENDIX-I/#reserved-route-names","text":"The following route names are reserved by the system. Route Purpose Modules distributed.tracing Distributed tracing logger platform-core temporary.inbox Event listener for RPC platform-core event.api.service Event API handler platform-core object.stream.manager Object stream event handler platform-core async.http.request HTTP request event handler REST automation async.http.response HTTP response event handler REST automation info.actuator.service admin endpoint for /info REST automation routes.actuator.service admin endpoint for /info/routes REST automation env.actuator.service admin endpoint for /env REST automation health.actuator.service admin endpoint for /health REST automation liveness.actuator.service admin endpoint for /livenessprobe REST automation rest.automation.housekeeper REST automation housekeeper REST automation event.script.manager Instantiate new event flow instance event-script task.executor Perform event choreography event-script http.flow.adapter Built-in flow adapter event-script no.op no-operation placeholder function event-script","title":"Reserved route names"},{"location":"guides/APPENDIX-I/#reserved-http-header-names","text":"Header Purpose X-Stream-Id Temporal route name for streaming content X-TTL Time to live in milliseconds for a streaming content X-Async This header, if set to true, indicates it is a drop-n-forget request X-Trace-Id This allows the system to propagate trace ID Chapter-8 Home Appendix-II Custom Flow Adapter Table of Contents Async HTTP client","title":"Reserved HTTP header names"},{"location":"guides/APPENDIX-II/","text":"Actuators and HTTP client Actuator endpoints The following are actuator endpoints: GET /info GET /info/routes GET /env GET /health GET /livenessprobe Endpoint Purpose /info Describe the application /info/routes List all private and public function route names /env Show selected environment variables and application parameters /health Application health check endpoint /livenessprobe Check if application is running normally System provided REST endpoints When REST automation is turned on, the following essential REST endpoints will be provided if they are not configured in rest.yaml. The \"POST /api/event\" is used for Event-Over-HTTP protocol and the others are actuator endpoints. To override the default parameters such as timeout, tracing and authentication, you can configure them in rest.yaml. rest: - service: \"event.api.service\" methods: ['POST'] url: \"/api/event\" timeout: 60s tracing: true - service: \"info.actuator.service\" methods: ['GET'] url: \"/info\" timeout: 10s - service: \"routes.actuator.service\" methods: ['GET'] url: \"/info/routes\" timeout: 10s - service: \"health.actuator.service\" methods: ['GET'] url: \"/health\" timeout: 10s - service: \"liveness.actuator.service\" methods: ['GET'] url: \"/livenessprobe\" timeout: 10s - service: \"env.actuator.service\" methods: ['GET'] url: \"/env\" timeout: 10s Custom health services You can extend the \"/health\" endpoint by implementing a composable functions to be added to the \"health check\" dependencies. health.dependencies=database.health, cache.health Your custom health service must respond to the following requests: Info request (type=info) - it should return a map that includes service name and href (protocol, hostname and port) Health check (type=health) - it should return a text string of the health check. e.g. read/write test result. It can throw AppException with status code and error message if health check fails. Note : The \"href\" entry in the health service's response should tell the operator about the target URL if the dependency connects to a cloud platform service such as Kafka, Redis, etc. A sample health service is available in the health-check.ts class of the hello world project as follows: import { preload, Composable, EventEnvelope, AppException } from 'mercury-composable'; const TYPE = 'type'; const INFO = 'info'; const HEALTH = 'health'; export class DemoHealthCheck implements Composable { @preload('demo.health') initialize(): Composable { return this; } // Your service should be declared as an async function with input as EventEnvelope async handleEvent(evt: EventEnvelope) { const command = evt.getHeader(TYPE); if (command == INFO) { return {'service': 'demo.service', 'href': 'http://127.0.0.1'}; } if (command == HEALTH) { // this is a dummy health check return {'status': 'demo.service is running fine'}; } throw new AppException(400, 'Request type must be info or health'); } } AsyncHttpClient API The \"async.http.request\" function can be used as a non-blocking HTTP client. To make an HTTP request to an external REST endpoint, you can create an HTTP request object using the AsyncHttpRequest class and make an async RPC call to the \"async.http.request\" function like this: const po = new PostOffice(evt.getHeaders()); const req = new AsyncHttpRequest(); req.setMethod(\"GET\"); req.setHeader(\"accept\", \"application/json\"); req.setUrl(\"/api/hello/world?hello world=abc\"); req.setQueryParameter(\"x1\", \"y\"); const list = new Array<string>(); list.push(\"a\"); list.push(\"b\"); req.setQueryParameter(\"x2\", list); req.setTargetHost(\"http://127.0.0.1:8083\"); const event = new EventEnvelope().setTo(\"async.http.request\").setBody(req); const result = po.request(event, 5000); // the result is an EventEnvelope Send HTTP request body for HTTP PUT, POST and PATCH methods For most cases, you can just set a JSON object into the request body and specify content-type as JSON. Example code may look like this: const req = new AsyncHttpRequest(); req.setMethod(\"POST\"); req.setHeader(\"accept\", \"application/json\"); req.setHeader(\"content-type\", \"application/json\"); req.setUrl(\"/api/book\"); req.setTargetHost(\"https://service_provider_host\"); req.setBody(jsonKeyValues); Send HTTP request body as a stream For larger payload, you may use the streaming method. See sample code below: const stream = new ObjectStreamIO(timeoutInSeconds); const out = stream.getOutputStream(); out.write(blockOne); out.write(blockTwo); // closing the output stream would send a EOF signal to the stream out.close(); // tell the HTTP client to read the input stream req.setStreamRoute(stream.getInputStreamId()); The AsyncHttpClient service (route name async.http.request ) uses native Node.js streams to integrate with the underlying Axios HTTP client. It uses the temporary local file system (folder /tmp/composable/node/temp-streams ) to reduce memory footprint. This makes the producer and consumer of a stream asynchronous. i.e. The producer can write data blocks into a stream before a consumer is available. Read HTTP response body stream If content length is not given, the response body will be received as a stream. Your application should check if the HTTP response header \"stream\" exists. Its value is the input \"stream ID\". Sample code to read a stream may look like this: static async downloadFile(streamId: string, filename: string) { let n = 0; let len = 0; const stream = new ObjectStreamReader(streamId, 5000); while (true) { try { const block = await stream.read(); if (block) { n++; if (block instanceof Buffer) { len += block.length; log.info(`Received ${filename}, block-${n} - ${block.length} bytes`) } } else { log.info(\"EOF reached\"); break; } } catch (e) { const status = e instanceof AppException? e.getStatus() : 500; log.error(`Exception - rc=${status}, message=${e.message}`); break; } } return len; } Content length for HTTP request IMPORTANT : Do not set the \"content-length\" HTTP header because the system will automatically compute the correct content-length for small payload. For large payload, it will use the chunking method. Application log format The system supports 3 types of log formats. You can set \"log.format\" parameter in application.yml to change the log format or override it at runtime using the -D argument. e.g. node myapp.js -Dlog.format=json Format Description text this is the default log format json application log will be printed in JSON format with line feed and indentation compact JSON format without line feed and indentation text and json formats are for human readers and compact format is designed for log analytics system. Externalize the application.yml If you want to externalize the application.yml configuration file, please keep a default application.yml in the \"src/resources\" folder. You can use command line to ask the system to reload a new base configuration file like this: node myapp.js -Dlog.format=json -C/tmp/config/application.yml Note : The -C command argument should point to a fully qualified file path. Use relative path if you know exactly the resolved path in a deployed container. You can have multiple \"-D\" parameters but you can only configure a single \"-C\" argument. Appendix-I Home Application config Table of Contents","title":"Appendix-II"},{"location":"guides/APPENDIX-II/#actuators-and-http-client","text":"","title":"Actuators and HTTP client"},{"location":"guides/APPENDIX-II/#actuator-endpoints","text":"The following are actuator endpoints: GET /info GET /info/routes GET /env GET /health GET /livenessprobe Endpoint Purpose /info Describe the application /info/routes List all private and public function route names /env Show selected environment variables and application parameters /health Application health check endpoint /livenessprobe Check if application is running normally","title":"Actuator endpoints"},{"location":"guides/APPENDIX-II/#system-provided-rest-endpoints","text":"When REST automation is turned on, the following essential REST endpoints will be provided if they are not configured in rest.yaml. The \"POST /api/event\" is used for Event-Over-HTTP protocol and the others are actuator endpoints. To override the default parameters such as timeout, tracing and authentication, you can configure them in rest.yaml. rest: - service: \"event.api.service\" methods: ['POST'] url: \"/api/event\" timeout: 60s tracing: true - service: \"info.actuator.service\" methods: ['GET'] url: \"/info\" timeout: 10s - service: \"routes.actuator.service\" methods: ['GET'] url: \"/info/routes\" timeout: 10s - service: \"health.actuator.service\" methods: ['GET'] url: \"/health\" timeout: 10s - service: \"liveness.actuator.service\" methods: ['GET'] url: \"/livenessprobe\" timeout: 10s - service: \"env.actuator.service\" methods: ['GET'] url: \"/env\" timeout: 10s","title":"System provided REST endpoints"},{"location":"guides/APPENDIX-II/#custom-health-services","text":"You can extend the \"/health\" endpoint by implementing a composable functions to be added to the \"health check\" dependencies. health.dependencies=database.health, cache.health Your custom health service must respond to the following requests: Info request (type=info) - it should return a map that includes service name and href (protocol, hostname and port) Health check (type=health) - it should return a text string of the health check. e.g. read/write test result. It can throw AppException with status code and error message if health check fails. Note : The \"href\" entry in the health service's response should tell the operator about the target URL if the dependency connects to a cloud platform service such as Kafka, Redis, etc. A sample health service is available in the health-check.ts class of the hello world project as follows: import { preload, Composable, EventEnvelope, AppException } from 'mercury-composable'; const TYPE = 'type'; const INFO = 'info'; const HEALTH = 'health'; export class DemoHealthCheck implements Composable { @preload('demo.health') initialize(): Composable { return this; } // Your service should be declared as an async function with input as EventEnvelope async handleEvent(evt: EventEnvelope) { const command = evt.getHeader(TYPE); if (command == INFO) { return {'service': 'demo.service', 'href': 'http://127.0.0.1'}; } if (command == HEALTH) { // this is a dummy health check return {'status': 'demo.service is running fine'}; } throw new AppException(400, 'Request type must be info or health'); } }","title":"Custom health services"},{"location":"guides/APPENDIX-II/#asynchttpclient-api","text":"The \"async.http.request\" function can be used as a non-blocking HTTP client. To make an HTTP request to an external REST endpoint, you can create an HTTP request object using the AsyncHttpRequest class and make an async RPC call to the \"async.http.request\" function like this: const po = new PostOffice(evt.getHeaders()); const req = new AsyncHttpRequest(); req.setMethod(\"GET\"); req.setHeader(\"accept\", \"application/json\"); req.setUrl(\"/api/hello/world?hello world=abc\"); req.setQueryParameter(\"x1\", \"y\"); const list = new Array<string>(); list.push(\"a\"); list.push(\"b\"); req.setQueryParameter(\"x2\", list); req.setTargetHost(\"http://127.0.0.1:8083\"); const event = new EventEnvelope().setTo(\"async.http.request\").setBody(req); const result = po.request(event, 5000); // the result is an EventEnvelope","title":"AsyncHttpClient API"},{"location":"guides/APPENDIX-II/#send-http-request-body-for-http-put-post-and-patch-methods","text":"For most cases, you can just set a JSON object into the request body and specify content-type as JSON. Example code may look like this: const req = new AsyncHttpRequest(); req.setMethod(\"POST\"); req.setHeader(\"accept\", \"application/json\"); req.setHeader(\"content-type\", \"application/json\"); req.setUrl(\"/api/book\"); req.setTargetHost(\"https://service_provider_host\"); req.setBody(jsonKeyValues);","title":"Send HTTP request body for HTTP PUT, POST and PATCH methods"},{"location":"guides/APPENDIX-II/#send-http-request-body-as-a-stream","text":"For larger payload, you may use the streaming method. See sample code below: const stream = new ObjectStreamIO(timeoutInSeconds); const out = stream.getOutputStream(); out.write(blockOne); out.write(blockTwo); // closing the output stream would send a EOF signal to the stream out.close(); // tell the HTTP client to read the input stream req.setStreamRoute(stream.getInputStreamId()); The AsyncHttpClient service (route name async.http.request ) uses native Node.js streams to integrate with the underlying Axios HTTP client. It uses the temporary local file system (folder /tmp/composable/node/temp-streams ) to reduce memory footprint. This makes the producer and consumer of a stream asynchronous. i.e. The producer can write data blocks into a stream before a consumer is available.","title":"Send HTTP request body as a stream"},{"location":"guides/APPENDIX-II/#read-http-response-body-stream","text":"If content length is not given, the response body will be received as a stream. Your application should check if the HTTP response header \"stream\" exists. Its value is the input \"stream ID\". Sample code to read a stream may look like this: static async downloadFile(streamId: string, filename: string) { let n = 0; let len = 0; const stream = new ObjectStreamReader(streamId, 5000); while (true) { try { const block = await stream.read(); if (block) { n++; if (block instanceof Buffer) { len += block.length; log.info(`Received ${filename}, block-${n} - ${block.length} bytes`) } } else { log.info(\"EOF reached\"); break; } } catch (e) { const status = e instanceof AppException? e.getStatus() : 500; log.error(`Exception - rc=${status}, message=${e.message}`); break; } } return len; }","title":"Read HTTP response body stream"},{"location":"guides/APPENDIX-II/#content-length-for-http-request","text":"IMPORTANT : Do not set the \"content-length\" HTTP header because the system will automatically compute the correct content-length for small payload. For large payload, it will use the chunking method.","title":"Content length for HTTP request"},{"location":"guides/APPENDIX-II/#application-log-format","text":"The system supports 3 types of log formats. You can set \"log.format\" parameter in application.yml to change the log format or override it at runtime using the -D argument. e.g. node myapp.js -Dlog.format=json Format Description text this is the default log format json application log will be printed in JSON format with line feed and indentation compact JSON format without line feed and indentation text and json formats are for human readers and compact format is designed for log analytics system.","title":"Application log format"},{"location":"guides/APPENDIX-II/#externalize-the-applicationyml","text":"If you want to externalize the application.yml configuration file, please keep a default application.yml in the \"src/resources\" folder. You can use command line to ask the system to reload a new base configuration file like this: node myapp.js -Dlog.format=json -C/tmp/config/application.yml Note : The -C command argument should point to a fully qualified file path. Use relative path if you know exactly the resolved path in a deployed container. You can have multiple \"-D\" parameters but you can only configure a single \"-C\" argument. Appendix-I Home Application config Table of Contents","title":"Externalize the application.yml"},{"location":"guides/CHAPTER-1/","text":"Introduction Mercury Composable is a software development toolkit for writing composable applications. Composable application means that an application is assembled from modular software components or functions that are self-contained and pluggable. You can mix-n-match functions to form new applications. You can retire outdated functions without adverse side effect to a production system. Multiple versions of a function can exist, and you can decide how to route user requests to different versions of a function. Applications would be easier to design, develop, maintain, deploy, and scale. Composable application architecture Figure 1 - Composable application architecture As shown in Figure 1, a composable application contains the following: Flow adapters : Each flow adapter listens to requests for onwards delivery to an event manager. Event Manager : it sends events to a set of user functions for them to work together as an application. User functions : these are self-contained functions with clear input and output that are immutable. HTTP flow adapter A non-blocking HTTP flow adapter is built-in. For other external interface types, you can implement your own flow adapters. e.g. Adapters for MQ, Kafka, Serverless, File based staging area, etc. The standard HTTP flow adapter leverages the underlying REST automation system to serve user facing REST API endpoints. For example, a hypothetical \"get profile\" endpoint is created like this in the \"rest.yaml\" configuration file: - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/profile/{profile_id}\" flow: 'get-profile' timeout: 10s cors: cors_1 headers: header_1 tracing: true In this REST configuration entry, the system creates a REST API endpoint for \"GET /api/profile/{profile_id}\". When a request arrives at this endpoint, the HTTP request will be converted to an incoming event by the flow adapter that routes the event to the \"event manager\" to execute a new instance of the \"get-profile\" flow. Flow configuration example The event manager is driven by configuration instead of code. A hypothetical \"get profile\" flow is defined in a YAML file like this: flow: id: 'get-profile' description: 'Get a user profile using profile ID' ttl: 10s exception: 'v1.hello.exception' first.task: 'v1.get.profile' tasks: - input: - 'input.path_parameter.profile_id -> header.profile_id' process: 'v1.get.profile' output: - 'result -> model.profile' description: 'Retrieve user profile from database using profile_id' execution: sequential next: - 'v1.decrypt.fields' - input: - 'model.profile -> dataset' - 'text(telephone, address) -> protected_fields' process: 'v1.decrypt.fields' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Decrypt fields' execution: end - input: - 'error.code -> status' - 'error.message -> message' - 'error.stack -> stack' process: 'v1.hello.exception' output: - 'result.status -> output.status' - 'result -> output.body' description: 'Just a demo exception handler' execution: end Note that the flow configuration is referring user functions by their \"route\" names. It is because all user functions are self-contained with clearly defined input and output and the event manager would set their inputs and collect their outputs accordingly. Note that you can map selected key-values or the whole event as a business object and this decoupling promotes highly reusable user functional software. The event manager will create a \"state machine\" to manage each transaction flow because all user functions are stateless. The \"state machine\" is referenced using the namespace \"model\". Assigning a route name to a user function You can assign a route name to a Composite class using the preLoad annotation like this: export class GetProfile implements Composable { @preload('v1.get.profile', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // your business logic here return result; } } Inside the \"handleEvent\" method, you can write regular TypeScript code using your preferred coding style and framework. You can define input/output as key-values (i.e. JSON objects). Building the Mercury libraries from source Assuming you clone the repository into the \"sandbox\" directory, you may build the libraries like this. cd sandbox/mercury-nodejs npm install npm run build The compiled libraries will be saved to the distribution folder ( dist ). For production, you may publish the distribution into your enterprise artifactory. Composable application example Let's take a test drive of a composable application example in this repo: composable-example To build the sample app, please clone example repo and build the application like this: cd sandbox/mercury-composable-examples cd node/composable-example npm install npm run build npm run test When you build the composable example application, the first thing that you may notice is that the build script will scan your source code and libraries. This is similar to the class scanner feature in other languages. This feature is essential in composable application design because it decouples composable classes from each others so that they can be written in a self-contained manner. Your application does not need to import the classes. Instead, the class scanner will create a \"Composable class loader\" during the build phase. The class loader will then load the available composable classes and register them into the event loop. Another important feature that you would find is that a composable application has a \"resources\" folder in the \"src\" and \"test\" sections to hold application configuration files including event flow YAML files. It supports hierarchy of configuration such that the system will search for configuration files in the libraries if your application does not provide a configuration file to override a default configuration file in the library. For example, your unit tests would use a configuration file in the \"src/resources\" folder if it is not in the \"tests/resources\" folder. For details, please refer to the Configuration management section in Appendix-I Your build log may look like this: INFO Scanning ./node_modules/mercury-composable/dist (scanPackage:preloader.js:20) INFO Class NoOp (scanLibrary:preloader.js:78) INFO Scanning ./src (main:preloader.js:200) INFO Class DemoAuth (scanSource:preloader.js:102) INFO Class DemoHealthCheck (scanSource:preloader.js:102) INFO Class HelloWorldService (scanSource:preloader.js:102) INFO Class CreateProfile (scanSource:preloader.js:102) INFO Class DecryptFields (scanSource:preloader.js:102) INFO Class DeleteProfile (scanSource:preloader.js:102) INFO Class EncryptFields (scanSource:preloader.js:102) INFO Class GetProfile (scanSource:preloader.js:102) INFO Class HelloException (scanSource:preloader.js:102) INFO Class SaveProfile (scanSource:preloader.js:102) INFO Composable class loader (/preload/preload.ts) generated (generatePreLoader:preloader.js:176) The build script will compile your TypeScript source files into Javascript and then run the \"preloader.js\" script to scan for your composable functions. Optionally, you can ask it to scan for composable libraries using the \"web.component.scan\" parameter. In the above example, it scan for the package \"mercury-composable\" in the \"node_modules\" folder and find the NoOp function. The first step in designing a composable application is to draw an event flow diagram. This is similar to a data flow diagram where the arrows are labeled with the event objects. Note that event flow diagram is not a flow chart and thus decision box is not required. If a user function (also known as a \"task\") contains decision logic, you can draw two or more output from the task to connect to the next set of functions. For example, label the arrows as true, false or a number starting from 1. The composable-example application is a hypothetical \"profile management system\" where you can create a profile, browse or delete it. Figure 2 - Create a profile Figure 2 illustrates an event flow to create a profile. Note that the \"create profile\" can send acknowledgement to the user first. It then encrypts and saves the profile into a data store. Figure 3 - Retrieve a profile Figure 3 demonstrates the case to retrieve a profile. It retrieves an encrypted profile and then passes it to the decryption decryption function to return \"clear text\" of the profile to the user. Figure 4 - Delete a profile Figure 4 shows the case to delete a profile. It deletes a profile using the given profile ID and sends an acknowledgement to the user. The REST endpoints for the three use cases are shown in the \"rest.yaml\" configuration file under the \"main/resources\" in the example subproject. Extract of some configuration parameters in \"application.yml\" is shown below: application.name: 'composable-example' web.component.scan: 'mercury-composable' server.port: 8086 rest.automation: true yaml.rest.automation: classpath:/rest.yaml yaml.flow.automation: classpath:/flows.yaml The flow configuration files are shown in the \"src/resources/flows\" folder where you will find the flow configuration files for the three event flows, namely get-profile.yml, delete-profile.yml and create-profile.yml. Starting the application To run the composable-example application, you can do this: node dist/composable-example.js When the application starts, you will see extract of the application log like this: INFO Event system started - 15cda88cb4bf4f658357bb6007869296 (platform.js:503) INFO PRIVATE distributed.tracing registered (platform.js:259) INFO PRIVATE async.http.request registered with 200 instances (platform.js:262) INFO PRIVATE no.op registered with 10 instances (platform.js:262) INFO PRIVATE v1.api.auth registered (platform.js:259) INFO PRIVATE demo.health registered (platform.js:259) INFO PUBLIC hello.world registered with 10 instances (platform.js:262) INFO PRIVATE v1.create.profile registered with 10 instances (platform.js:262) INFO PRIVATE v1.decrypt.fields registered with 10 instances (platform.js:262) INFO PRIVATE v1.delete.profile registered with 10 instances (platform.js:262) INFO PRIVATE v1.encrypt.fields registered with 10 instances (platform.js:262) INFO PRIVATE v1.get.profile registered with 10 instances (platform.js:262) INFO PRIVATE v1.hello.exception registered with 10 instances (platform.js:262) INFO PRIVATE v1.save.profile registered with 10 instances (platform.js:262) INFO Loading event scripts from classpath:/flows.yaml (CompileFlows.start:compile-flows.js:72) INFO Parsing create-profile.yml (CompileFlows.createFlow:compile-flows.js:108) INFO Parsing delete-profile.yml (CompileFlows.createFlow:compile-flows.js:108) INFO Parsing get-profile.yml (CompileFlows.createFlow:compile-flows.js:108) INFO Loaded create-profile (CompileFlows.start:compile-flows.js:102) INFO Loaded delete-profile (CompileFlows.start:compile-flows.js:102) INFO Loaded get-profile (CompileFlows.start:compile-flows.js:102) INFO Event scripts deployed: 3 (CompileFlows.start:compile-flows.js:104) INFO Loading EventScriptManager as event.script.manager (FunctionRegistry.save:function-registry.js:33) INFO PRIVATE event.script.manager registered (platform.js:259) INFO Loading TaskExecutor as task.executor (FunctionRegistry.save:function-registry.js:33) INFO PRIVATE task.executor registered (platform.js:259) INFO Loading HttpToFlow as http.flow.adapter (FunctionRegistry.save:function-registry.js:33) INFO PRIVATE http.flow.adapter registered with 200 instances (platform.js:262) INFO To stop application, press Control-C (EventSystem.runForever:platform.js:589) INFO Composable application started (main:composable-example.js:27) INFO REST automation service started on port 8086 (rest-automation.js:443) It shows that the 3 flow configuration files are compiled as objects to optimize performance. The user functions are loaded into the event system and the REST endpoints are rendered from the \"rest.yaml\" file. Testing the application You can create a test user profile with this python code. Alternatively, you can also use PostMan or other means to do this. >>> import requests, json >>> d = { 'id': 100, 'name': 'Hello World', 'address': '100 World Blvd', 'telephone': '123-456-7890' } >>> h = { 'content-type': 'application/json', 'accept': 'application/json' } >>> r = requests.post('http://127.0.0.1:8100/api/profile', data=json.dumps(d), headers=h) >>> print(r.status_code) 201 >>> print(r.text) { \"profile\": { \"address\": \"***\", \"name\": \"Hello World\", \"telephone\": \"***\", \"id\": 100 }, \"type\": \"CREATE\", \"secure\": [ \"address\", \"telephone\" ] } To verify that the user profile has been created, you can point your browser to http://127.0.0.1:8100/api/profile/100 Your browser will return the following: { \"address\": \"100 World Blvd\", \"name\": \"Hello World\", \"telephone\": \"123-456-7890\", \"id\": 100 } You have successfully tested the two REST endpoints. Tracing information in the application log may look like this: DistributedTrace:76 - trace={path=POST /api/profile, service=http.flow.adapter, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.524Z, exec_time=0.284, from=http.request, id=f6a6ae62340e43afb0a6f30445166e08} DistributedTrace:76 - trace={path=POST /api/profile, service=event.script.manager, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.525Z, exec_time=0.57, from=http.flow.adapter, id=f6a6ae62340e43afb0a6f30445166e08} DistributedTrace:76 - trace={path=POST /api/profile, service=v1.create.profile, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.526Z, exec_time=0.342, from=task.executor, id=f6a6ae62340e43afb0a6f30445166e08} DistributedTrace:76 - trace={path=POST /api/profile, service=async.http.response, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.528Z, exec_time=0.294, from=task.executor, id=f6a6ae62340e43afb0a6f30445166e08} DistributedTrace:76 - trace={path=POST /api/profile, service=v1.encrypt.fields, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.528Z, exec_time=3.64, from=task.executor, id=f6a6ae62340e43afb0a6f30445166e08} SaveProfile:52 - Profile 100 saved TaskExecutor:186 - TaskExecutor:262 - { \"execution\": \"Run 3 tasks in 11 ms\", \"id\": \"a0eef12d94bd4ab3b5fd6c25e2461130\", \"flow\": \"get-profile\", \"tasks\": [ \"v1.create.profile\", \"v1.encrypt.fields\", \"v1.save.profile\" ], \"status\": \"completed\" } DistributedTrace:76 - trace={path=POST /api/profile, service=v1.save.profile, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.533Z, exec_time=2.006, from=task.executor, id=f6a6ae62340e43afb0a6f30445166e08} DistributedTrace:76 - trace={path=GET /api/profile/100, service=http.flow.adapter, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:52.089Z, exec_time=0.152, from=http.request, id=1a29105044e94cc3ac68aee002f6f429} DistributedTrace:76 - trace={path=GET /api/profile/100, service=event.script.manager, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:52.090Z, exec_time=0.291, from=http.flow.adapter, id=1a29105044e94cc3ac68aee002f6f429} DistributedTrace:76 - trace={path=GET /api/profile/100, service=v1.get.profile, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:52.091Z, exec_time=1.137, from=task.executor, id=1a29105044e94cc3ac68aee002f6f429} DistributedTrace:76 - trace={path=GET /api/profile/100, service=v1.decrypt.fields, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:52.093Z, exec_time=1.22, from=task.executor, id=1a29105044e94cc3ac68aee002f6f429} TaskExecutor:262 - { \"execution\": \"Run 2 tasks in 7 ms\", \"id\": \"a0eef12d94bd4ab3b5fd6c25e2461130\", \"flow\": \"get-profile\", \"tasks\": [ \"v1.get.profile\", \"v1.decrypt.fields\" ], \"status\": \"completed\" } DistributedTrace:76 - trace={path=GET /api/profile/100, service=async.http.response, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:52.095Z, exec_time=0.214, from=task.executor, id=1a29105044e94cc3ac68aee002f6f429} Main application entry point Every application has an entry point. The main entry point in the example app contains the entry point like this: async function main() { // Load composable functions into memory and start the application modules await ComposableLoader.initialize(); } // run the application main(); The \"ComposableLoader.initializer()\" command will load the composable functions into the event loop and run the application as a service. If your application needs additional setup code, you can create a composable function like this: export class MainApp implements Composable { @preload('main.app') initialize(): Composable { return this; } // This 'main.app' function is configured in the 'modules.autostart' parameter in application.yml // It will be started automatically. async handleEvent(evt: EventEnvelope) { // put business logic of any additional setup procedure here log.info(\"Application started\"); // release this function to guarantee that it is executed only once Platform.getInstance().release('main.app'); // return value is ignored because start up code runs asynchronously return true; } } The above composable function is labeled as main.app , you would need to add this to the application.yml as follows: modules.autostart: - 'main.app' - 'flow://my-startup-flow' For more sophisticated startup procedure, you can use a flow to execute multiple tasks. The second item in the modules.autostart illustrates this use case. Note : autostart modules or flows should assume there is no input dataset except a header ('type = start') to indicate that the request is triggered by \"autostart\" process. Startup modules usually take input parameters from the environment variables or a secret manager. Graceful shutdown If your application has some dependencies that must be shutdown gracefully, you can create a composable function to handle the shutdown. The following configuration parameter in application.yml will invoke the composable function with the route name \"shutdown.hook\". The system will wait for the completion of the shutdown.hook before closing the application. modules.autostop: - 'shutdown.hook' Note : Similar to the autostart design, autostop modules should assume there is no input dataset except a header ('type = stop') to to indicate that the request is triggered by \"autostop\". For simplicity, the autostop feature does not support shutdown sequence using a flow. Commad line application If you want to run your application as a command line application instead of a service, your application can close itself like this: await platform.getReady(); // execute your business logic and then run the \"platform.stop()\" command to exit await platform.stop(); Dependency management As a best practice, your user functions should not have any dependencies with other user functions. The second principle of composable design is \"zero to one dependency\". If your composable function must use an external system, platform or database, you can encapsulate the dependency in a composable function. Component scan Please update the following in the application.yml to include packages of your own functions: web.component.scan=your-package-name You should replace \"your-package-name\" with the real package name(s) that you use in your application. \"web.component.scan\" is a comma separated list of package names. Deploy your application Composable design can be used to create microservices. You can put related functions in a bounded context with database persistence. Each composable application can be compiled and built into a single \"executable\" for deployment using npm run build . The executable Javascript application bundle is in the dist folder. Composable application is by definition cloud native. It is designed to be deployable using Kubernetes or serverless. Event choreography by configuration The best practice for composable design is event choreography by configuration ( Event Script ) discussed above. We will examine the Event Script syntax in Chapter 4 . Generally, you only need to use a very minimal set of mercury core APIs in your user function. e.g. use PostOffice to obtain a trackable event emitter and AsyncHttpRequest to connect to external system. For composable applications that use Event Script, Mercury core APIs (Platform, PostOffice and FastRPC) are only required for writing unit tests, \"custom flow adapters\", \"legacy functional wrappers\" or \"external gateways\". Orchestration by code Orchestration by code is strongly discouraged because it would result in tightly coupled code . For example, just an \"Import\" statement of another function would create tight coupling of two pieces of code, even when using reactive or event-driven programming styles. However, if there is a use case that you prefer to write orchestration logic by code, you may use the Mercury core APIs to do event-driven programming. API overview will be covered in Chapter 7 . Methodology Home Chapter-2 Methodology Table of Contents Function Execution Strategy","title":"Chapter-1"},{"location":"guides/CHAPTER-1/#introduction","text":"Mercury Composable is a software development toolkit for writing composable applications. Composable application means that an application is assembled from modular software components or functions that are self-contained and pluggable. You can mix-n-match functions to form new applications. You can retire outdated functions without adverse side effect to a production system. Multiple versions of a function can exist, and you can decide how to route user requests to different versions of a function. Applications would be easier to design, develop, maintain, deploy, and scale.","title":"Introduction"},{"location":"guides/CHAPTER-1/#composable-application-architecture","text":"Figure 1 - Composable application architecture As shown in Figure 1, a composable application contains the following: Flow adapters : Each flow adapter listens to requests for onwards delivery to an event manager. Event Manager : it sends events to a set of user functions for them to work together as an application. User functions : these are self-contained functions with clear input and output that are immutable.","title":"Composable application architecture"},{"location":"guides/CHAPTER-1/#http-flow-adapter","text":"A non-blocking HTTP flow adapter is built-in. For other external interface types, you can implement your own flow adapters. e.g. Adapters for MQ, Kafka, Serverless, File based staging area, etc. The standard HTTP flow adapter leverages the underlying REST automation system to serve user facing REST API endpoints. For example, a hypothetical \"get profile\" endpoint is created like this in the \"rest.yaml\" configuration file: - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/profile/{profile_id}\" flow: 'get-profile' timeout: 10s cors: cors_1 headers: header_1 tracing: true In this REST configuration entry, the system creates a REST API endpoint for \"GET /api/profile/{profile_id}\". When a request arrives at this endpoint, the HTTP request will be converted to an incoming event by the flow adapter that routes the event to the \"event manager\" to execute a new instance of the \"get-profile\" flow.","title":"HTTP flow adapter"},{"location":"guides/CHAPTER-1/#flow-configuration-example","text":"The event manager is driven by configuration instead of code. A hypothetical \"get profile\" flow is defined in a YAML file like this: flow: id: 'get-profile' description: 'Get a user profile using profile ID' ttl: 10s exception: 'v1.hello.exception' first.task: 'v1.get.profile' tasks: - input: - 'input.path_parameter.profile_id -> header.profile_id' process: 'v1.get.profile' output: - 'result -> model.profile' description: 'Retrieve user profile from database using profile_id' execution: sequential next: - 'v1.decrypt.fields' - input: - 'model.profile -> dataset' - 'text(telephone, address) -> protected_fields' process: 'v1.decrypt.fields' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Decrypt fields' execution: end - input: - 'error.code -> status' - 'error.message -> message' - 'error.stack -> stack' process: 'v1.hello.exception' output: - 'result.status -> output.status' - 'result -> output.body' description: 'Just a demo exception handler' execution: end Note that the flow configuration is referring user functions by their \"route\" names. It is because all user functions are self-contained with clearly defined input and output and the event manager would set their inputs and collect their outputs accordingly. Note that you can map selected key-values or the whole event as a business object and this decoupling promotes highly reusable user functional software. The event manager will create a \"state machine\" to manage each transaction flow because all user functions are stateless. The \"state machine\" is referenced using the namespace \"model\".","title":"Flow configuration example"},{"location":"guides/CHAPTER-1/#assigning-a-route-name-to-a-user-function","text":"You can assign a route name to a Composite class using the preLoad annotation like this: export class GetProfile implements Composable { @preload('v1.get.profile', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // your business logic here return result; } } Inside the \"handleEvent\" method, you can write regular TypeScript code using your preferred coding style and framework. You can define input/output as key-values (i.e. JSON objects).","title":"Assigning a route name to a user function"},{"location":"guides/CHAPTER-1/#building-the-mercury-libraries-from-source","text":"Assuming you clone the repository into the \"sandbox\" directory, you may build the libraries like this. cd sandbox/mercury-nodejs npm install npm run build The compiled libraries will be saved to the distribution folder ( dist ). For production, you may publish the distribution into your enterprise artifactory.","title":"Building the Mercury libraries from source"},{"location":"guides/CHAPTER-1/#composable-application-example","text":"Let's take a test drive of a composable application example in this repo: composable-example To build the sample app, please clone example repo and build the application like this: cd sandbox/mercury-composable-examples cd node/composable-example npm install npm run build npm run test When you build the composable example application, the first thing that you may notice is that the build script will scan your source code and libraries. This is similar to the class scanner feature in other languages. This feature is essential in composable application design because it decouples composable classes from each others so that they can be written in a self-contained manner. Your application does not need to import the classes. Instead, the class scanner will create a \"Composable class loader\" during the build phase. The class loader will then load the available composable classes and register them into the event loop. Another important feature that you would find is that a composable application has a \"resources\" folder in the \"src\" and \"test\" sections to hold application configuration files including event flow YAML files. It supports hierarchy of configuration such that the system will search for configuration files in the libraries if your application does not provide a configuration file to override a default configuration file in the library. For example, your unit tests would use a configuration file in the \"src/resources\" folder if it is not in the \"tests/resources\" folder. For details, please refer to the Configuration management section in Appendix-I Your build log may look like this: INFO Scanning ./node_modules/mercury-composable/dist (scanPackage:preloader.js:20) INFO Class NoOp (scanLibrary:preloader.js:78) INFO Scanning ./src (main:preloader.js:200) INFO Class DemoAuth (scanSource:preloader.js:102) INFO Class DemoHealthCheck (scanSource:preloader.js:102) INFO Class HelloWorldService (scanSource:preloader.js:102) INFO Class CreateProfile (scanSource:preloader.js:102) INFO Class DecryptFields (scanSource:preloader.js:102) INFO Class DeleteProfile (scanSource:preloader.js:102) INFO Class EncryptFields (scanSource:preloader.js:102) INFO Class GetProfile (scanSource:preloader.js:102) INFO Class HelloException (scanSource:preloader.js:102) INFO Class SaveProfile (scanSource:preloader.js:102) INFO Composable class loader (/preload/preload.ts) generated (generatePreLoader:preloader.js:176) The build script will compile your TypeScript source files into Javascript and then run the \"preloader.js\" script to scan for your composable functions. Optionally, you can ask it to scan for composable libraries using the \"web.component.scan\" parameter. In the above example, it scan for the package \"mercury-composable\" in the \"node_modules\" folder and find the NoOp function. The first step in designing a composable application is to draw an event flow diagram. This is similar to a data flow diagram where the arrows are labeled with the event objects. Note that event flow diagram is not a flow chart and thus decision box is not required. If a user function (also known as a \"task\") contains decision logic, you can draw two or more output from the task to connect to the next set of functions. For example, label the arrows as true, false or a number starting from 1. The composable-example application is a hypothetical \"profile management system\" where you can create a profile, browse or delete it. Figure 2 - Create a profile Figure 2 illustrates an event flow to create a profile. Note that the \"create profile\" can send acknowledgement to the user first. It then encrypts and saves the profile into a data store. Figure 3 - Retrieve a profile Figure 3 demonstrates the case to retrieve a profile. It retrieves an encrypted profile and then passes it to the decryption decryption function to return \"clear text\" of the profile to the user. Figure 4 - Delete a profile Figure 4 shows the case to delete a profile. It deletes a profile using the given profile ID and sends an acknowledgement to the user. The REST endpoints for the three use cases are shown in the \"rest.yaml\" configuration file under the \"main/resources\" in the example subproject. Extract of some configuration parameters in \"application.yml\" is shown below: application.name: 'composable-example' web.component.scan: 'mercury-composable' server.port: 8086 rest.automation: true yaml.rest.automation: classpath:/rest.yaml yaml.flow.automation: classpath:/flows.yaml The flow configuration files are shown in the \"src/resources/flows\" folder where you will find the flow configuration files for the three event flows, namely get-profile.yml, delete-profile.yml and create-profile.yml.","title":"Composable application example"},{"location":"guides/CHAPTER-1/#starting-the-application","text":"To run the composable-example application, you can do this: node dist/composable-example.js When the application starts, you will see extract of the application log like this: INFO Event system started - 15cda88cb4bf4f658357bb6007869296 (platform.js:503) INFO PRIVATE distributed.tracing registered (platform.js:259) INFO PRIVATE async.http.request registered with 200 instances (platform.js:262) INFO PRIVATE no.op registered with 10 instances (platform.js:262) INFO PRIVATE v1.api.auth registered (platform.js:259) INFO PRIVATE demo.health registered (platform.js:259) INFO PUBLIC hello.world registered with 10 instances (platform.js:262) INFO PRIVATE v1.create.profile registered with 10 instances (platform.js:262) INFO PRIVATE v1.decrypt.fields registered with 10 instances (platform.js:262) INFO PRIVATE v1.delete.profile registered with 10 instances (platform.js:262) INFO PRIVATE v1.encrypt.fields registered with 10 instances (platform.js:262) INFO PRIVATE v1.get.profile registered with 10 instances (platform.js:262) INFO PRIVATE v1.hello.exception registered with 10 instances (platform.js:262) INFO PRIVATE v1.save.profile registered with 10 instances (platform.js:262) INFO Loading event scripts from classpath:/flows.yaml (CompileFlows.start:compile-flows.js:72) INFO Parsing create-profile.yml (CompileFlows.createFlow:compile-flows.js:108) INFO Parsing delete-profile.yml (CompileFlows.createFlow:compile-flows.js:108) INFO Parsing get-profile.yml (CompileFlows.createFlow:compile-flows.js:108) INFO Loaded create-profile (CompileFlows.start:compile-flows.js:102) INFO Loaded delete-profile (CompileFlows.start:compile-flows.js:102) INFO Loaded get-profile (CompileFlows.start:compile-flows.js:102) INFO Event scripts deployed: 3 (CompileFlows.start:compile-flows.js:104) INFO Loading EventScriptManager as event.script.manager (FunctionRegistry.save:function-registry.js:33) INFO PRIVATE event.script.manager registered (platform.js:259) INFO Loading TaskExecutor as task.executor (FunctionRegistry.save:function-registry.js:33) INFO PRIVATE task.executor registered (platform.js:259) INFO Loading HttpToFlow as http.flow.adapter (FunctionRegistry.save:function-registry.js:33) INFO PRIVATE http.flow.adapter registered with 200 instances (platform.js:262) INFO To stop application, press Control-C (EventSystem.runForever:platform.js:589) INFO Composable application started (main:composable-example.js:27) INFO REST automation service started on port 8086 (rest-automation.js:443) It shows that the 3 flow configuration files are compiled as objects to optimize performance. The user functions are loaded into the event system and the REST endpoints are rendered from the \"rest.yaml\" file.","title":"Starting the application"},{"location":"guides/CHAPTER-1/#testing-the-application","text":"You can create a test user profile with this python code. Alternatively, you can also use PostMan or other means to do this. >>> import requests, json >>> d = { 'id': 100, 'name': 'Hello World', 'address': '100 World Blvd', 'telephone': '123-456-7890' } >>> h = { 'content-type': 'application/json', 'accept': 'application/json' } >>> r = requests.post('http://127.0.0.1:8100/api/profile', data=json.dumps(d), headers=h) >>> print(r.status_code) 201 >>> print(r.text) { \"profile\": { \"address\": \"***\", \"name\": \"Hello World\", \"telephone\": \"***\", \"id\": 100 }, \"type\": \"CREATE\", \"secure\": [ \"address\", \"telephone\" ] } To verify that the user profile has been created, you can point your browser to http://127.0.0.1:8100/api/profile/100 Your browser will return the following: { \"address\": \"100 World Blvd\", \"name\": \"Hello World\", \"telephone\": \"123-456-7890\", \"id\": 100 } You have successfully tested the two REST endpoints. Tracing information in the application log may look like this: DistributedTrace:76 - trace={path=POST /api/profile, service=http.flow.adapter, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.524Z, exec_time=0.284, from=http.request, id=f6a6ae62340e43afb0a6f30445166e08} DistributedTrace:76 - trace={path=POST /api/profile, service=event.script.manager, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.525Z, exec_time=0.57, from=http.flow.adapter, id=f6a6ae62340e43afb0a6f30445166e08} DistributedTrace:76 - trace={path=POST /api/profile, service=v1.create.profile, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.526Z, exec_time=0.342, from=task.executor, id=f6a6ae62340e43afb0a6f30445166e08} DistributedTrace:76 - trace={path=POST /api/profile, service=async.http.response, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.528Z, exec_time=0.294, from=task.executor, id=f6a6ae62340e43afb0a6f30445166e08} DistributedTrace:76 - trace={path=POST /api/profile, service=v1.encrypt.fields, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.528Z, exec_time=3.64, from=task.executor, id=f6a6ae62340e43afb0a6f30445166e08} SaveProfile:52 - Profile 100 saved TaskExecutor:186 - TaskExecutor:262 - { \"execution\": \"Run 3 tasks in 11 ms\", \"id\": \"a0eef12d94bd4ab3b5fd6c25e2461130\", \"flow\": \"get-profile\", \"tasks\": [ \"v1.create.profile\", \"v1.encrypt.fields\", \"v1.save.profile\" ], \"status\": \"completed\" } DistributedTrace:76 - trace={path=POST /api/profile, service=v1.save.profile, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:23.533Z, exec_time=2.006, from=task.executor, id=f6a6ae62340e43afb0a6f30445166e08} DistributedTrace:76 - trace={path=GET /api/profile/100, service=http.flow.adapter, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:52.089Z, exec_time=0.152, from=http.request, id=1a29105044e94cc3ac68aee002f6f429} DistributedTrace:76 - trace={path=GET /api/profile/100, service=event.script.manager, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:52.090Z, exec_time=0.291, from=http.flow.adapter, id=1a29105044e94cc3ac68aee002f6f429} DistributedTrace:76 - trace={path=GET /api/profile/100, service=v1.get.profile, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:52.091Z, exec_time=1.137, from=task.executor, id=1a29105044e94cc3ac68aee002f6f429} DistributedTrace:76 - trace={path=GET /api/profile/100, service=v1.decrypt.fields, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:52.093Z, exec_time=1.22, from=task.executor, id=1a29105044e94cc3ac68aee002f6f429} TaskExecutor:262 - { \"execution\": \"Run 2 tasks in 7 ms\", \"id\": \"a0eef12d94bd4ab3b5fd6c25e2461130\", \"flow\": \"get-profile\", \"tasks\": [ \"v1.get.profile\", \"v1.decrypt.fields\" ], \"status\": \"completed\" } DistributedTrace:76 - trace={path=GET /api/profile/100, service=async.http.response, success=true, origin=202406249aea0a481d46401d8379c8896a6698a2, start=2024-06-24T22:41:52.095Z, exec_time=0.214, from=task.executor, id=1a29105044e94cc3ac68aee002f6f429}","title":"Testing the application"},{"location":"guides/CHAPTER-1/#main-application-entry-point","text":"Every application has an entry point. The main entry point in the example app contains the entry point like this: async function main() { // Load composable functions into memory and start the application modules await ComposableLoader.initialize(); } // run the application main(); The \"ComposableLoader.initializer()\" command will load the composable functions into the event loop and run the application as a service. If your application needs additional setup code, you can create a composable function like this: export class MainApp implements Composable { @preload('main.app') initialize(): Composable { return this; } // This 'main.app' function is configured in the 'modules.autostart' parameter in application.yml // It will be started automatically. async handleEvent(evt: EventEnvelope) { // put business logic of any additional setup procedure here log.info(\"Application started\"); // release this function to guarantee that it is executed only once Platform.getInstance().release('main.app'); // return value is ignored because start up code runs asynchronously return true; } } The above composable function is labeled as main.app , you would need to add this to the application.yml as follows: modules.autostart: - 'main.app' - 'flow://my-startup-flow' For more sophisticated startup procedure, you can use a flow to execute multiple tasks. The second item in the modules.autostart illustrates this use case. Note : autostart modules or flows should assume there is no input dataset except a header ('type = start') to indicate that the request is triggered by \"autostart\" process. Startup modules usually take input parameters from the environment variables or a secret manager.","title":"Main application entry point"},{"location":"guides/CHAPTER-1/#graceful-shutdown","text":"If your application has some dependencies that must be shutdown gracefully, you can create a composable function to handle the shutdown. The following configuration parameter in application.yml will invoke the composable function with the route name \"shutdown.hook\". The system will wait for the completion of the shutdown.hook before closing the application. modules.autostop: - 'shutdown.hook' Note : Similar to the autostart design, autostop modules should assume there is no input dataset except a header ('type = stop') to to indicate that the request is triggered by \"autostop\". For simplicity, the autostop feature does not support shutdown sequence using a flow.","title":"Graceful shutdown"},{"location":"guides/CHAPTER-1/#commad-line-application","text":"If you want to run your application as a command line application instead of a service, your application can close itself like this: await platform.getReady(); // execute your business logic and then run the \"platform.stop()\" command to exit await platform.stop();","title":"Commad line application"},{"location":"guides/CHAPTER-1/#dependency-management","text":"As a best practice, your user functions should not have any dependencies with other user functions. The second principle of composable design is \"zero to one dependency\". If your composable function must use an external system, platform or database, you can encapsulate the dependency in a composable function.","title":"Dependency management"},{"location":"guides/CHAPTER-1/#component-scan","text":"Please update the following in the application.yml to include packages of your own functions: web.component.scan=your-package-name You should replace \"your-package-name\" with the real package name(s) that you use in your application. \"web.component.scan\" is a comma separated list of package names.","title":"Component scan"},{"location":"guides/CHAPTER-1/#deploy-your-application","text":"Composable design can be used to create microservices. You can put related functions in a bounded context with database persistence. Each composable application can be compiled and built into a single \"executable\" for deployment using npm run build . The executable Javascript application bundle is in the dist folder. Composable application is by definition cloud native. It is designed to be deployable using Kubernetes or serverless.","title":"Deploy your application"},{"location":"guides/CHAPTER-1/#event-choreography-by-configuration","text":"The best practice for composable design is event choreography by configuration ( Event Script ) discussed above. We will examine the Event Script syntax in Chapter 4 . Generally, you only need to use a very minimal set of mercury core APIs in your user function. e.g. use PostOffice to obtain a trackable event emitter and AsyncHttpRequest to connect to external system. For composable applications that use Event Script, Mercury core APIs (Platform, PostOffice and FastRPC) are only required for writing unit tests, \"custom flow adapters\", \"legacy functional wrappers\" or \"external gateways\".","title":"Event choreography by configuration"},{"location":"guides/CHAPTER-1/#orchestration-by-code","text":"Orchestration by code is strongly discouraged because it would result in tightly coupled code . For example, just an \"Import\" statement of another function would create tight coupling of two pieces of code, even when using reactive or event-driven programming styles. However, if there is a use case that you prefer to write orchestration logic by code, you may use the Mercury core APIs to do event-driven programming. API overview will be covered in Chapter 7 . Methodology Home Chapter-2 Methodology Table of Contents Function Execution Strategy","title":"Orchestration by code"},{"location":"guides/CHAPTER-2/","text":"Function Execution Strategies Define a function In a composable application, each function is self-contained with zero dependencies with other user functions. Only flow adapter, data adapter, notification function or gateway has a single external dependency such as a network event system, a database or an external REST resource. A \"task\" or \"function\" is a class that implements the Composable interface. Within each function boundary, it may have private methods that are fully contained within the class. As discussed in Chapter-1, a function may look like this: export class MyFirstFunction implements Composable { @preload(my.first.function', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // your business logic here return result; } } A function is an event listener with the \"handleEvent\" method. The data structures of input and output are defined by API interface contract in an event flow configuration. In the above example, the input is an event envelope and the output is a set of key-values as a JSON object. You can access event body (i.e. payload ), headers and metadata from the event envelope. For event choreography, input body is represented as a JSON object of key-values so that you can use the dot-bracket convention to map a subset from one function to another if needed. In addition to the event body, you may pass additional parameters to the user function as event headers. We will discuss this in Chapter 4 - Event Script Syntax . Non-blocking design By design, Javascript libraries are usually asynchronous. With functional isolation, each composable function is executed through an event loop. Inside your composable function, you can apply sequential, object-oriented or reactive programming styles. Just make sure your code is not blocked. The composable interface enforces a function to be implemented using either the \"Promise\" or the \"async\" pattern. Both of these patterns are non-blocking. The async pattern turns a \"Promise\" function into sequential execution style to improve code readability. You should use the async/await pattern as much as possible unless you have reason to use the Promise reactive coding style. Object serialization consideration The system is designed to deliver primitive and JSON object of key-values through an event stream. If you pass JSON object or primitive such as string or Buffer, you do not need to do any serialization. Note : You should not pass data class object as event body. Instead, please convert them using the JSON parser API and pass a JSON object of key-values. The standard JSON library can handle serialization and deserialization efficiently. Extensible authentication function You can add authentication function using the optional authentication tag in a service. In \"rest.yaml\", a service for a REST endpoint refers to a function in your application. An authentication function can be written using a Composable function that takes the input body as an \"AsyncHttpRequest\" like this: const request = new AsyncHttpRequest(evt.getBody()); Your authentication function can return a boolean value to indicate if the request should be accepted or rejected. A typical authentication function may validate an HTTP header or cookie. e.g. forward the \"Bearer token\" from the \"Authorization\" header to your organization's OAuth 2.0 Identity Provider for validation. To approve an incoming request, your custom authentication function can return true . Optionally, you can add \"session\" key-values by returning an EventEnvelope like this: return new EventEnvelope().setHeader(\"user_id\", \"A12345\").setBody(true); The above example approves the incoming request and returns a \"session\" variable (\"user_id\": \"A12345\") to the next task. If your authentication function returns false , the user will receive a \"HTTP-401 Unauthorized\" error response. You can also control the status code and error message by throwing an AppException like this: throw new AppException(401, \"Invalid credentials\"); Alternatively, you may implement authentication as a user function in the first step of an event flow. In this case, the input to the function is defined by the \"input data mapping\" rules in the event flow configuration. The advantage of this approach is that authentication is shown as part of an event flow so that the application design intention is clear. A composable application is assembled from a collection of self-contained functions that are highly reusable. Number of workers for a function In the following annotation, the execution concurrency is the second parameter in the preload annotation. It tells the system to reserve a number of workers for the function. Workers are running on-demand to handle concurrent user requests. @preload(my.first.function', 10) initialize(): Composable { return this; } Note that you can use smaller number of workers to handle many concurrent users if your function finishes processing very quickly. If not, you should reserve more workers to handle the work load. Concurrency requires careful planning for optimal performance and throughput. Functional isolation of legacy code Node.js supports \"functional isolation\" using \"worker-threads\" technology. Each worker will run in a separate Chromium V8 engine with isolated memory space. For some open source or legacy libraries that you have no control to convert into Composable functions, you can encapsulate them into a separate worker thread and expose their capabilities as Composable functions. For more details, please refer to: Composable Developer Guide Chapter-4 Mozilla Developer Guide Node.js Worker Thread API Documentation A worked example is available in Composable-example The composable-worker.ts class may be used as a template: ComposableWorker Chapter-1 Home Chapter-3 Introduction Table of Contents REST Automation","title":"Chapter-2"},{"location":"guides/CHAPTER-2/#function-execution-strategies","text":"","title":"Function Execution Strategies"},{"location":"guides/CHAPTER-2/#define-a-function","text":"In a composable application, each function is self-contained with zero dependencies with other user functions. Only flow adapter, data adapter, notification function or gateway has a single external dependency such as a network event system, a database or an external REST resource. A \"task\" or \"function\" is a class that implements the Composable interface. Within each function boundary, it may have private methods that are fully contained within the class. As discussed in Chapter-1, a function may look like this: export class MyFirstFunction implements Composable { @preload(my.first.function', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // your business logic here return result; } } A function is an event listener with the \"handleEvent\" method. The data structures of input and output are defined by API interface contract in an event flow configuration. In the above example, the input is an event envelope and the output is a set of key-values as a JSON object. You can access event body (i.e. payload ), headers and metadata from the event envelope. For event choreography, input body is represented as a JSON object of key-values so that you can use the dot-bracket convention to map a subset from one function to another if needed. In addition to the event body, you may pass additional parameters to the user function as event headers. We will discuss this in Chapter 4 - Event Script Syntax .","title":"Define a function"},{"location":"guides/CHAPTER-2/#non-blocking-design","text":"By design, Javascript libraries are usually asynchronous. With functional isolation, each composable function is executed through an event loop. Inside your composable function, you can apply sequential, object-oriented or reactive programming styles. Just make sure your code is not blocked. The composable interface enforces a function to be implemented using either the \"Promise\" or the \"async\" pattern. Both of these patterns are non-blocking. The async pattern turns a \"Promise\" function into sequential execution style to improve code readability. You should use the async/await pattern as much as possible unless you have reason to use the Promise reactive coding style.","title":"Non-blocking design"},{"location":"guides/CHAPTER-2/#object-serialization-consideration","text":"The system is designed to deliver primitive and JSON object of key-values through an event stream. If you pass JSON object or primitive such as string or Buffer, you do not need to do any serialization. Note : You should not pass data class object as event body. Instead, please convert them using the JSON parser API and pass a JSON object of key-values. The standard JSON library can handle serialization and deserialization efficiently.","title":"Object serialization consideration"},{"location":"guides/CHAPTER-2/#extensible-authentication-function","text":"You can add authentication function using the optional authentication tag in a service. In \"rest.yaml\", a service for a REST endpoint refers to a function in your application. An authentication function can be written using a Composable function that takes the input body as an \"AsyncHttpRequest\" like this: const request = new AsyncHttpRequest(evt.getBody()); Your authentication function can return a boolean value to indicate if the request should be accepted or rejected. A typical authentication function may validate an HTTP header or cookie. e.g. forward the \"Bearer token\" from the \"Authorization\" header to your organization's OAuth 2.0 Identity Provider for validation. To approve an incoming request, your custom authentication function can return true . Optionally, you can add \"session\" key-values by returning an EventEnvelope like this: return new EventEnvelope().setHeader(\"user_id\", \"A12345\").setBody(true); The above example approves the incoming request and returns a \"session\" variable (\"user_id\": \"A12345\") to the next task. If your authentication function returns false , the user will receive a \"HTTP-401 Unauthorized\" error response. You can also control the status code and error message by throwing an AppException like this: throw new AppException(401, \"Invalid credentials\"); Alternatively, you may implement authentication as a user function in the first step of an event flow. In this case, the input to the function is defined by the \"input data mapping\" rules in the event flow configuration. The advantage of this approach is that authentication is shown as part of an event flow so that the application design intention is clear. A composable application is assembled from a collection of self-contained functions that are highly reusable.","title":"Extensible authentication function"},{"location":"guides/CHAPTER-2/#number-of-workers-for-a-function","text":"In the following annotation, the execution concurrency is the second parameter in the preload annotation. It tells the system to reserve a number of workers for the function. Workers are running on-demand to handle concurrent user requests. @preload(my.first.function', 10) initialize(): Composable { return this; } Note that you can use smaller number of workers to handle many concurrent users if your function finishes processing very quickly. If not, you should reserve more workers to handle the work load. Concurrency requires careful planning for optimal performance and throughput.","title":"Number of workers for a function"},{"location":"guides/CHAPTER-2/#functional-isolation-of-legacy-code","text":"Node.js supports \"functional isolation\" using \"worker-threads\" technology. Each worker will run in a separate Chromium V8 engine with isolated memory space. For some open source or legacy libraries that you have no control to convert into Composable functions, you can encapsulate them into a separate worker thread and expose their capabilities as Composable functions. For more details, please refer to: Composable Developer Guide Chapter-4 Mozilla Developer Guide Node.js Worker Thread API Documentation A worked example is available in Composable-example The composable-worker.ts class may be used as a template: ComposableWorker Chapter-1 Home Chapter-3 Introduction Table of Contents REST Automation","title":"Functional isolation of legacy code"},{"location":"guides/CHAPTER-3/","text":"REST automation The foundation library contains a built-in non-blocking HTTP server that you can use to create REST endpoints. Behind the curtain, it is using the Express server library, and we extend it to support dynamic creation of REST endpoints. The REST automation system is not a code generator. The REST endpoints in the rest.yaml file are handled by the system directly - \"Config is the code\". We will use the \"rest.yaml\" sample configuration file in the \"composable-example\" app to elaborate the configuration approach. The rest.yaml configuration has three sections: REST endpoint definition CORS header processing HTTP header transformation Turn on the REST automation engine REST automation is optional. To turn on REST automation, add the REST automation start up script in your main app: import { Logger, Platform, RestAutomation } from 'mercury-composable'; import { ComposableLoader } from '../preload/preload.js'; ... async function main() { // Load composable functions into memory and initialize configuration management ComposableLoader.initialize(); // keep the server running const platform = Platform.getInstance(); platform.runForever(); log.info('Composable application started'); } main(); Note : The class \"preload.ts\" is automatically generated when you do \"npm run preload\" or \"npm run build\". The compiled file is located in the \"dist/preload/preload.js\". Therefore, you must use an import statement for '../preload/preload.js'. Please review the \"composable-example.ts\" for more details. The yaml.rest.automation parameter in the application.yml file tells the system the location of the rest.yaml configuration file. The default value is \"classpath:/rest.yaml\". The classpath:/ prefix means that the config file is available under the \"src/resources\" folder in your project. If you want the rest.yaml configuration file to be externalized to the local file system, you can use the file:/ prefix. e.g. \"file:/tmp/config/rest.yaml\". yaml.rest.automation: 'classpath:/rest.yaml' Defining a REST endpoint The \"rest\" section of the rest.yaml configuration file may contain one or more REST endpoints. A REST endpoint may look like this: - service: [\"hello.world\"] methods: ['GET', 'PUT', 'POST', 'HEAD', 'PATCH', 'DELETE'] url: \"/api/hello/world\" timeout: 10s cors: cors_1 headers: header_1 authentication: 'v1.api.auth' tracing: true In this example, the URL for the REST endpoint is \"/api/hello/world\" and it accepts a list of HTTP methods. When an HTTP request is sent to the URL, the HTTP event will be sent to the function declared with service route name \"hello.world\". The input event \"body\" will be an \"AsyncHttpRequest\" object. You can retrieve HTTP metadata such as method, url path, HTTP request headers from the object. The \"timeout\" value is the maximum time that REST endpoint will wait for a response from your function. If there is no response within the specified time interval, the user will receive an HTTP-408 timeout exception. The \"authentication\" tag is optional. If configured, the route name given in the authentication tag will be used. The input event will be delivered to a function with the authentication route name. In this example, it is \"v1.api.auth\". Your custom authentication function may look like this: export class DemoAuth implements Composable { @preload('v1.api.auth') initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { const req = new AsyncHttpRequest(evt.getBody() as object); const method = req.getMethod(); const url = req.getUrl(); log.info(`${method} ${url} authenticated`); // this is a demo so we approve all requests return true; } } Your authentication function can return a boolean value to indicate if the request should be accepted or rejected. Optionally, you can also return an EventEnvelope containing a boolean body and a set of key-values in the headers. If true, the system will send the HTTP request to the service. In this example, it is the \"hello.world\" function. If false, the user will receive an \"HTTP-401 Unauthorized\" exception. Optionally, you can use the authentication function to return some session information after authentication. For example, your authentication can forward the \"Authorization\" header of the incoming HTTP request to your organization's OAuth 2.0 Identity Provider for authentication. To return session information to the next function, the authentication function can return an EventEnvelope. It can set the session information as key-values in the response event headers. You can test this by visiting http://127.0.0.1:8086/api/hello/world to invoke the \"hello.world\" function. The console will print: INFO {\"trace\":{\"origin\":\"11efb0d8fcff4924b90aaf738deabed0\", \"id\":\"4dd5db2e64b54eef8746ab5fbb4489a3\",\"path\":\"GET /api/hello/world\", \"service\":\"v1.api.auth\",\"start\":\"2023-06-10T00:01:07.492Z\",\"success\":true, \"exec_time\":0.525,\"round_trip\":0.8,\"from\":\"http.request\"}} (handleEvent:tracer.js:27) INFO {\"trace\":{\"origin\":\"11efb0d8fcff4924b90aaf738deabed0\", \"id\":\"4dd5db2e64b54eef8746ab5fbb4489a3\",\"path\":\"GET /api/hello/world\", \"service\":\"hello.world\",\"start\":\"2023-06-10T00:01:07.495Z\",\"success\":true, \"exec_time\":0.478,\"round_trip\":1.238,\"from\":\"http.request\"}} (handleEvent:tracer.js:27) This illustrates that the HTTP request has been processed by the \"v1.api.auth\" function. The tracing parameter tells the system to turn on \"distributed tracing\". In the console log shown above, you see two lines of log from \"distributed trace\" showing that the HTTP request is processed by \"v1.api.auth\" and \"hello.world\" before returning result to the browser. The optional cors and headers sections point to the specific CORS and HEADERS sections respectively. CORS section For ease of development, you can define CORS headers using the CORS section like this. This is a convenient feature for development. For cloud native production system, it is most likely that CORS processing is done at the API gateway level. You can define different sets of CORS headers using different IDs. cors: - id: cors_1 options: - \"Access-Control-Allow-Origin: ${api.origin:*}\" - \"Access-Control-Allow-Methods: GET, DELETE, PUT, POST, PATCH, OPTIONS\" - \"Access-Control-Allow-Headers: Origin, Authorization, X-Session-Id, X-Correlation-Id, Accept, Content-Type, X-Requested-With\" - \"Access-Control-Max-Age: 86400\" headers: - \"Access-Control-Allow-Origin: ${api.origin:*}\" - \"Access-Control-Allow-Methods: GET, DELETE, PUT, POST, PATCH, OPTIONS\" - \"Access-Control-Allow-Headers: Origin, Authorization, X-Session-Id, X-Correlation-Id, Accept, Content-Type, X-Requested-With\" - \"Access-Control-Allow-Credentials: true\" HEADERS section The HEADERS section is used to do some simple transformation for HTTP request and response headers. You can add, keep or drop headers for HTTP request and response. Sample HEADERS section is shown below. headers: - id: header_1 request: # # headers to be inserted # add: [\"hello-world: nice\"] # # keep and drop are mutually exclusive where keep has precedence over drop # i.e. when keep is not empty, it will drop all headers except those to be kept # when keep is empty and drop is not, it will drop only the headers in the drop list # e.g. # keep: ['x-session-id', 'user-agent'] # drop: ['Upgrade-Insecure-Requests', 'cache-control', 'accept-encoding', 'host', 'connection'] # drop: ['Upgrade-Insecure-Requests', 'cache-control', 'accept-encoding', 'host', 'connection'] response: # # the system can filter the response headers set by a target service, # but it cannot remove any response headers set by the underlying servlet container. # However, you may override non-essential headers using the \"add\" directive. # i.e. don't touch essential headers such as content-length. # # keep: ['only_this_header_and_drop_all'] # drop: ['drop_only_these_headers', 'another_drop_header'] # # add: [\"server: mercury\"] # # You may want to add cache-control to disable browser and CDN caching. # add: [\"Cache-Control: no-cache, no-store\", \"Pragma: no-cache\", # \"Expires: Thu, 01 Jan 1970 00:00:00 GMT\"] # add: - \"Strict-Transport-Security: max-age=31536000\" - \"Cache-Control: no-cache, no-store\" - \"Pragma: no-cache\" - \"Expires: Thu, 01 Jan 1970 00:00:00 GMT\" Feature variation from the Java version In the Node.js version, the underlying HTTP server is Express. We have configured the bodyParser to render HTTP request body in this order: URL encoded parameters JSON text \"application/xml\" or content type starts with \"text/\" \"multipart/form-data\" for file upload all other types of content will be rendered as byte array (Buffer) with a payload limit of 2 MB Chapter-2 Home Chapter-4 Function Execution Strategies Table of Contents Event Script Syntax","title":"Chapter-3"},{"location":"guides/CHAPTER-3/#rest-automation","text":"The foundation library contains a built-in non-blocking HTTP server that you can use to create REST endpoints. Behind the curtain, it is using the Express server library, and we extend it to support dynamic creation of REST endpoints. The REST automation system is not a code generator. The REST endpoints in the rest.yaml file are handled by the system directly - \"Config is the code\". We will use the \"rest.yaml\" sample configuration file in the \"composable-example\" app to elaborate the configuration approach. The rest.yaml configuration has three sections: REST endpoint definition CORS header processing HTTP header transformation","title":"REST automation"},{"location":"guides/CHAPTER-3/#turn-on-the-rest-automation-engine","text":"REST automation is optional. To turn on REST automation, add the REST automation start up script in your main app: import { Logger, Platform, RestAutomation } from 'mercury-composable'; import { ComposableLoader } from '../preload/preload.js'; ... async function main() { // Load composable functions into memory and initialize configuration management ComposableLoader.initialize(); // keep the server running const platform = Platform.getInstance(); platform.runForever(); log.info('Composable application started'); } main(); Note : The class \"preload.ts\" is automatically generated when you do \"npm run preload\" or \"npm run build\". The compiled file is located in the \"dist/preload/preload.js\". Therefore, you must use an import statement for '../preload/preload.js'. Please review the \"composable-example.ts\" for more details. The yaml.rest.automation parameter in the application.yml file tells the system the location of the rest.yaml configuration file. The default value is \"classpath:/rest.yaml\". The classpath:/ prefix means that the config file is available under the \"src/resources\" folder in your project. If you want the rest.yaml configuration file to be externalized to the local file system, you can use the file:/ prefix. e.g. \"file:/tmp/config/rest.yaml\". yaml.rest.automation: 'classpath:/rest.yaml'","title":"Turn on the REST automation engine"},{"location":"guides/CHAPTER-3/#defining-a-rest-endpoint","text":"The \"rest\" section of the rest.yaml configuration file may contain one or more REST endpoints. A REST endpoint may look like this: - service: [\"hello.world\"] methods: ['GET', 'PUT', 'POST', 'HEAD', 'PATCH', 'DELETE'] url: \"/api/hello/world\" timeout: 10s cors: cors_1 headers: header_1 authentication: 'v1.api.auth' tracing: true In this example, the URL for the REST endpoint is \"/api/hello/world\" and it accepts a list of HTTP methods. When an HTTP request is sent to the URL, the HTTP event will be sent to the function declared with service route name \"hello.world\". The input event \"body\" will be an \"AsyncHttpRequest\" object. You can retrieve HTTP metadata such as method, url path, HTTP request headers from the object. The \"timeout\" value is the maximum time that REST endpoint will wait for a response from your function. If there is no response within the specified time interval, the user will receive an HTTP-408 timeout exception. The \"authentication\" tag is optional. If configured, the route name given in the authentication tag will be used. The input event will be delivered to a function with the authentication route name. In this example, it is \"v1.api.auth\". Your custom authentication function may look like this: export class DemoAuth implements Composable { @preload('v1.api.auth') initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { const req = new AsyncHttpRequest(evt.getBody() as object); const method = req.getMethod(); const url = req.getUrl(); log.info(`${method} ${url} authenticated`); // this is a demo so we approve all requests return true; } } Your authentication function can return a boolean value to indicate if the request should be accepted or rejected. Optionally, you can also return an EventEnvelope containing a boolean body and a set of key-values in the headers. If true, the system will send the HTTP request to the service. In this example, it is the \"hello.world\" function. If false, the user will receive an \"HTTP-401 Unauthorized\" exception. Optionally, you can use the authentication function to return some session information after authentication. For example, your authentication can forward the \"Authorization\" header of the incoming HTTP request to your organization's OAuth 2.0 Identity Provider for authentication. To return session information to the next function, the authentication function can return an EventEnvelope. It can set the session information as key-values in the response event headers. You can test this by visiting http://127.0.0.1:8086/api/hello/world to invoke the \"hello.world\" function. The console will print: INFO {\"trace\":{\"origin\":\"11efb0d8fcff4924b90aaf738deabed0\", \"id\":\"4dd5db2e64b54eef8746ab5fbb4489a3\",\"path\":\"GET /api/hello/world\", \"service\":\"v1.api.auth\",\"start\":\"2023-06-10T00:01:07.492Z\",\"success\":true, \"exec_time\":0.525,\"round_trip\":0.8,\"from\":\"http.request\"}} (handleEvent:tracer.js:27) INFO {\"trace\":{\"origin\":\"11efb0d8fcff4924b90aaf738deabed0\", \"id\":\"4dd5db2e64b54eef8746ab5fbb4489a3\",\"path\":\"GET /api/hello/world\", \"service\":\"hello.world\",\"start\":\"2023-06-10T00:01:07.495Z\",\"success\":true, \"exec_time\":0.478,\"round_trip\":1.238,\"from\":\"http.request\"}} (handleEvent:tracer.js:27) This illustrates that the HTTP request has been processed by the \"v1.api.auth\" function. The tracing parameter tells the system to turn on \"distributed tracing\". In the console log shown above, you see two lines of log from \"distributed trace\" showing that the HTTP request is processed by \"v1.api.auth\" and \"hello.world\" before returning result to the browser. The optional cors and headers sections point to the specific CORS and HEADERS sections respectively.","title":"Defining a REST endpoint"},{"location":"guides/CHAPTER-3/#cors-section","text":"For ease of development, you can define CORS headers using the CORS section like this. This is a convenient feature for development. For cloud native production system, it is most likely that CORS processing is done at the API gateway level. You can define different sets of CORS headers using different IDs. cors: - id: cors_1 options: - \"Access-Control-Allow-Origin: ${api.origin:*}\" - \"Access-Control-Allow-Methods: GET, DELETE, PUT, POST, PATCH, OPTIONS\" - \"Access-Control-Allow-Headers: Origin, Authorization, X-Session-Id, X-Correlation-Id, Accept, Content-Type, X-Requested-With\" - \"Access-Control-Max-Age: 86400\" headers: - \"Access-Control-Allow-Origin: ${api.origin:*}\" - \"Access-Control-Allow-Methods: GET, DELETE, PUT, POST, PATCH, OPTIONS\" - \"Access-Control-Allow-Headers: Origin, Authorization, X-Session-Id, X-Correlation-Id, Accept, Content-Type, X-Requested-With\" - \"Access-Control-Allow-Credentials: true\"","title":"CORS section"},{"location":"guides/CHAPTER-3/#headers-section","text":"The HEADERS section is used to do some simple transformation for HTTP request and response headers. You can add, keep or drop headers for HTTP request and response. Sample HEADERS section is shown below. headers: - id: header_1 request: # # headers to be inserted # add: [\"hello-world: nice\"] # # keep and drop are mutually exclusive where keep has precedence over drop # i.e. when keep is not empty, it will drop all headers except those to be kept # when keep is empty and drop is not, it will drop only the headers in the drop list # e.g. # keep: ['x-session-id', 'user-agent'] # drop: ['Upgrade-Insecure-Requests', 'cache-control', 'accept-encoding', 'host', 'connection'] # drop: ['Upgrade-Insecure-Requests', 'cache-control', 'accept-encoding', 'host', 'connection'] response: # # the system can filter the response headers set by a target service, # but it cannot remove any response headers set by the underlying servlet container. # However, you may override non-essential headers using the \"add\" directive. # i.e. don't touch essential headers such as content-length. # # keep: ['only_this_header_and_drop_all'] # drop: ['drop_only_these_headers', 'another_drop_header'] # # add: [\"server: mercury\"] # # You may want to add cache-control to disable browser and CDN caching. # add: [\"Cache-Control: no-cache, no-store\", \"Pragma: no-cache\", # \"Expires: Thu, 01 Jan 1970 00:00:00 GMT\"] # add: - \"Strict-Transport-Security: max-age=31536000\" - \"Cache-Control: no-cache, no-store\" - \"Pragma: no-cache\" - \"Expires: Thu, 01 Jan 1970 00:00:00 GMT\"","title":"HEADERS section"},{"location":"guides/CHAPTER-3/#feature-variation-from-the-java-version","text":"In the Node.js version, the underlying HTTP server is Express. We have configured the bodyParser to render HTTP request body in this order: URL encoded parameters JSON text \"application/xml\" or content type starts with \"text/\" \"multipart/form-data\" for file upload all other types of content will be rendered as byte array (Buffer) with a payload limit of 2 MB Chapter-2 Home Chapter-4 Function Execution Strategies Table of Contents Event Script Syntax","title":"Feature variation from the Java version"},{"location":"guides/CHAPTER-4/","text":"Event Script Syntax Event Script is a Domain Specific Language (DSL) that uses YAML to represent an end-to-end transaction flow. A transaction is a business use case, and the flow can be an API service, a batch job or a real-time transaction. Flow list This configuration file sits in the project \"resources\" project and contains a list of filenames. The default flow list is \"flows.yaml\" under the \"resources\" folder. It may look like this. flows: - 'get-profile.yml' - 'create-profile.yml' - 'delete-profile.yml' location: 'classpath:/flows/' The \"location\" parameter is optional. If present, you can tell the system to load the flow config files from another folder location. Multiple flow lists You can provide more than one flow list to your application and it can become very handy under different situations. For instance, to achieve better modularity in complex application, flows can be grouped to multiple categories based on development team's choice and these flows can be managed in multiple flow lists. Another great place to use multiple flow list is to include external libraries which contain pre-defined flow lists. The following example demonstrates that an application loads a list of flows defined in \"flows.yaml\" and additional flows defined in \"more-flows.yaml\" file of a composable library. yaml.flow.automation=classpath:/flows.yaml, classpath:/more-flows.yaml Writing new REST endpoint and function You can use the \"composable-example\" subproject as a template to write your own composable application. Before you update the code, please clean the project using npm run clean . This will remove the scanned list of composable functions in the ComposableLoader ( preload.ts ) class so that you can write your own functions. For each filename in the flows.yml, you should create a corresponding configuration file under the \"resources/flows\" folder. Let's write a new flow called \"greetings\". You can copy-n-paste the following into a file called \"greetings.yml\" under the \"resources/flows\" folder. flow: id: 'greetings' description: 'Simplest flow' ttl: 10s first.task: 'greeting.demo' tasks: - input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end In the application.properties, you can specify the following parameter: yaml.flow.automation=classpath:/flows.yaml and update the \"flows.yaml\" file in the resources folder as follows: flows: - 'get-profile.yml' - 'create-profile.yml' - 'delete-profile.yml' - 'greetings.yml' Then, you can add a new REST endpoint in the \"rest.yaml\" configuration file like this. - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/greetings/{user}\" flow: 'greetings' timeout: 10s cors: cors_1 headers: header_1 The above REST endpoint takes the path parameter \"user\". The task executor will map the path parameter to the input arguments (headers and body) in your function. Now you can write your new function with the named route \"greeting.demo\". Please copy-n-paste the following into a TypeScript class called \"greetings.ts\" and save it under the \"tasks\" folder in the source project. import { AppException, Composable, EventEnvelope, preload } from \"mercury-composable\"; export class Greetings implements Composable { @preload('greeting.demo', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { const input = evt.getBody() as object; if ('user' in input) { const result = {}; result['time'] = new Date().toISOString(); result['message'] = 'Welcome'; result['user'] = input['user']; return result; } else { throw new AppException(400, \"Missing path parameter 'user'\") } } } To test your new REST endpoint, flow configuration and function, please point your browser to http://127.0.0.1:8086/api/greetings/my_name You can replace \"my_name\" with your first name to see the response to the browser. Flow configuration syntax In your \"greetings.yml\" file above, you find the following key-values: flow.id - Each flow must have a unique flow ID. The flow ID is usually originated from a user facing endpoint through an event adapter. For example, you may write an adapter to listen to a cloud event in a serverless deployment. In The most common one is the HTTP adapter. The flow ID is originated from the \"rest.yaml\". The flow-engine will find the corresponding flow configuration and create a new flow instance to process the user request. flow.description - this describes the purpose of the flow flow.ttl - \"Time to live (TTL)\" timer for each flow. You can define the maximum time for a flow to finish processing. All events are delivered asynchronously and there is no timeout value for each event. The TTL defines the time budget for a complete end-to-end flow. Upon expiry, an unfinished flow will be aborted. You can use suffix \"s\" for seconds, \"m\" for minutes and \"h\" for hours. e.g. \"30s\" for 30 seconds. Note : When using the HTTP Flow Adapter, the flow.ttl value can be higher than the REST endpoint's timeout value. This would happen when one of your tasks in the event flow responds to the caller and the event flow continues to execute the rest of the flow. This type of task is called \"response\" task. first.task - this points to the route name of a function (aka \"task\") to which the flow engine will deliver the incoming event. The configuration file contains a list of task entries where each task is defined by \"input\", \"process\", \"output\" and \"execution\" type. In the above example, the execution type is \"end\", meaning that it is the end of a transaction and its result set will be delivered to the user. Underlying Event System The Event Script system uses platform-core as the event system where it encapsulates EventEmitter from the standard library of Node.js. The integration points are intentionally minimalist. For most use cases, the user application does not need to make any API calls to the underlying event system. REST automation and HTTP flow adapter The most common transaction entry point is a REST endpoint. The event flow may look like this: REQUEST -> \"http.request\" -> \"task.executor\" -> user defined tasks -> \"async.http.response\" -> RESPONSE REST automation is part of the platform-core library. It contains a non-blocking HTTP server that converts HTTP requests and responses into events. It routes an HTTP request event to the HTTP adapter if the \"flow\" tag is provided. In the following example, the REST endpoint definition is declared in a \"rest.yaml\" configuration. It will route the URI \"/api/decision\" to the HTTP flow adapter that exposes its service route name as \"http.flow.adapter\". rest: - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/decision?decision=_\" flow: 'decision-test' timeout: 10s cors: cors_1 headers: header_1 tracing: true The \"cors\" and \"headers\" sections are optional. When specified, the REST endpoint will insert CORS headers and HTTP request headers accordingly. For REST automation syntax, please refer to Chapter 3 The HTTP flow adapter maps the HTTP request dataset and the flow ID into a standard event envelope for delivery to the flow engine. The HTTP request dataset, addressable with the \"input.\" namespace, contains the following: Key Values method HTTP method uri URI path header HTTP headers cookie HTTP cookies path_parameter Path parameters if any query HTTP query parameters if any body HTTP request body if any stream input stream route ID if any ip remote IP address filename filename if request is a multipart file upload session authenticated session key-values if any For easy matching, please use lower case for headers, cookies, query and path parameters. Regular API uses JSON that will be converted to an object of key-values in the event's body. Task and its corresponding function Each task in a flow must have a corresponding function. You can assign a task name to the function using the preload annotation like this. @preload('greeting.demo', 10) initialize(): Composable { return this; } The \"route\" in the preload annotation is the task name. The concurrency number define the maximum number of \"workers\" that the function can handle concurrently. The system is designed to be reactive and the function does not consume memory and CPU resources until an event arrives. Unique task naming Composable functions are designed to be reusable. By changing some input data mapping to feed different parameters and payload, your function can behave differently. Therefore, it is quite common to use the same function (i.e. the process parameter) more than once in a single event flow. When a task is not named, the \"process\" parameter is used to name the task. Since each task must have a unique name for event routing, we cannot use the same \"process\" name more than once in an event flow. To handle this use case, you can create unique names for the same function using the name parameter like this: flow: id: 'greetings' description: 'Simplest flow' ttl: 10s first.task: 'my.first.task' tasks: - name: 'my.first.task' input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: sequential next: - 'another.task' The above event flow configuration uses \"my.first.task\" as a named route for \"greeting.demo\" by adding the \"name\" parameter to the composable function. Note : The Event Manager performs event choreography using the unique task name. Therefore, when the \"process\" name for the function is not unique, you must create unique task \"names\" for the same function to ensure correct routing. Hierarchy of flows As shown in Figure 1, you can run one or more sub-flows inside a primary flow. Figure 1 - Hierarchy of flows To do this, you can use the flow protocol identifier ( flow:// ) to indicate that the task is a flow. For example, when running the following task, \"flow://my-sub-flow\" will be executed like a regular task. tasks: - input: - 'input.path_parameter.user -> header.user' - 'input.body -> body' process: 'flow://my-sub-flow' output: - 'result -> model.json' description: 'Execute a sub-flow' execution: sequential next: - 'my.next.function' If the sub-flow is not available, the system will throw an error stating that it is not found. Hierarchy of flows would reduce the complexity of a single flow configuration file. The \"time-to-live (TTL)\" value of the parent flow should be set to a value that covers the complete flow including the time used in the sub-flows. In the input/output data mapping sections, the configuration management system provides a parent state machine using the namespace model.parent. to be shared by the primary flow and all sub-flows that are instantiated from it. Just like a task, a subflow has \"input\" and \"output\". You can map data to the \"input\" of a subflow using the namespaces \"body\" and \"header\" where they are maps of key-values. Inside a task of the subflow, the body and header namespaces can be accessed for their key-values like this: - input: - 'input.header.user -> header.user' - 'input.body -> *' process: 'first.task.in.subflow' output: - 'result -> model.parent.subflow_result' description: 'Execute a task in a subflow' execution: end Since the parent flow and subflows has a shared state machine, passing \"body\" and \"header\" key-values to the \"input\" of a subflow is optional. You can pass key-values between the parent and subflows using the shared state machine easily. Note : The namespace model.root. is an alias of model.parent. This would reduce ambiguity if you prefer to use \"root\" referring to the parent flow that creates one or more subflows. Tasks and data mapping All tasks for a flow are defined in the \"tasks\" section. Input/Output data mapping A function is self-contained. This modularity reduces application complexity because the developer only needs interface contract details for a specific function. To handle this level of modularity, the system provides configurable input/output data mapping. Namespaces for I/O data mapping Type Keyword and/or namespace LHS / RHS Mappings Flow input dataset input. left input Flow output dataset output. right output Function input body no namespace required right input Function input or output headers header or header. both I/O Function output result set result. left output Function output status code status left output Decision value decision right output State machine dataset model. both I/O Parent state machine dataset model.parent. both I/O External state machine key-value ext: right I/O For state machine (model and model.parent namespaces), the system prohibits access to the whole namespace. You should only access specific key-values in the model or model.parent namespaces. The namespace model.parent. is shared by the primary flow and all sub-flows that are instantiated from it. The external state machine namespace uses the namespace ext: to indicate that the key-value is external. Constants for input data mapping Type Keyword for the left-hand-side argument String text(example_value) Integer int(number) Long long(number) Float float(number) Double double(number) Boolean boolean(true or false) Map map(k1=v1, k2=v2) map(base.config.parameter) File file(text:file_path) File file(binary:file_path) File file(json:file_path) Classpath classpath(text:file_path) Classpath classpath(binary:file_path) Classpath classpath(json:file_path) For input data mapping, the \"file\" constant type is used to load some file content as an argument of a user function. You can tell the system to render the file as \"text\", \"binary\" or \"json\". Similarly, the \"classpath\" constant type refers to static file in the application source code's \"resources\" folder. When file type mapping is \"json\", the file content will be rendered as a Map or a List from a JSON string. The \"map\" constant type is used for two purposes: 1. Map of key-values The following example illustrates creation of a map of key-values. In the first entry, a map of 2 key-values is set as the input argument \"myMap\" of a user function. In the second entry, the map's values are retrieved from the key \"some.key\" in base configuration and the environment variable \"ENV_VAR_ONE\". 'map(k1=v1, k2=v2) -> myMap' 'map(k1=${some.key}, k2=${ENV_VAR_ONE}) -> myMap' Note : The comma character is used as a separator for each key-value pair. If the value contains a comma, the system cannot parse the key-values correctly. In this case, please use the 2nd method below. 2. Mapping values from application.yml The following input data mapping sets the value of \"my.key\" from the application.yml base configuration file to the input argument \"myKey\" of a user function. 'map(my.key) -> myKey' The \"map(my.key)\" would set a primitive value (text, integer, float, boolean), a JSON object of key-values or an array of values. Special content type for output data mapping Type Keyword for the right-hand-side argument File file(file_path) File file(append:file_path) For output data mapping, the \"file\" content type is used to save some data from the output of a user function to a file in the local file system. If the left-hand-side (LHS) resolved value is null, the file in the RHS will be deleted. This allows you to clean up temporary files before your flow finishes. An optional prefix \"append\" may be used to tell the system to append file content instead of overwriting it. Decision value The \"decision\" keyword applies to \"right hand side\" of output data mapping statement in a decision task only (See \"Decision\" in the task section). Each flow has its own input and output Each function has its input headers, input body and output result set. Optionally, a function can return an EventEnvelope object to hold its result set in the \"body\", a \"status\" code and one or more header key-values. Since each function is stateless, a state machine (with namespace model. ) is available as a temporary memory store for transaction states that can be passed from one task to another. All variables are addressable using the standard dot-bracket convention. For example, \"hello.world\" will retrieve the value 100 from this data structure: { \"hello\": { \"world\": 100 } } and \"numbers[1]\" will retrieve the value 200 below: { \"numbers\": [100, 200] } The assignment is done using the assignment ( -> ) syntax. In the following example, the HTTP input query parameter 'amount' is passed as input body argument 'amount' to the task 'simple.decision'. The result (function \"return value\") from the task will be mapped to the special \"decision\" variable that the flow engine will evaluate. This assumes the result is a boolean or numeric value. The \"decision\" value is also saved to the state machine ( model ) for subsequent tasks to evaluate. - input: - 'input.query.amount -> amount' process: 'simple.decision' output: - 'result -> decision' - 'result -> model.decision' Environment variables You can use the standard ${ENV_VAR:default} syntax to resolve environment variables or parameters from the application.properties. Runtime model variables To use a runtime model variable value as a key or constant, you can use the {model.variable_name} syntax. For example, - input: - 'text(wonderful day) -> model.world' - 'text(world) -> model.pointer' - 'model.{model.pointer} -> value1' - 'text(new {model.pointer}) -> value2' - 'text(keep {this}/{one} unchanged) -> value3' process: 'demo.function' model.{model.pointer} is resolved as model.world , giving value1 = wonderful day and value2 = new world . The text inside a set of brackets that is not a model variable will be kept unchanged, thus value3 = keep {this}/{one} unchanged The use of string substitution is subject to event script syntax validation. Therefore, When this feature is used in the left-hand-side of an input data mapping, it can be used to substitute a constant or a segment of a key in the input. and model. namespaces. The above example shows the use of the model namespace in model.{model.pointer} -> value1 . Similarly, when used in the left-hand-side of an output data mapping, it can be used to substitute a constant or a segment of a key in the input. , model. , header. or result. namespaces. When used in the right-hand-side of an input data mapping, namespace is optional because it may map as an argument to a task. When used in the right-hand-side of an output data mapping, it can be used to substitute a model. namespace, file( output, flow output. namespace or an external state machine ext: namespace. Important : For security reason, the key inside the brackets must be a model variable. The resolved value from a model variable must be either text or number. Otherwise, it will be converted to a value of \"null\". For simplicity, nested substitution is not allowed. i.e. model.{model.{model.n}} or model.{model.list[model.n]} will be rejected. If the bracketed text is not a model variable, the brackets and the enclosed text will be kept unchanged. Handling arrays in a dataset An array of data elements is expressed as a list. { \"numbers\": [100, 200] } As discussed earlier, an array element can be retrieved using a number as index. For example, to take the second element with value 200 above, you can use this data mapping like this: 'input.body.numbers[1] -> second_number' In the above example, it is an \"input data mapping\". It maps the second element of value 200 as the input argument \"second_number\" to a composable function. For-loop feature is supported in pipeline in an event flow. It would be convenient to use the iterator value as an index to map an input argument. We can do something like this: 'input.body.numbers[model.n] -> second_number' where model.n is the iterator value in a for-loop. Similarly, it is possible to do output data mapping. For example, 'result.computed -> model.list[model.n]' To address an array element, we can use a number or a \"dynamic model variable\" as an index. The model variable must resolved to a number. Note : There are some consideration when using a dynamic model variable as an index. The left-hand-side of a data mapping is a GET operation. The right-hand-side is a SET operation. If the model variable is non-numeric, the GET operation will return null and SET operation will throw exception. To avoid setting an arbitrary high index, the size of the index is limited by the parameter \"max.model.array.size\" in application.yml Append an element to an array An empty array index in the right hand side tells the system to append an element to an array. For example, the value resolved from the left hand side \"result.item1\" and \"result.item2\" will be appended to the model.items array in the state machine. - 'result.item1 -> model.items[]' - 'result.item2 -> model.items[]' If model.items does not exist, the first element will be set as array index \"0\". Therefore, the above output data mapping statements are the same as: - 'result.item1 -> model.items[0]' - 'result.item2 -> model.items[1]' Simple type matching and conversion Event script's state machine supports simple type matching and conversion for the model namespace. This \"impedance matching\" feature allows us to accommodate minor interface contract changes without refactoring business logic of a user function. This is supported in both the left-hand-side and right-hand-side of both input and output data mappings. For the left-hand-side, the state machine's model value is matched or converted to the target data type before setting the value of the right-hand-side. The state machine values are unchanged. For the right-hand-side, the matched or converted value is applied to the state machine's model value. The syntax is model.somekey:type where \"type\" is one of the following: Type Match value as Example text text string model.someKey:text binary byte array model.someKey:binary int integer or -1 if not numeric model.someKey:int long long or -1 if not numeric model.someKey:long float float or -1 if not numeric model.someKey:float double double or -1 if not numeric model.someKey:double boolean true or false model.someKey:boolean boolean(value) true if value matches model.someKey:boolean(positive) boolean(value=true) true if value matches model.someKey:boolean(positive=true) boolean(value=false) false if value matches model.someKey:boolean(negative=false) and(model.key) boolean AND of 2 model keys model.someKey:and(model.another) or(model.key) boolean OR of 2 model keys model.someKey:or(model.another) !model.key negate of a model variable !model.someKey substring(start, end) extract a substring model.someKey:substring(0, 5) substring(start) extract a substring model.someKey:substring(5) concat(vars...) concat model variables & text model.a:concat(model.b, text(!)) b64 byte-array to Base64 text model.someKey:b64 b64 Base64 text to byte-array model.someKey:b64 uuid generated UUID-4 value model.unique_id:uuid length length of model list variable model.someList:length For Base64 type matching, it handles two symmetrical use cases. If the key-value is a text string, the system would assume it is a Base64 text string and convert it to a byte-array. If the key-value is a byte-array, the system will encode it into a Base64 text string. For uuid type matching, the system will ignore the value of the model variable in the left hand side because UUID is a generated value. When using it in the right hand side, the model variable will be updated with a generated UUID value accordingly. For simplicity of syntax, each type matching command is a single operation. For more complex operation such as multiple AND, OR and NEGATE operators, you can configure multiple steps of operation. For string concatenation, you may concat a model variable with one or more model variables and text constants. A more convenient alternative to string concatenation is the use of \"runtime model variables\". You can replace the \"concat\" method with \"runtime model variable\" method as follows: # assuming the bearer token value is in model.token - 'text(Bearer ) -> model.bearer' - 'model.bearer:concat(model.token) -> authorization' # the above is the same as - 'text(Bearer {model.token}) -> authorization' An interesting use case is a simple decision task using the built-in no-op function. For boolean with value matching, you can test if the key-value in the left-hand-side is a null value. For example, when a control file for the application is not available, your application will switch to run in dev mode. A sample task may look like this: first.task: 'no.op' tasks: - input: - 'file(binary:/tmp/interesting-config-file) -> model.is-local:boolean(null=true)' process: 'no.op' output: - 'model.is-local -> decision' execution: decision next: - 'start.in.dev.mode' - 'start.in.cloud' Another use case is type conversion for HTTP path parameter which is always a text string. If your composable function requires a path parameter to be accepted as an integer, you can do this: - input: - 'input.path_parameter.userid -> model.userid:int' - 'model.userid -> userid' The above input data mapping example illustrates the use of a model variable to convert a text parameter into an integer. Note that if the path parameter is not numeric, the converted value will be -1. Note : The system only supports \"type matching modifier\" in the model namespace because of the design principle of data immutability. The model is a state machine for a flow instance. As a temporary store, we can use it for this purpose without side effect that the user application would accidentally modify a value of the flow's input. Convenient data mapping using model variable To address the common use case of using a model variable as an intermediate value, the system supports the following formats for input data mapping and output data mapping. // 2-part data mapping format LHS -> RHS // 3-part data mapping format LHS -> model.variable -> RHS For the 2-part data mapping format, there are left-hand-side and right-hand-side where the value retrieved from the left-hand-side variable is mapped to the right-hand-side. The 3-part data mapping allows us to use a model variable as an intermediate for simple type matching. In the previous example, it uses two entries to convert a HTTP path parameter from a text string to a number and set the number as input argument. The configuration syntax can be simplified as follows: - input: - 'input.path_parameter.userid -> model.userid:int -> userid' The above 3-part data mapping entry will be expanded into two entries internally. This extra processing is done at the \"CompileFlows\" step and thus there is no impact to the task execution speed. Please note that the 3-part data mapping format is not supported when the left-hand-side is a text constant. It is because a text constant may contain any special characters including the mapping signature -> . Metadata for each flow instance For each flow instance, the state machine in the \"model\" namespace provides the following metadata that you can use in the input/output data mapping. For example, you can set this for an exception handler to log additional information. Type Keyword Comment Flow ID model.flow The ID of the event flow config Trace ID model.trace Optional traceId when tracing is turned on Correlation ID model.cid Correlation ID of the inbound request Special handling for header When function input keyword header is specified in the \"right hand side\" of an input data mapping statement, it refers to the input event envelope's headers. Therefore, it assumes the \"left hand side\" to resolve into a JSON object of key-values. Otherwise, it will reject the input data mapping statement with an error like this: Invalid input mapping 'text(ok) -> header', expect: JSON, Actual: string When function input namespace header. is used, the system will map the value resolved from the \"left hand side\" statement into the specific header. For example, the input data mapping statement text(ok) -> header.demo will set \"demo=ok\" into the input event envelope's headers. When function output keyword header is specified in the \"left hand side\" of an output data mapping statement, it will resolve as a JSON object of key-values from the function output event envelope's headers. Similarly, when function output namespace header. is used, the system will resolve the value from a specific key of the function output event envelope's headers. Function input and output To support flexible input data mapping, the input to a function must be either a JSON object of key-values. However, the output (i.e. result set) of a function can be JSON object or primitive (boolean, string, number, Buffer, etc.). Your function must implement the Composable interface to configure input and output. Since a data structure is passed to your function's input argument as key-values, you may create a class to deserialize the data structure. To tell the system that your function is expecting input as a JSON object, you can use the special notation * on the right hand side. For example, the following entry tells the system to set the value in \"model.dataset\" as a JSON payload. - input: - 'model.dataset -> *' Note : If the value from the left hand side is not a JSON, the system will ignore the input mapping command and print out an error message in the application log. Setting function input headers When function input body is used to hold a JSON object, we may use function input headers to pass other arguments to the function without changing the data structure of a user defined JSON object of key-values. In the following example, the HTTP query parameter \"userid\" will be mapped to the function input header key \"user\" and the HTTP request body will be mapped to the function input body. - input: - 'input.query.userid -> header.user' - 'input.body -> *' process: 'my.user.function' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' Task types Decision task A decision task makes decision to select the next task to execute. It has the tag execution=decision . In the output data mapping section, it must map the corresponding result set or its key-value to the decision object. The \"next\" tag contains a list of tasks to be selected based on the decision value. If decision value is boolean, a true value will select the first task. Otherwise, the second task will be selected. If decision value is an integer, the number should start from 1 where the corresponding \"next\" task will be selected. tasks: - input: - 'input.query.decision -> decision' process: 'simple.decision' output: - 'result -> model.decision' - 'result -> decision' description: 'Simple decision test' execution: decision next: - 'decision.case.one' - 'decision.case.two' Response task A response task will provide result set as a flow output or \"response\". A response task allows the flow to respond to the user or caller immediately and then move on to the next task asynchronously. For example, telling the user that it has accepted a request and then moving on to process the request that may take longer time to run. A response task has the tag execution=response and a \"next\" task. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' - 'result -> output.body' description: 'Pass a JSON to another task' execution: response next: - 'sequential.two' End task An end task indicates that it is the last task of the transaction processing in a flow. If the flow has not executed a response task, the end task will generate the response. Response is defined by output data mapping. This task has the tag execution=end . For example, the greeting task in the unit tests is an end task. - input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end Sequential task Upon completion of a sequential task, the next task will be executed. This task has the tag execution=sequential . In the following example, sequential.two will be executed after sequential.one . tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' description: 'Pass a JSON to another task' execution: sequential next: - 'sequential.two' Parallel task Upon completion of a parallel task, all tasks in the \"next\" task list will be executed in parallel. This task has the tag execution=parallel . In this example, parallel.one and parallel.two will run after begin.parallel.test tasks: - input: - 'int(2) -> count' process: 'begin.parallel.test' output: [] description: 'Setup counter for two parallel tasks' execution: parallel next: - 'parallel.one' - 'parallel.two' Fork-n-join task Fork-n-join is a parallel processing pattern. A \"fork\" task will execute multiple \"next\" tasks in parallel and then wait for the result sets before running the \"join\" task. This task has the tag execution=fork . It must have a list of \"next\" tasks and a \"join\" task. It may look like this: tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' description: 'Pass a JSON to another task' execution: fork next: - 'echo.one' - 'echo.two' join: 'join.task' Dynamic fork-n-join task A special version of the fork-n-join pattern is called dynamic fork-n-join which refers to parallel processing of multiple instances of the same \"next\" task for each element in a list. For example, you have a list of 100 elements in an incoming request and each element would be processed by the same backend service. You want to process the 100 elements in parallel by multiple instances of a service wraper that connects to the backend service. The use case can be configured like this: tasks: - input: - 'input.elements -> elements' process: 'data.validation' output: - 'result.elements -> model.elements' description: 'Validate list of elements' execution: fork source: 'model.elements' next: - 'element.processor' join: 'join.task' - name: 'element.processor' input: - 'model.elements.ITEM -> item' - 'model.elements.INDEX -> index' process: 'v1.element.processor' output: [] description: 'Hello world' execution: sink To handle this special use case, you can add a source parameter in the fork task. The \"source\" parameter tells the system which model variable holds the list of elements. You should only configure a single \"next\" task. The system will spin up parallel instances of the next task to handle each element from the model variable containing the list. In the input data mapping section, there are two special suffixes .ITEM and .INDEX . The system will iterate the list of elements and spin up an instance of the \"next\" task to retrieve the element (item) and index of the element in the list. The two special suffixes are relevant only when adding to the model variable configured in the \"source\" parameter. Important : The model variables with special suffixes '.ITEM' and '.INDEX' are virtual objects for the purpose of mapping as input arguments to a task. They cannot be used as regular model variables. Dynamic fork-n-join is designed to execute the same task for a list of elements in parallel. It does not support subflow. i.e. the \"process\" tag of the \"next\" task cannot be a subflow. Sink task A sink task is a task without any next tasks. Sink tasks are used by fork-n-join and pipeline tasks as reusable modules. This task has the tag execution=sink . - input: - 'text(hello-world-two) -> key2' process: 'echo.two' output: - 'result.key2 -> model.key2' description: 'Hello world' execution: sink Pipeline feature Pipeline is an advanced feature of Event Script. Pipeline task A pipeline is a list of tasks that will be executed orderly within the current task. When the pipeline is done, the system will execute the \"next\" task. This task has the tag execution=pipeline . tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' description: 'Pass a JSON to another task' execution: pipeline pipeline: - 'echo.one' - 'echo.two' next: - 'echo.three' Some special uses of pipelines include \"for/while-loop\" and \"continue/break\" features. Simple for-loop In the following example, the loop.statement contains a for-loop that uses a variable in the state machine to evaluate the loop. In this example, the pipeline will be executed three times before passing control to the \"next\" task. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' description: 'Pass a JSON to another task' execution: pipeline loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' pipeline: - 'echo.one' - 'echo.two' - 'echo.three' next: - 'echo.four' Simple while loop The loop.statement may use a \"while loop\" syntax like this: loop: statement: 'while (model.running)' To exit the above while loop, one of the functions in the pipeline should return a boolean \"false\" value with output \"data mapping\" to the model.running variable. For loop with break/continue decision In the following example, the system will evaluate if the model.quit variable is true. If yes, the break or continue condition will be executed. The state variable is obtained after output data mapping and any task in the pipeline can set a key-value for mapping into the state variable. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' description: 'Pass a JSON to another task' execution: pipeline loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' condition: 'if (model.quit) break' pipeline: - 'echo.one' - 'echo.two' - 'echo.three' next: - 'echo.four' Note that the \"condition\" parameter can be a single condition or a list of conditions. In the following example, the system will evaluate both the model.quit and model.jump values. loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' condition: - 'if (model.quit) break' - 'if (model.jump) break' Handling exception You can define exception handler at the top level or at the task level. Exception is said to occur when a user function throws exception or returns an EventEnvelope object with a status code equals to or larger than 400. The event status uses the same numbering scheme as HTTP exception status code. Therefore, status code less than 400 is not considered an exception. Top-level exception handler Top-level exception handler is a \"catch-all\" handler. You can define it like this: flow: id: 'greetings' description: 'Simplest flow of one task' ttl: 10s exception: 'v1.my.exception.handler' In this example, the v1.my.exception.handler should point to a corresponding exception handler that you provide. The following input arguments will be delivered to your function when exception happens. Key Description status Exception status code message Error message stack Stack trace in a text string The exception handler function can be an \"end\" task to abort the transaction or a decision task to take care of the exception. For example, the exception handler can be a \"circuit-breaker\" to retry a request. Note : for efficiency, stack trace transport is limited to the first 10 lines. Task-level exception handler You can attach an exception handler to a task. One typical use is the \"circuit breaker\" pattern. In the following example, the user function \"breakable.function\" may throw an exception for some error condition. The exception will be caught by the \"v1.circuit.breaker\" function. - input: - 'input.path_parameter.accept -> accept' - 'model.attempt -> attempt' process: 'breakable.function' output: - 'int(0) -> model.attempt' - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'This demo function will break until the \"accept\" number is reached' execution: end exception: 'v1.circuit.breaker' The configuration for the circuit breaker function may look like this: - input: - 'model.attempt -> attempt' - 'int(2) -> max_attempts' - 'error.code -> status' - 'error.message -> message' - 'error.stack -> stack' process: 'v1.circuit.breaker' output: - 'result.attempt -> model.attempt' - 'result.decision -> decision' - 'result.status -> model.status' - 'result.message -> model.message' description: 'Just a demo circuit breaker' execution: decision next: - 'breakable.function' - 'abort.request' An exception handler will be provided with the \"error\" object that contains error code, error message and a stack trace. The exception handler can inspect the error object to make decision of the next step. For circuit breaker, we can keep the number of retry attempts in the state machine under \"model.attempt\" or any key name that you prefer. In the above example, it sets an integer constant of 2 for the maximum attempts. The circuit breaker can then evaluate if the number of attempts is less than the maximum attempts. If yes, it will return a decision of \"true\" value to tell the system to route to the \"breakable.function\" again. Otherwise, it will return a decision of \"false\" value to abort the request. A more sophisticated circuit breaker may be configured with \"alternative execution paths\" depending on the error status and stack trace. In this case, the decision value can be a number from 1 to n that corresponds to the \"next\" task list. Exception handlers may be used in both queries and transactions. For a complex transaction, the exception handler may implement database rollback logic or recovery mechanism. Best practice When a task-level exception handler throws exception, it will be caught by the top-level exception handler, if any. A top-level exception handler should not throw exception. Otherwise it may go into an exception loop. Therefore, we recommend that an exception handler should return regular result set in a JSON object. An example of task-level exception handler is shown in the \"hello-exception.ts\" class in the \"task\" folder where it set the status code in the result set so that the system can map the status code from the result set to the next task or to the HTTP output status code. Advanced features No-operation function A convenient no-operation function with the route name no.op is available. It can be used when you want to perform some input/output data mapping without executing any business logic. Generic resilience handler function Another useful built-in function is a resilience handler with the route name resilience.handler . It is a generic resilience handler. It will retry, abort, use an alternative path or exercise a brief backoff. Figure 2 - Resilience Handler The following parameters (input data mapping) define behavior for the handler: max_attempts - when the handler has used all the attempts, it will abort. attempt - this tells the handler how many attempts it has tried status - you should map the error status code in this field message - you should map the error message in this field alternative - the optional codes and range of status codes to tell the handler to reroute delay - the delay in milliseconds before exercising retry or reroute. Minimum value is 10 ms. Delay is skipped for the first retry. This slight delay is a protection mechanism. Optional backoff behavior: cumulative - the total number of failures since last success or backoff reset if any backoff - the time of a backoff period (epoch milliseconds) if any backoff_trigger - the total number of failures that triggers a backoff backoff_seconds - the time to backoff after an abort has occurred. During this period, It will abort without updating attempt. This avoids overwhelming the target service that may result in recovery storm. Return value (output data mapping): result.attempt - the handler will clear or increment this counter result.cumulative - the handler will clear or increment this counter. Not set if \"backoff_trigger\" is not given in input. result.decision - 1, 2 or 3 where 1=retry, 2=abort, 3=reroute that corresponds to the next tasks result.status - the status code that the handler aborts the retry or reroute. Not set if retry or reroute. result.message - the reason that the handler aborts the retry or reroute. Not set if retry or reroute. result.backoff - the time of a backoff period (epoch milliseconds). Not set if not in backoff mode. Note : \"result.attempt\" should be saved in the state machine with the \"model.\" namespace. \"result.cumulative\" and \"result.backoff\" should be saved in the temporary file system or an external state machine. For more details, please refer to the event script resilience-demo.yml in the test resources folder and the unit test will handle backoff, retry and abort in resilience handler under the flow.test.ts class. Extract of the task configuration for the resilience handler is shown as follows. In the following example, \"my.task\" is the function that is configured with the 'resilience.handler' as an exception handler. The input data mapping tells the handler to enter into \"backoff\" period when the cumulative failure count reaches the \"backoff_trigger\" threshold of 3. After that, all requests will be aborted until the backoff period expires. - input: - 'error.code -> status' - 'error.message -> message' - 'model.attempt -> attempt' - 'int(10) -> max_attempts' - 'text(401, 403-404) -> alternative' - 'file(text:/tmp/resilience/cumulative) -> cumulative' - 'file(text:/tmp/resilience/backoff) -> backoff' - 'int(3) -> backoff_trigger' - 'int(2) -> backoff_seconds' - 'int(500) -> delay' process: 'resilience.handler' output: - 'result.status -> model.status' - 'result.message -> model.message' - 'result.attempt -> model.attempt' - 'result.decision -> decision' - 'result.backoff -> file(/tmp/resilience/backoff)' - 'result.cumulative -> file(/tmp/resilience/cumulative)' description: 'Resilience handler with alternative path and backoff features' execution: decision next: - 'my.task' - 'abort.request' - 'alternative.task' Note : When the \"backoff\" feature is enabled, you should configure the resilience handler as a gatekeeper to protect your user function. This allows the system to abort requests during the backoff period. You may also use this resilience handler as a starting point to write your own exception handler for more complex recovery use cases. External state machine The in-memory state machine is created for each query or transaction flow and it is temporal. For complex transactions or long running work flows, you would typically want to externalize some transaction states to a persistent store such as a distributed cache system or a high performance key-value data store. In these use cases, you can implement an external state machine function and configure it in a flow. Below is an example from a unit test. When you externalize a key-value to an external state machine, you must configure the route name (aka level-3 functional topic) of the external state machine. Note : Passing a null value to a key of an external state machine means \"removal\". external.state.machine: 'v1.ext.state.machine' tasks: - input: # A function can call an external state machine using input or output mapping. # In this example, it calls external state machine from input data mapping. - 'input.path_parameter.user -> ext:/${app.id}/user' - 'input.body -> model.body' # demonstrate saving constant to state machine and remove it using model.none - 'text(world) -> ext:hello' - 'model.none -> ext:hello' process: 'no.op' output: - 'text(application/json) -> output.header.content-type' # It calls external state machine again from output data mapping - 'input.body -> ext:/${app.id}/body' - 'input.body -> output.body' - 'text(message) -> ext:test' - 'model.none -> ext:test' description: 'Hello World' execution: end The \"external.state.machine\" parameter is optional. When present, the system will send a key-value from the current flow instance's state machine to the function implementing the external state machine. The system uses the \"ext:\" namespace to externalize a state machine's key-value. Note : The delivery of key-values to the external state machine is asynchronous. Therefore, please assume eventual consistency. You should implement a user function as the external state machine. The input interface contract to the external state machine for saving a key-value is: header.type = 'put' header.key = key body.data = value Your function should save the input key-value to a persistent store. In another flow that requires the key-value, you can add an initial task to retrieve from the persistent store and do \"output data mapping\" to save to the in-memory state machine so that your transaction flow can use the persisted key-values to continue processing. In the unit tests of the event-script-engine subproject, these two flows work together: externalize-put-key-value externalize-get-key-value IMPORTANT : Events to an external state machine are delivered asynchronously. If you want to guarantee message sequencing, please do not set the \"instances\" parameter in the preLoad annotation. To illustrate a minimalist implementation, below is an example of an external state machine in the event-script-engine's unit test section. class ExtStateMachine implements Composable { initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { if (!evt.getHeader(KEY)) { throw new Error(\"Missing key in headers\"); } const type = evt.getHeader(TYPE); const key = evt.getHeader(KEY); const input = evt.getBody(); if (PUT == type && input instanceof Object && 'data' in input) { var data = input['data']; if (data) { log.info(`Saving ${key} to store`); store[key] = data; return true; } } if (GET == type) { const v = store[key]; if (v) { log.info(`Retrieve ${key} from store`); return v; } else { return null; } } if (REMOVE == type) { if (key in store) { delete store[key]; log.info(`Removed ${key} from store`); return true; } else { return false; } } return false; } } For more sophisticated operation, you may also configure the external state machine as a \"flow\" like this: external.state.machine: 'flow://ext-state-machine' You can then define the flow for \"ext-state-machine\" like this: flow: id: 'ext-state-machine' description: 'Flow to execute an external state machine' ttl: 10s first.task: 'v1.ext.state.machine' tasks: - input: - 'input.header.key -> header.key' - 'input.header.type -> header.type' - 'input.body.data -> data' process: 'v1.ext.state.machine' output: [] description: 'Execute external state machine' execution: end Note : By definition, external state machine flow is outside the scope of the calling flow. Future task scheduling You may add a \u201cdelay\u201d tag in a task so that it will be executed later. This feature is usually used for unit tests or \"future task scheduling\". Since the system is event-driven and non-blocking, the delay is simulated by event scheduling. It does not block the processing flow. Type Value Example Fixed delay Milliseconds delay: '1000 ms' Variable delay State machine variable delay: model.delay Note that the \"ms\" suffix is optional for documentation purpose. It denotes milliseconds if present. When delay is set to a state variable that its value is not configured by a prior data mapping, the delay command will be ignored. An example task that has an artificial delay of 2 seconds: tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.ex -> exception' - 'text(hello world) -> greeting' process: 'greeting.test' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end delay: '2000 ms' Using Worker Threads In some use cases, there is a need to use legacy libraries and open sources that we cannot refactor into composable functions. We have provided a worked example to encapsulate any legacy library running in a worker thread to behave as a composable function. This allows you to accelerate integration with existing enterprise software assets for production use so that you can gradually and orderly refactor them into composable functions. Please review the composable-worker.ts source file to examine how it encapsulates a worker thread: Composable example repository The composable-worker.ts class may be used as a template: ComposableWorker An extract of the source code is shown below. export class ComposableWorker implements Composable { @preload('composable.worker.demo', 5, true, true) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // deferred startup until triggered by autostart or your own setup task if (!loaded && 'start' == evt.getHeader('type')) { workerBridge(); } if (worker) { // sending the original event to the worker to preserve metadata for tracing and correlation sendEventToWorker(evt); } return null; } } The workerBridge will start a new worker thread and the sendEventToWorker method will forward an incoming event to the worker thread. After processing the request, the worker thread would send acknowledgement or response back to the main thread in the composable function that will forward it to the caller accordingly. In the ComposableWorker sample code, it also illustrates the technique to pass environment variables, application runtime arguments and the Composable configuration system from the main thread in your Composable application to the legacy code in the worker thread. A demo endpoint \"GET /api/worker/demo\" or \"POST /api/worker/demo\" is available to demonstrate the ComposableWorker example. Using using the POST method, please set content-type to 'application/json' and provide HTTP request payload as a JSON string. Please use the worked example as a template to write your own composable function to encapsulate some legacy code or open sources that you have no direct control. Chapter-3 Home Chapter-5 REST Automation Table of Contents Build, Test and Deploy","title":"Chapter-4"},{"location":"guides/CHAPTER-4/#event-script-syntax","text":"Event Script is a Domain Specific Language (DSL) that uses YAML to represent an end-to-end transaction flow. A transaction is a business use case, and the flow can be an API service, a batch job or a real-time transaction.","title":"Event Script Syntax"},{"location":"guides/CHAPTER-4/#flow-list","text":"This configuration file sits in the project \"resources\" project and contains a list of filenames. The default flow list is \"flows.yaml\" under the \"resources\" folder. It may look like this. flows: - 'get-profile.yml' - 'create-profile.yml' - 'delete-profile.yml' location: 'classpath:/flows/' The \"location\" parameter is optional. If present, you can tell the system to load the flow config files from another folder location.","title":"Flow list"},{"location":"guides/CHAPTER-4/#multiple-flow-lists","text":"You can provide more than one flow list to your application and it can become very handy under different situations. For instance, to achieve better modularity in complex application, flows can be grouped to multiple categories based on development team's choice and these flows can be managed in multiple flow lists. Another great place to use multiple flow list is to include external libraries which contain pre-defined flow lists. The following example demonstrates that an application loads a list of flows defined in \"flows.yaml\" and additional flows defined in \"more-flows.yaml\" file of a composable library. yaml.flow.automation=classpath:/flows.yaml, classpath:/more-flows.yaml","title":"Multiple flow lists"},{"location":"guides/CHAPTER-4/#writing-new-rest-endpoint-and-function","text":"You can use the \"composable-example\" subproject as a template to write your own composable application. Before you update the code, please clean the project using npm run clean . This will remove the scanned list of composable functions in the ComposableLoader ( preload.ts ) class so that you can write your own functions. For each filename in the flows.yml, you should create a corresponding configuration file under the \"resources/flows\" folder. Let's write a new flow called \"greetings\". You can copy-n-paste the following into a file called \"greetings.yml\" under the \"resources/flows\" folder. flow: id: 'greetings' description: 'Simplest flow' ttl: 10s first.task: 'greeting.demo' tasks: - input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end In the application.properties, you can specify the following parameter: yaml.flow.automation=classpath:/flows.yaml and update the \"flows.yaml\" file in the resources folder as follows: flows: - 'get-profile.yml' - 'create-profile.yml' - 'delete-profile.yml' - 'greetings.yml' Then, you can add a new REST endpoint in the \"rest.yaml\" configuration file like this. - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/greetings/{user}\" flow: 'greetings' timeout: 10s cors: cors_1 headers: header_1 The above REST endpoint takes the path parameter \"user\". The task executor will map the path parameter to the input arguments (headers and body) in your function. Now you can write your new function with the named route \"greeting.demo\". Please copy-n-paste the following into a TypeScript class called \"greetings.ts\" and save it under the \"tasks\" folder in the source project. import { AppException, Composable, EventEnvelope, preload } from \"mercury-composable\"; export class Greetings implements Composable { @preload('greeting.demo', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { const input = evt.getBody() as object; if ('user' in input) { const result = {}; result['time'] = new Date().toISOString(); result['message'] = 'Welcome'; result['user'] = input['user']; return result; } else { throw new AppException(400, \"Missing path parameter 'user'\") } } } To test your new REST endpoint, flow configuration and function, please point your browser to http://127.0.0.1:8086/api/greetings/my_name You can replace \"my_name\" with your first name to see the response to the browser.","title":"Writing new REST endpoint and function"},{"location":"guides/CHAPTER-4/#flow-configuration-syntax","text":"In your \"greetings.yml\" file above, you find the following key-values: flow.id - Each flow must have a unique flow ID. The flow ID is usually originated from a user facing endpoint through an event adapter. For example, you may write an adapter to listen to a cloud event in a serverless deployment. In The most common one is the HTTP adapter. The flow ID is originated from the \"rest.yaml\". The flow-engine will find the corresponding flow configuration and create a new flow instance to process the user request. flow.description - this describes the purpose of the flow flow.ttl - \"Time to live (TTL)\" timer for each flow. You can define the maximum time for a flow to finish processing. All events are delivered asynchronously and there is no timeout value for each event. The TTL defines the time budget for a complete end-to-end flow. Upon expiry, an unfinished flow will be aborted. You can use suffix \"s\" for seconds, \"m\" for minutes and \"h\" for hours. e.g. \"30s\" for 30 seconds. Note : When using the HTTP Flow Adapter, the flow.ttl value can be higher than the REST endpoint's timeout value. This would happen when one of your tasks in the event flow responds to the caller and the event flow continues to execute the rest of the flow. This type of task is called \"response\" task. first.task - this points to the route name of a function (aka \"task\") to which the flow engine will deliver the incoming event. The configuration file contains a list of task entries where each task is defined by \"input\", \"process\", \"output\" and \"execution\" type. In the above example, the execution type is \"end\", meaning that it is the end of a transaction and its result set will be delivered to the user.","title":"Flow configuration syntax"},{"location":"guides/CHAPTER-4/#underlying-event-system","text":"The Event Script system uses platform-core as the event system where it encapsulates EventEmitter from the standard library of Node.js. The integration points are intentionally minimalist. For most use cases, the user application does not need to make any API calls to the underlying event system.","title":"Underlying Event System"},{"location":"guides/CHAPTER-4/#rest-automation-and-http-flow-adapter","text":"The most common transaction entry point is a REST endpoint. The event flow may look like this: REQUEST -> \"http.request\" -> \"task.executor\" -> user defined tasks -> \"async.http.response\" -> RESPONSE REST automation is part of the platform-core library. It contains a non-blocking HTTP server that converts HTTP requests and responses into events. It routes an HTTP request event to the HTTP adapter if the \"flow\" tag is provided. In the following example, the REST endpoint definition is declared in a \"rest.yaml\" configuration. It will route the URI \"/api/decision\" to the HTTP flow adapter that exposes its service route name as \"http.flow.adapter\". rest: - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/decision?decision=_\" flow: 'decision-test' timeout: 10s cors: cors_1 headers: header_1 tracing: true The \"cors\" and \"headers\" sections are optional. When specified, the REST endpoint will insert CORS headers and HTTP request headers accordingly. For REST automation syntax, please refer to Chapter 3 The HTTP flow adapter maps the HTTP request dataset and the flow ID into a standard event envelope for delivery to the flow engine. The HTTP request dataset, addressable with the \"input.\" namespace, contains the following: Key Values method HTTP method uri URI path header HTTP headers cookie HTTP cookies path_parameter Path parameters if any query HTTP query parameters if any body HTTP request body if any stream input stream route ID if any ip remote IP address filename filename if request is a multipart file upload session authenticated session key-values if any For easy matching, please use lower case for headers, cookies, query and path parameters. Regular API uses JSON that will be converted to an object of key-values in the event's body.","title":"REST automation and HTTP flow adapter"},{"location":"guides/CHAPTER-4/#task-and-its-corresponding-function","text":"Each task in a flow must have a corresponding function. You can assign a task name to the function using the preload annotation like this. @preload('greeting.demo', 10) initialize(): Composable { return this; } The \"route\" in the preload annotation is the task name. The concurrency number define the maximum number of \"workers\" that the function can handle concurrently. The system is designed to be reactive and the function does not consume memory and CPU resources until an event arrives.","title":"Task and its corresponding function"},{"location":"guides/CHAPTER-4/#unique-task-naming","text":"Composable functions are designed to be reusable. By changing some input data mapping to feed different parameters and payload, your function can behave differently. Therefore, it is quite common to use the same function (i.e. the process parameter) more than once in a single event flow. When a task is not named, the \"process\" parameter is used to name the task. Since each task must have a unique name for event routing, we cannot use the same \"process\" name more than once in an event flow. To handle this use case, you can create unique names for the same function using the name parameter like this: flow: id: 'greetings' description: 'Simplest flow' ttl: 10s first.task: 'my.first.task' tasks: - name: 'my.first.task' input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: sequential next: - 'another.task' The above event flow configuration uses \"my.first.task\" as a named route for \"greeting.demo\" by adding the \"name\" parameter to the composable function. Note : The Event Manager performs event choreography using the unique task name. Therefore, when the \"process\" name for the function is not unique, you must create unique task \"names\" for the same function to ensure correct routing.","title":"Unique task naming"},{"location":"guides/CHAPTER-4/#hierarchy-of-flows","text":"As shown in Figure 1, you can run one or more sub-flows inside a primary flow. Figure 1 - Hierarchy of flows To do this, you can use the flow protocol identifier ( flow:// ) to indicate that the task is a flow. For example, when running the following task, \"flow://my-sub-flow\" will be executed like a regular task. tasks: - input: - 'input.path_parameter.user -> header.user' - 'input.body -> body' process: 'flow://my-sub-flow' output: - 'result -> model.json' description: 'Execute a sub-flow' execution: sequential next: - 'my.next.function' If the sub-flow is not available, the system will throw an error stating that it is not found. Hierarchy of flows would reduce the complexity of a single flow configuration file. The \"time-to-live (TTL)\" value of the parent flow should be set to a value that covers the complete flow including the time used in the sub-flows. In the input/output data mapping sections, the configuration management system provides a parent state machine using the namespace model.parent. to be shared by the primary flow and all sub-flows that are instantiated from it. Just like a task, a subflow has \"input\" and \"output\". You can map data to the \"input\" of a subflow using the namespaces \"body\" and \"header\" where they are maps of key-values. Inside a task of the subflow, the body and header namespaces can be accessed for their key-values like this: - input: - 'input.header.user -> header.user' - 'input.body -> *' process: 'first.task.in.subflow' output: - 'result -> model.parent.subflow_result' description: 'Execute a task in a subflow' execution: end Since the parent flow and subflows has a shared state machine, passing \"body\" and \"header\" key-values to the \"input\" of a subflow is optional. You can pass key-values between the parent and subflows using the shared state machine easily. Note : The namespace model.root. is an alias of model.parent. This would reduce ambiguity if you prefer to use \"root\" referring to the parent flow that creates one or more subflows.","title":"Hierarchy of flows"},{"location":"guides/CHAPTER-4/#tasks-and-data-mapping","text":"All tasks for a flow are defined in the \"tasks\" section.","title":"Tasks and data mapping"},{"location":"guides/CHAPTER-4/#inputoutput-data-mapping","text":"A function is self-contained. This modularity reduces application complexity because the developer only needs interface contract details for a specific function. To handle this level of modularity, the system provides configurable input/output data mapping. Namespaces for I/O data mapping Type Keyword and/or namespace LHS / RHS Mappings Flow input dataset input. left input Flow output dataset output. right output Function input body no namespace required right input Function input or output headers header or header. both I/O Function output result set result. left output Function output status code status left output Decision value decision right output State machine dataset model. both I/O Parent state machine dataset model.parent. both I/O External state machine key-value ext: right I/O For state machine (model and model.parent namespaces), the system prohibits access to the whole namespace. You should only access specific key-values in the model or model.parent namespaces. The namespace model.parent. is shared by the primary flow and all sub-flows that are instantiated from it. The external state machine namespace uses the namespace ext: to indicate that the key-value is external. Constants for input data mapping Type Keyword for the left-hand-side argument String text(example_value) Integer int(number) Long long(number) Float float(number) Double double(number) Boolean boolean(true or false) Map map(k1=v1, k2=v2) map(base.config.parameter) File file(text:file_path) File file(binary:file_path) File file(json:file_path) Classpath classpath(text:file_path) Classpath classpath(binary:file_path) Classpath classpath(json:file_path) For input data mapping, the \"file\" constant type is used to load some file content as an argument of a user function. You can tell the system to render the file as \"text\", \"binary\" or \"json\". Similarly, the \"classpath\" constant type refers to static file in the application source code's \"resources\" folder. When file type mapping is \"json\", the file content will be rendered as a Map or a List from a JSON string. The \"map\" constant type is used for two purposes: 1. Map of key-values The following example illustrates creation of a map of key-values. In the first entry, a map of 2 key-values is set as the input argument \"myMap\" of a user function. In the second entry, the map's values are retrieved from the key \"some.key\" in base configuration and the environment variable \"ENV_VAR_ONE\". 'map(k1=v1, k2=v2) -> myMap' 'map(k1=${some.key}, k2=${ENV_VAR_ONE}) -> myMap' Note : The comma character is used as a separator for each key-value pair. If the value contains a comma, the system cannot parse the key-values correctly. In this case, please use the 2nd method below. 2. Mapping values from application.yml The following input data mapping sets the value of \"my.key\" from the application.yml base configuration file to the input argument \"myKey\" of a user function. 'map(my.key) -> myKey' The \"map(my.key)\" would set a primitive value (text, integer, float, boolean), a JSON object of key-values or an array of values. Special content type for output data mapping Type Keyword for the right-hand-side argument File file(file_path) File file(append:file_path) For output data mapping, the \"file\" content type is used to save some data from the output of a user function to a file in the local file system. If the left-hand-side (LHS) resolved value is null, the file in the RHS will be deleted. This allows you to clean up temporary files before your flow finishes. An optional prefix \"append\" may be used to tell the system to append file content instead of overwriting it. Decision value The \"decision\" keyword applies to \"right hand side\" of output data mapping statement in a decision task only (See \"Decision\" in the task section). Each flow has its own input and output Each function has its input headers, input body and output result set. Optionally, a function can return an EventEnvelope object to hold its result set in the \"body\", a \"status\" code and one or more header key-values. Since each function is stateless, a state machine (with namespace model. ) is available as a temporary memory store for transaction states that can be passed from one task to another. All variables are addressable using the standard dot-bracket convention. For example, \"hello.world\" will retrieve the value 100 from this data structure: { \"hello\": { \"world\": 100 } } and \"numbers[1]\" will retrieve the value 200 below: { \"numbers\": [100, 200] } The assignment is done using the assignment ( -> ) syntax. In the following example, the HTTP input query parameter 'amount' is passed as input body argument 'amount' to the task 'simple.decision'. The result (function \"return value\") from the task will be mapped to the special \"decision\" variable that the flow engine will evaluate. This assumes the result is a boolean or numeric value. The \"decision\" value is also saved to the state machine ( model ) for subsequent tasks to evaluate. - input: - 'input.query.amount -> amount' process: 'simple.decision' output: - 'result -> decision' - 'result -> model.decision'","title":"Input/Output data mapping"},{"location":"guides/CHAPTER-4/#environment-variables","text":"You can use the standard ${ENV_VAR:default} syntax to resolve environment variables or parameters from the application.properties.","title":"Environment variables"},{"location":"guides/CHAPTER-4/#runtime-model-variables","text":"To use a runtime model variable value as a key or constant, you can use the {model.variable_name} syntax. For example, - input: - 'text(wonderful day) -> model.world' - 'text(world) -> model.pointer' - 'model.{model.pointer} -> value1' - 'text(new {model.pointer}) -> value2' - 'text(keep {this}/{one} unchanged) -> value3' process: 'demo.function' model.{model.pointer} is resolved as model.world , giving value1 = wonderful day and value2 = new world . The text inside a set of brackets that is not a model variable will be kept unchanged, thus value3 = keep {this}/{one} unchanged The use of string substitution is subject to event script syntax validation. Therefore, When this feature is used in the left-hand-side of an input data mapping, it can be used to substitute a constant or a segment of a key in the input. and model. namespaces. The above example shows the use of the model namespace in model.{model.pointer} -> value1 . Similarly, when used in the left-hand-side of an output data mapping, it can be used to substitute a constant or a segment of a key in the input. , model. , header. or result. namespaces. When used in the right-hand-side of an input data mapping, namespace is optional because it may map as an argument to a task. When used in the right-hand-side of an output data mapping, it can be used to substitute a model. namespace, file( output, flow output. namespace or an external state machine ext: namespace. Important : For security reason, the key inside the brackets must be a model variable. The resolved value from a model variable must be either text or number. Otherwise, it will be converted to a value of \"null\". For simplicity, nested substitution is not allowed. i.e. model.{model.{model.n}} or model.{model.list[model.n]} will be rejected. If the bracketed text is not a model variable, the brackets and the enclosed text will be kept unchanged.","title":"Runtime model variables"},{"location":"guides/CHAPTER-4/#handling-arrays-in-a-dataset","text":"An array of data elements is expressed as a list. { \"numbers\": [100, 200] } As discussed earlier, an array element can be retrieved using a number as index. For example, to take the second element with value 200 above, you can use this data mapping like this: 'input.body.numbers[1] -> second_number' In the above example, it is an \"input data mapping\". It maps the second element of value 200 as the input argument \"second_number\" to a composable function. For-loop feature is supported in pipeline in an event flow. It would be convenient to use the iterator value as an index to map an input argument. We can do something like this: 'input.body.numbers[model.n] -> second_number' where model.n is the iterator value in a for-loop. Similarly, it is possible to do output data mapping. For example, 'result.computed -> model.list[model.n]' To address an array element, we can use a number or a \"dynamic model variable\" as an index. The model variable must resolved to a number. Note : There are some consideration when using a dynamic model variable as an index. The left-hand-side of a data mapping is a GET operation. The right-hand-side is a SET operation. If the model variable is non-numeric, the GET operation will return null and SET operation will throw exception. To avoid setting an arbitrary high index, the size of the index is limited by the parameter \"max.model.array.size\" in application.yml","title":"Handling arrays in a dataset"},{"location":"guides/CHAPTER-4/#append-an-element-to-an-array","text":"An empty array index in the right hand side tells the system to append an element to an array. For example, the value resolved from the left hand side \"result.item1\" and \"result.item2\" will be appended to the model.items array in the state machine. - 'result.item1 -> model.items[]' - 'result.item2 -> model.items[]' If model.items does not exist, the first element will be set as array index \"0\". Therefore, the above output data mapping statements are the same as: - 'result.item1 -> model.items[0]' - 'result.item2 -> model.items[1]'","title":"Append an element to an array"},{"location":"guides/CHAPTER-4/#simple-type-matching-and-conversion","text":"Event script's state machine supports simple type matching and conversion for the model namespace. This \"impedance matching\" feature allows us to accommodate minor interface contract changes without refactoring business logic of a user function. This is supported in both the left-hand-side and right-hand-side of both input and output data mappings. For the left-hand-side, the state machine's model value is matched or converted to the target data type before setting the value of the right-hand-side. The state machine values are unchanged. For the right-hand-side, the matched or converted value is applied to the state machine's model value. The syntax is model.somekey:type where \"type\" is one of the following: Type Match value as Example text text string model.someKey:text binary byte array model.someKey:binary int integer or -1 if not numeric model.someKey:int long long or -1 if not numeric model.someKey:long float float or -1 if not numeric model.someKey:float double double or -1 if not numeric model.someKey:double boolean true or false model.someKey:boolean boolean(value) true if value matches model.someKey:boolean(positive) boolean(value=true) true if value matches model.someKey:boolean(positive=true) boolean(value=false) false if value matches model.someKey:boolean(negative=false) and(model.key) boolean AND of 2 model keys model.someKey:and(model.another) or(model.key) boolean OR of 2 model keys model.someKey:or(model.another) !model.key negate of a model variable !model.someKey substring(start, end) extract a substring model.someKey:substring(0, 5) substring(start) extract a substring model.someKey:substring(5) concat(vars...) concat model variables & text model.a:concat(model.b, text(!)) b64 byte-array to Base64 text model.someKey:b64 b64 Base64 text to byte-array model.someKey:b64 uuid generated UUID-4 value model.unique_id:uuid length length of model list variable model.someList:length For Base64 type matching, it handles two symmetrical use cases. If the key-value is a text string, the system would assume it is a Base64 text string and convert it to a byte-array. If the key-value is a byte-array, the system will encode it into a Base64 text string. For uuid type matching, the system will ignore the value of the model variable in the left hand side because UUID is a generated value. When using it in the right hand side, the model variable will be updated with a generated UUID value accordingly. For simplicity of syntax, each type matching command is a single operation. For more complex operation such as multiple AND, OR and NEGATE operators, you can configure multiple steps of operation. For string concatenation, you may concat a model variable with one or more model variables and text constants. A more convenient alternative to string concatenation is the use of \"runtime model variables\". You can replace the \"concat\" method with \"runtime model variable\" method as follows: # assuming the bearer token value is in model.token - 'text(Bearer ) -> model.bearer' - 'model.bearer:concat(model.token) -> authorization' # the above is the same as - 'text(Bearer {model.token}) -> authorization' An interesting use case is a simple decision task using the built-in no-op function. For boolean with value matching, you can test if the key-value in the left-hand-side is a null value. For example, when a control file for the application is not available, your application will switch to run in dev mode. A sample task may look like this: first.task: 'no.op' tasks: - input: - 'file(binary:/tmp/interesting-config-file) -> model.is-local:boolean(null=true)' process: 'no.op' output: - 'model.is-local -> decision' execution: decision next: - 'start.in.dev.mode' - 'start.in.cloud' Another use case is type conversion for HTTP path parameter which is always a text string. If your composable function requires a path parameter to be accepted as an integer, you can do this: - input: - 'input.path_parameter.userid -> model.userid:int' - 'model.userid -> userid' The above input data mapping example illustrates the use of a model variable to convert a text parameter into an integer. Note that if the path parameter is not numeric, the converted value will be -1. Note : The system only supports \"type matching modifier\" in the model namespace because of the design principle of data immutability. The model is a state machine for a flow instance. As a temporary store, we can use it for this purpose without side effect that the user application would accidentally modify a value of the flow's input.","title":"Simple type matching and conversion"},{"location":"guides/CHAPTER-4/#convenient-data-mapping-using-model-variable","text":"To address the common use case of using a model variable as an intermediate value, the system supports the following formats for input data mapping and output data mapping. // 2-part data mapping format LHS -> RHS // 3-part data mapping format LHS -> model.variable -> RHS For the 2-part data mapping format, there are left-hand-side and right-hand-side where the value retrieved from the left-hand-side variable is mapped to the right-hand-side. The 3-part data mapping allows us to use a model variable as an intermediate for simple type matching. In the previous example, it uses two entries to convert a HTTP path parameter from a text string to a number and set the number as input argument. The configuration syntax can be simplified as follows: - input: - 'input.path_parameter.userid -> model.userid:int -> userid' The above 3-part data mapping entry will be expanded into two entries internally. This extra processing is done at the \"CompileFlows\" step and thus there is no impact to the task execution speed. Please note that the 3-part data mapping format is not supported when the left-hand-side is a text constant. It is because a text constant may contain any special characters including the mapping signature -> .","title":"Convenient data mapping using model variable"},{"location":"guides/CHAPTER-4/#metadata-for-each-flow-instance","text":"For each flow instance, the state machine in the \"model\" namespace provides the following metadata that you can use in the input/output data mapping. For example, you can set this for an exception handler to log additional information. Type Keyword Comment Flow ID model.flow The ID of the event flow config Trace ID model.trace Optional traceId when tracing is turned on Correlation ID model.cid Correlation ID of the inbound request","title":"Metadata for each flow instance"},{"location":"guides/CHAPTER-4/#special-handling-for-header","text":"When function input keyword header is specified in the \"right hand side\" of an input data mapping statement, it refers to the input event envelope's headers. Therefore, it assumes the \"left hand side\" to resolve into a JSON object of key-values. Otherwise, it will reject the input data mapping statement with an error like this: Invalid input mapping 'text(ok) -> header', expect: JSON, Actual: string When function input namespace header. is used, the system will map the value resolved from the \"left hand side\" statement into the specific header. For example, the input data mapping statement text(ok) -> header.demo will set \"demo=ok\" into the input event envelope's headers. When function output keyword header is specified in the \"left hand side\" of an output data mapping statement, it will resolve as a JSON object of key-values from the function output event envelope's headers. Similarly, when function output namespace header. is used, the system will resolve the value from a specific key of the function output event envelope's headers.","title":"Special handling for header"},{"location":"guides/CHAPTER-4/#function-input-and-output","text":"To support flexible input data mapping, the input to a function must be either a JSON object of key-values. However, the output (i.e. result set) of a function can be JSON object or primitive (boolean, string, number, Buffer, etc.). Your function must implement the Composable interface to configure input and output. Since a data structure is passed to your function's input argument as key-values, you may create a class to deserialize the data structure. To tell the system that your function is expecting input as a JSON object, you can use the special notation * on the right hand side. For example, the following entry tells the system to set the value in \"model.dataset\" as a JSON payload. - input: - 'model.dataset -> *' Note : If the value from the left hand side is not a JSON, the system will ignore the input mapping command and print out an error message in the application log.","title":"Function input and output"},{"location":"guides/CHAPTER-4/#setting-function-input-headers","text":"When function input body is used to hold a JSON object, we may use function input headers to pass other arguments to the function without changing the data structure of a user defined JSON object of key-values. In the following example, the HTTP query parameter \"userid\" will be mapped to the function input header key \"user\" and the HTTP request body will be mapped to the function input body. - input: - 'input.query.userid -> header.user' - 'input.body -> *' process: 'my.user.function' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body'","title":"Setting function input headers"},{"location":"guides/CHAPTER-4/#task-types","text":"","title":"Task types"},{"location":"guides/CHAPTER-4/#decision-task","text":"A decision task makes decision to select the next task to execute. It has the tag execution=decision . In the output data mapping section, it must map the corresponding result set or its key-value to the decision object. The \"next\" tag contains a list of tasks to be selected based on the decision value. If decision value is boolean, a true value will select the first task. Otherwise, the second task will be selected. If decision value is an integer, the number should start from 1 where the corresponding \"next\" task will be selected. tasks: - input: - 'input.query.decision -> decision' process: 'simple.decision' output: - 'result -> model.decision' - 'result -> decision' description: 'Simple decision test' execution: decision next: - 'decision.case.one' - 'decision.case.two'","title":"Decision task"},{"location":"guides/CHAPTER-4/#response-task","text":"A response task will provide result set as a flow output or \"response\". A response task allows the flow to respond to the user or caller immediately and then move on to the next task asynchronously. For example, telling the user that it has accepted a request and then moving on to process the request that may take longer time to run. A response task has the tag execution=response and a \"next\" task. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' - 'result -> output.body' description: 'Pass a JSON to another task' execution: response next: - 'sequential.two'","title":"Response task"},{"location":"guides/CHAPTER-4/#end-task","text":"An end task indicates that it is the last task of the transaction processing in a flow. If the flow has not executed a response task, the end task will generate the response. Response is defined by output data mapping. This task has the tag execution=end . For example, the greeting task in the unit tests is an end task. - input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end","title":"End task"},{"location":"guides/CHAPTER-4/#sequential-task","text":"Upon completion of a sequential task, the next task will be executed. This task has the tag execution=sequential . In the following example, sequential.two will be executed after sequential.one . tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' description: 'Pass a JSON to another task' execution: sequential next: - 'sequential.two'","title":"Sequential task"},{"location":"guides/CHAPTER-4/#parallel-task","text":"Upon completion of a parallel task, all tasks in the \"next\" task list will be executed in parallel. This task has the tag execution=parallel . In this example, parallel.one and parallel.two will run after begin.parallel.test tasks: - input: - 'int(2) -> count' process: 'begin.parallel.test' output: [] description: 'Setup counter for two parallel tasks' execution: parallel next: - 'parallel.one' - 'parallel.two'","title":"Parallel task"},{"location":"guides/CHAPTER-4/#fork-n-join-task","text":"Fork-n-join is a parallel processing pattern. A \"fork\" task will execute multiple \"next\" tasks in parallel and then wait for the result sets before running the \"join\" task. This task has the tag execution=fork . It must have a list of \"next\" tasks and a \"join\" task. It may look like this: tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' description: 'Pass a JSON to another task' execution: fork next: - 'echo.one' - 'echo.two' join: 'join.task'","title":"Fork-n-join task"},{"location":"guides/CHAPTER-4/#dynamic-fork-n-join-task","text":"A special version of the fork-n-join pattern is called dynamic fork-n-join which refers to parallel processing of multiple instances of the same \"next\" task for each element in a list. For example, you have a list of 100 elements in an incoming request and each element would be processed by the same backend service. You want to process the 100 elements in parallel by multiple instances of a service wraper that connects to the backend service. The use case can be configured like this: tasks: - input: - 'input.elements -> elements' process: 'data.validation' output: - 'result.elements -> model.elements' description: 'Validate list of elements' execution: fork source: 'model.elements' next: - 'element.processor' join: 'join.task' - name: 'element.processor' input: - 'model.elements.ITEM -> item' - 'model.elements.INDEX -> index' process: 'v1.element.processor' output: [] description: 'Hello world' execution: sink To handle this special use case, you can add a source parameter in the fork task. The \"source\" parameter tells the system which model variable holds the list of elements. You should only configure a single \"next\" task. The system will spin up parallel instances of the next task to handle each element from the model variable containing the list. In the input data mapping section, there are two special suffixes .ITEM and .INDEX . The system will iterate the list of elements and spin up an instance of the \"next\" task to retrieve the element (item) and index of the element in the list. The two special suffixes are relevant only when adding to the model variable configured in the \"source\" parameter. Important : The model variables with special suffixes '.ITEM' and '.INDEX' are virtual objects for the purpose of mapping as input arguments to a task. They cannot be used as regular model variables. Dynamic fork-n-join is designed to execute the same task for a list of elements in parallel. It does not support subflow. i.e. the \"process\" tag of the \"next\" task cannot be a subflow.","title":"Dynamic fork-n-join task"},{"location":"guides/CHAPTER-4/#sink-task","text":"A sink task is a task without any next tasks. Sink tasks are used by fork-n-join and pipeline tasks as reusable modules. This task has the tag execution=sink . - input: - 'text(hello-world-two) -> key2' process: 'echo.two' output: - 'result.key2 -> model.key2' description: 'Hello world' execution: sink","title":"Sink task"},{"location":"guides/CHAPTER-4/#pipeline-feature","text":"Pipeline is an advanced feature of Event Script.","title":"Pipeline feature"},{"location":"guides/CHAPTER-4/#pipeline-task","text":"A pipeline is a list of tasks that will be executed orderly within the current task. When the pipeline is done, the system will execute the \"next\" task. This task has the tag execution=pipeline . tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' description: 'Pass a JSON to another task' execution: pipeline pipeline: - 'echo.one' - 'echo.two' next: - 'echo.three' Some special uses of pipelines include \"for/while-loop\" and \"continue/break\" features.","title":"Pipeline task"},{"location":"guides/CHAPTER-4/#simple-for-loop","text":"In the following example, the loop.statement contains a for-loop that uses a variable in the state machine to evaluate the loop. In this example, the pipeline will be executed three times before passing control to the \"next\" task. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' description: 'Pass a JSON to another task' execution: pipeline loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' pipeline: - 'echo.one' - 'echo.two' - 'echo.three' next: - 'echo.four'","title":"Simple for-loop"},{"location":"guides/CHAPTER-4/#simple-while-loop","text":"The loop.statement may use a \"while loop\" syntax like this: loop: statement: 'while (model.running)' To exit the above while loop, one of the functions in the pipeline should return a boolean \"false\" value with output \"data mapping\" to the model.running variable.","title":"Simple while loop"},{"location":"guides/CHAPTER-4/#for-loop-with-breakcontinue-decision","text":"In the following example, the system will evaluate if the model.quit variable is true. If yes, the break or continue condition will be executed. The state variable is obtained after output data mapping and any task in the pipeline can set a key-value for mapping into the state variable. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.json' description: 'Pass a JSON to another task' execution: pipeline loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' condition: 'if (model.quit) break' pipeline: - 'echo.one' - 'echo.two' - 'echo.three' next: - 'echo.four' Note that the \"condition\" parameter can be a single condition or a list of conditions. In the following example, the system will evaluate both the model.quit and model.jump values. loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' condition: - 'if (model.quit) break' - 'if (model.jump) break'","title":"For loop with break/continue decision"},{"location":"guides/CHAPTER-4/#handling-exception","text":"You can define exception handler at the top level or at the task level. Exception is said to occur when a user function throws exception or returns an EventEnvelope object with a status code equals to or larger than 400. The event status uses the same numbering scheme as HTTP exception status code. Therefore, status code less than 400 is not considered an exception.","title":"Handling exception"},{"location":"guides/CHAPTER-4/#top-level-exception-handler","text":"Top-level exception handler is a \"catch-all\" handler. You can define it like this: flow: id: 'greetings' description: 'Simplest flow of one task' ttl: 10s exception: 'v1.my.exception.handler' In this example, the v1.my.exception.handler should point to a corresponding exception handler that you provide. The following input arguments will be delivered to your function when exception happens. Key Description status Exception status code message Error message stack Stack trace in a text string The exception handler function can be an \"end\" task to abort the transaction or a decision task to take care of the exception. For example, the exception handler can be a \"circuit-breaker\" to retry a request. Note : for efficiency, stack trace transport is limited to the first 10 lines.","title":"Top-level exception handler"},{"location":"guides/CHAPTER-4/#task-level-exception-handler","text":"You can attach an exception handler to a task. One typical use is the \"circuit breaker\" pattern. In the following example, the user function \"breakable.function\" may throw an exception for some error condition. The exception will be caught by the \"v1.circuit.breaker\" function. - input: - 'input.path_parameter.accept -> accept' - 'model.attempt -> attempt' process: 'breakable.function' output: - 'int(0) -> model.attempt' - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'This demo function will break until the \"accept\" number is reached' execution: end exception: 'v1.circuit.breaker' The configuration for the circuit breaker function may look like this: - input: - 'model.attempt -> attempt' - 'int(2) -> max_attempts' - 'error.code -> status' - 'error.message -> message' - 'error.stack -> stack' process: 'v1.circuit.breaker' output: - 'result.attempt -> model.attempt' - 'result.decision -> decision' - 'result.status -> model.status' - 'result.message -> model.message' description: 'Just a demo circuit breaker' execution: decision next: - 'breakable.function' - 'abort.request' An exception handler will be provided with the \"error\" object that contains error code, error message and a stack trace. The exception handler can inspect the error object to make decision of the next step. For circuit breaker, we can keep the number of retry attempts in the state machine under \"model.attempt\" or any key name that you prefer. In the above example, it sets an integer constant of 2 for the maximum attempts. The circuit breaker can then evaluate if the number of attempts is less than the maximum attempts. If yes, it will return a decision of \"true\" value to tell the system to route to the \"breakable.function\" again. Otherwise, it will return a decision of \"false\" value to abort the request. A more sophisticated circuit breaker may be configured with \"alternative execution paths\" depending on the error status and stack trace. In this case, the decision value can be a number from 1 to n that corresponds to the \"next\" task list. Exception handlers may be used in both queries and transactions. For a complex transaction, the exception handler may implement database rollback logic or recovery mechanism.","title":"Task-level exception handler"},{"location":"guides/CHAPTER-4/#best-practice","text":"When a task-level exception handler throws exception, it will be caught by the top-level exception handler, if any. A top-level exception handler should not throw exception. Otherwise it may go into an exception loop. Therefore, we recommend that an exception handler should return regular result set in a JSON object. An example of task-level exception handler is shown in the \"hello-exception.ts\" class in the \"task\" folder where it set the status code in the result set so that the system can map the status code from the result set to the next task or to the HTTP output status code.","title":"Best practice"},{"location":"guides/CHAPTER-4/#advanced-features","text":"","title":"Advanced features"},{"location":"guides/CHAPTER-4/#no-operation-function","text":"A convenient no-operation function with the route name no.op is available. It can be used when you want to perform some input/output data mapping without executing any business logic.","title":"No-operation function"},{"location":"guides/CHAPTER-4/#generic-resilience-handler-function","text":"Another useful built-in function is a resilience handler with the route name resilience.handler . It is a generic resilience handler. It will retry, abort, use an alternative path or exercise a brief backoff. Figure 2 - Resilience Handler The following parameters (input data mapping) define behavior for the handler: max_attempts - when the handler has used all the attempts, it will abort. attempt - this tells the handler how many attempts it has tried status - you should map the error status code in this field message - you should map the error message in this field alternative - the optional codes and range of status codes to tell the handler to reroute delay - the delay in milliseconds before exercising retry or reroute. Minimum value is 10 ms. Delay is skipped for the first retry. This slight delay is a protection mechanism. Optional backoff behavior: cumulative - the total number of failures since last success or backoff reset if any backoff - the time of a backoff period (epoch milliseconds) if any backoff_trigger - the total number of failures that triggers a backoff backoff_seconds - the time to backoff after an abort has occurred. During this period, It will abort without updating attempt. This avoids overwhelming the target service that may result in recovery storm. Return value (output data mapping): result.attempt - the handler will clear or increment this counter result.cumulative - the handler will clear or increment this counter. Not set if \"backoff_trigger\" is not given in input. result.decision - 1, 2 or 3 where 1=retry, 2=abort, 3=reroute that corresponds to the next tasks result.status - the status code that the handler aborts the retry or reroute. Not set if retry or reroute. result.message - the reason that the handler aborts the retry or reroute. Not set if retry or reroute. result.backoff - the time of a backoff period (epoch milliseconds). Not set if not in backoff mode. Note : \"result.attempt\" should be saved in the state machine with the \"model.\" namespace. \"result.cumulative\" and \"result.backoff\" should be saved in the temporary file system or an external state machine. For more details, please refer to the event script resilience-demo.yml in the test resources folder and the unit test will handle backoff, retry and abort in resilience handler under the flow.test.ts class. Extract of the task configuration for the resilience handler is shown as follows. In the following example, \"my.task\" is the function that is configured with the 'resilience.handler' as an exception handler. The input data mapping tells the handler to enter into \"backoff\" period when the cumulative failure count reaches the \"backoff_trigger\" threshold of 3. After that, all requests will be aborted until the backoff period expires. - input: - 'error.code -> status' - 'error.message -> message' - 'model.attempt -> attempt' - 'int(10) -> max_attempts' - 'text(401, 403-404) -> alternative' - 'file(text:/tmp/resilience/cumulative) -> cumulative' - 'file(text:/tmp/resilience/backoff) -> backoff' - 'int(3) -> backoff_trigger' - 'int(2) -> backoff_seconds' - 'int(500) -> delay' process: 'resilience.handler' output: - 'result.status -> model.status' - 'result.message -> model.message' - 'result.attempt -> model.attempt' - 'result.decision -> decision' - 'result.backoff -> file(/tmp/resilience/backoff)' - 'result.cumulative -> file(/tmp/resilience/cumulative)' description: 'Resilience handler with alternative path and backoff features' execution: decision next: - 'my.task' - 'abort.request' - 'alternative.task' Note : When the \"backoff\" feature is enabled, you should configure the resilience handler as a gatekeeper to protect your user function. This allows the system to abort requests during the backoff period. You may also use this resilience handler as a starting point to write your own exception handler for more complex recovery use cases.","title":"Generic resilience handler function"},{"location":"guides/CHAPTER-4/#external-state-machine","text":"The in-memory state machine is created for each query or transaction flow and it is temporal. For complex transactions or long running work flows, you would typically want to externalize some transaction states to a persistent store such as a distributed cache system or a high performance key-value data store. In these use cases, you can implement an external state machine function and configure it in a flow. Below is an example from a unit test. When you externalize a key-value to an external state machine, you must configure the route name (aka level-3 functional topic) of the external state machine. Note : Passing a null value to a key of an external state machine means \"removal\". external.state.machine: 'v1.ext.state.machine' tasks: - input: # A function can call an external state machine using input or output mapping. # In this example, it calls external state machine from input data mapping. - 'input.path_parameter.user -> ext:/${app.id}/user' - 'input.body -> model.body' # demonstrate saving constant to state machine and remove it using model.none - 'text(world) -> ext:hello' - 'model.none -> ext:hello' process: 'no.op' output: - 'text(application/json) -> output.header.content-type' # It calls external state machine again from output data mapping - 'input.body -> ext:/${app.id}/body' - 'input.body -> output.body' - 'text(message) -> ext:test' - 'model.none -> ext:test' description: 'Hello World' execution: end The \"external.state.machine\" parameter is optional. When present, the system will send a key-value from the current flow instance's state machine to the function implementing the external state machine. The system uses the \"ext:\" namespace to externalize a state machine's key-value. Note : The delivery of key-values to the external state machine is asynchronous. Therefore, please assume eventual consistency. You should implement a user function as the external state machine. The input interface contract to the external state machine for saving a key-value is: header.type = 'put' header.key = key body.data = value Your function should save the input key-value to a persistent store. In another flow that requires the key-value, you can add an initial task to retrieve from the persistent store and do \"output data mapping\" to save to the in-memory state machine so that your transaction flow can use the persisted key-values to continue processing. In the unit tests of the event-script-engine subproject, these two flows work together: externalize-put-key-value externalize-get-key-value IMPORTANT : Events to an external state machine are delivered asynchronously. If you want to guarantee message sequencing, please do not set the \"instances\" parameter in the preLoad annotation. To illustrate a minimalist implementation, below is an example of an external state machine in the event-script-engine's unit test section. class ExtStateMachine implements Composable { initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { if (!evt.getHeader(KEY)) { throw new Error(\"Missing key in headers\"); } const type = evt.getHeader(TYPE); const key = evt.getHeader(KEY); const input = evt.getBody(); if (PUT == type && input instanceof Object && 'data' in input) { var data = input['data']; if (data) { log.info(`Saving ${key} to store`); store[key] = data; return true; } } if (GET == type) { const v = store[key]; if (v) { log.info(`Retrieve ${key} from store`); return v; } else { return null; } } if (REMOVE == type) { if (key in store) { delete store[key]; log.info(`Removed ${key} from store`); return true; } else { return false; } } return false; } } For more sophisticated operation, you may also configure the external state machine as a \"flow\" like this: external.state.machine: 'flow://ext-state-machine' You can then define the flow for \"ext-state-machine\" like this: flow: id: 'ext-state-machine' description: 'Flow to execute an external state machine' ttl: 10s first.task: 'v1.ext.state.machine' tasks: - input: - 'input.header.key -> header.key' - 'input.header.type -> header.type' - 'input.body.data -> data' process: 'v1.ext.state.machine' output: [] description: 'Execute external state machine' execution: end Note : By definition, external state machine flow is outside the scope of the calling flow.","title":"External state machine"},{"location":"guides/CHAPTER-4/#future-task-scheduling","text":"You may add a \u201cdelay\u201d tag in a task so that it will be executed later. This feature is usually used for unit tests or \"future task scheduling\". Since the system is event-driven and non-blocking, the delay is simulated by event scheduling. It does not block the processing flow. Type Value Example Fixed delay Milliseconds delay: '1000 ms' Variable delay State machine variable delay: model.delay Note that the \"ms\" suffix is optional for documentation purpose. It denotes milliseconds if present. When delay is set to a state variable that its value is not configured by a prior data mapping, the delay command will be ignored. An example task that has an artificial delay of 2 seconds: tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.ex -> exception' - 'text(hello world) -> greeting' process: 'greeting.test' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end delay: '2000 ms'","title":"Future task scheduling"},{"location":"guides/CHAPTER-4/#using-worker-threads","text":"In some use cases, there is a need to use legacy libraries and open sources that we cannot refactor into composable functions. We have provided a worked example to encapsulate any legacy library running in a worker thread to behave as a composable function. This allows you to accelerate integration with existing enterprise software assets for production use so that you can gradually and orderly refactor them into composable functions. Please review the composable-worker.ts source file to examine how it encapsulates a worker thread: Composable example repository The composable-worker.ts class may be used as a template: ComposableWorker An extract of the source code is shown below. export class ComposableWorker implements Composable { @preload('composable.worker.demo', 5, true, true) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // deferred startup until triggered by autostart or your own setup task if (!loaded && 'start' == evt.getHeader('type')) { workerBridge(); } if (worker) { // sending the original event to the worker to preserve metadata for tracing and correlation sendEventToWorker(evt); } return null; } } The workerBridge will start a new worker thread and the sendEventToWorker method will forward an incoming event to the worker thread. After processing the request, the worker thread would send acknowledgement or response back to the main thread in the composable function that will forward it to the caller accordingly. In the ComposableWorker sample code, it also illustrates the technique to pass environment variables, application runtime arguments and the Composable configuration system from the main thread in your Composable application to the legacy code in the worker thread. A demo endpoint \"GET /api/worker/demo\" or \"POST /api/worker/demo\" is available to demonstrate the ComposableWorker example. Using using the POST method, please set content-type to 'application/json' and provide HTTP request payload as a JSON string. Please use the worked example as a template to write your own composable function to encapsulate some legacy code or open sources that you have no direct control. Chapter-3 Home Chapter-5 REST Automation Table of Contents Build, Test and Deploy","title":"Using Worker Threads"},{"location":"guides/CHAPTER-5/","text":"Build, Test and Deploy The first step in writing an application is to create an entry point for your application. Main application A minimalist main application template is shown as follows: import { ComposableLoader } from './preload/preload.js'; async function main() { // This assumes you have a \".env\" environment variable file. If not, remove this line. process.loadEnvFile(); // Load composable functions into memory and automatically starts your application modules await ComposableLoader.initialize(); } // run the application main(); In your application.yml configuration file, you would configure autostart modules like this: modules.autostart: - 'main.app' In the above example, the \"main.app\" is defined in the main-application.ts in the composable-example project. It looks like this: export class MainApp implements Composable { @preload('main.app') initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // put your start up business logic here log.info(\"Application started\"); // release this function to guarantee that it is executed only once Platform.getInstance().release('main.app'); // return value is ignored because start up code runs asynchronously return true; } } You can also build and run the application from command line like this: cd sandbox/composable-nodejs-example npm install npm run build npm run test Since all functions are connected using the in-memory event bus, you can test any function by sending events from a unit test module. Writing your functions Please follow the step-by-step learning guide in Chapter-1 to write your own functions. You can then configure new REST endpoints to use your new functions. HTTP forwarding In Chapter-3 , we have presented the configuration syntax for the \"rest.yaml\" REST automation definition file. Please review the sample rest.yaml file in the lambda-example project. You may notice that it has an entry for HTTP forwarding. The following entry in the sample rest.yaml file illustrates an HTTP forwarding endpoint. In HTTP forwarding, you can replace the \"service\" route name with a direct HTTP target host. You can do \"URL rewrite\" to change the URL path to the target endpoint path. In the below example, /api/v1/* will be mapped to /api/* in the target endpoint. - service: \"http://127.0.0.1:${rest.server.port}\" trust_all_cert: true methods: ['GET', 'PUT', 'POST'] url: \"/api/v1/*\" url_rewrite: ['/api/v1', '/api'] timeout: 20 cors: cors_1 headers: header_1 tracing: true Sending HTTP request event to more than one service One feature in REST automation \"rest.yaml\" configuration is that you can configure more than one function in the \"service\" section. In the following example, there are two function route names (\"hello.world\" and \"hello.copy\"). The first one \"hello.world\" is the primary service provider. The second one \"hello.copy\" will receive a copy of the incoming event. This feature allows you to write new version of a function without disruption to current functionality. Once you are happy with the new version of function, you can route the endpoint directly to the new version by updating the \"rest.yaml\" configuration file. - service: [\"hello.world\", \"hello.copy\"] Writing a unit test In unit tests, we want to tell the system to use the \"tests/resources\" folder to override the \"src/resources\" folder so that we can adjust the configuration to test different scenarios. This can be done by using the ComposableLoader's initialize method in the BeforeAll section like this: import { ComposableLoader } from '../test/preload/preload.ts'; describe('End-to-end tests', () => { beforeAll(async () => { await ComposableLoader.initialize(8305, true); }); // your unit test here afterAll(async () => { await Platform.getInstance().stop(); // give console.log a moment to finish await util.sleep(2000); log.info(\"End-to-end tests completed\"); }); it('can do health check', async () => { const po = new PostOffice(); const req = new EventEnvelope().setTo('demo.health').setHeader('type', 'health'); const result = await po.request(req, 2000); expect(result).toBeTruthy(); expect(result.getBody()).toEqual({\"status\": \"demo.service is running fine\"}); }); } In the above example, we set the server port for REST automation to 8305 and set the \"unit test\" parameter to true. Please refer to the e2e.test.ts and service.test.ts test suites as examples. Note : You must select a unique server port number for each test class because the test engine (vitest) will instantiate a new Javascript V8 engine for each test class. Composable loader Since version 4.3.3, the build script \"npm run build\" will generate two \"preloader.ts\" files, one in the src folder and the other in the test folder. For unit tests, please import the preloader.ts file under the \"test\" folder because it can initialize Composable functions in your unit tests. Pseudo annotation The TypeScript annotation preload only works in the main application under the src folder. To support writing composable functions for unit test purpose, the class scanner supports a concept called \"pseudo annotation\". This can be done by commenting out the \"preload annotation\" like this: import { Composable, EventEnvelope } from \"mercury-composable\"; export class SimpleTestTask implements Composable { static readonly routeName = 'simple.test.task' // @preload(SimpleTestTask.routeName, 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { return evt.setHeader('type', 'simple-test'); } } The class scanner will recognize the commented-out \"preload\" annotation and register the composable function accordingly. However, functions with commented-out \"preload\" annotation are only visible in unit tests under the \"test\" folder. They are not active when declared in the main application under the \"src\" folder. The ability to write composable functions in unit tests allows the developer to use composable design pattern consistently in the main application and unit tests. Unit test limitation Although the preload annotation is not supported in TypeScript classes in the \"test\" folder, you can use \"pseudo annotation\" described earlier when writing composable functions that are used in unit tests. Similar to its Java counterpart, the configuration management system will use configuration files in composable libraries and those configuration files in \"src/resources\" folder if the requested configuration file does not exist in the \"tests/resources\" folder. However, the application.yml file must be presented in the \"tests/resources\" folder when running unit tests. The system does not scan the \"src/resources\" or composable libraries for this base configuration file. Convenient utility classes The Utility and MultiLevelMap classes are convenient tools for unit tests. The MultiLevelMap supports reading an element using the convenient \"dot and bracket\" format. For example, given a map like this: { \"body\": { \"time\": \"2023-03-27T18:10:34.234Z\", \"hello\": [1, 2, 3] } } Example Command Result 1 map.getElement(\"body.time\") 2023-03-27T18:10:34.234Z 2 map.getElement(\"body.hello[2]\") 3 Event Flow mocking framework We recommend using Event Script to write Composable application for highest level of decoupling. Event Script supports sophisticated event choreography by configuration. In Event Script, you have a event flow configuration and a few Composable functions in an application. Composable functions are self-contained with zero dependencies with other composable functions. You can invoke an event flow from an event flow adapter. The most common flow adapter is the \"HTTP flow adapter\" and it is available as a built-in module in the event-script-engine module in the system. You can associate many REST endpoints to the HTTP flow adapter. Since function routes for each composable function is defined in a event flow configuration and the same function route may be used for more than one task in the flow, the system provides a mock helper class called \"EventScriptMock\" to let your unit tests to override a task's function routes during test. In the following unit test example for a \"pipeline\" test, we created a mock function \"my.mock.function\" to override the \"no.op\" function that is associated with the first task \"echo.one\" in a pipeline. The original \"no.op\" function is an echo function. The mocked function increments a counter in addition to just echoing the input payload. In this fashion, the unit test can count the number of iteration of a pipeline to validate the looping feature of a pipeline. The unit test programmatically registers the mock function and override an existing function route with the new route for the mock function. it('can do for-loop in pipeline', async () => { const po = new PostOffice(); const req1 = new AsyncHttpRequest().setMethod('GET') .setTargetHost(baseUrl).setUrl('/api/for-loop/test-user') .setQueryParameter('seq', '100') .setHeader('accept', 'application/json'); // the iterationCount will be incremented by \"my.mock.function\" iterationCount = 0; var mock = new EventScriptMock(\"for-loop-test\"); var previousRoute = mock.getFunctionRoute('echo.one'); var currentRoute = mock.assignFunctionRoute('echo.one', 'my.mock.function').getFunctionRoute('echo.one'); expect(previousRoute).toBe('no.op'); expect(currentRoute).toBe('my.mock.function'); const reqEvent = new EventEnvelope().setTo(ASYNC_HTTP_CLIENT).setBody(req1.toMap()); const result = await po.request(reqEvent); expect(result.getStatus()).toBe(200); expect(result.getBody() instanceof Object); const map = new MultiLevelMap(result.getBody() as object); expect(result.getHeader('content-type')).toBe('application/json'); expect(map.getElement(\"data.sequence\")).toBe(100); expect(map.getElement(\"data.user\")).toBe('test-user'); expect(map.getElement(\"n\")).toBe(3); expect(iterationCount).toBe(3); }); When the event flow finishes, you will see an \"end-of-flow\" log like this. It shows that the function route for the \"echo.one\" task has been changed to \"my.mock.function\". This end-of-flow log is useful during application development and tests so that the developer knows exactly which function has been executed. Flow for-loop-test (0afcf555fc4141f4a16393422e468dc9) completed. Run 11 tasks in 28 ms. [ sequential.one, echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.four(no.op) ] Deployment The npm run build command will generate an executable javascript bundle in the \"dist\" folder. Your pipeline can deploy the bundle in a Docker instance accordingly. Composable application is designed to be deployable using Kubernetes or serverless. Distributed tracing The system has a built-in distributed tracing feature. You can enable tracing for any REST endpoint by adding \"tracing=true\" in the endpoint definition in the \"rest.yaml\" configuration file. You may also upload performance metrics from the distributed tracing data to your favorite telemetry system dashboard. To do that, please implement a custom metrics function with the route name distributed.trace.forwarder . The input to the function will be a JSON of key-values like this: trace={path=/api/upload/demo, service=hello.upload, success=true, origin=2023032731e2a5eeae8f4da09f3d9ac6b55fb0a4, exec_time=77.462, start=2023-03-27T19:38:30.061Z, from=http.request, id=12345, round_trip=132.296, status=200} The system will detect if distributed.trace.forwarder is available. If yes, it will forward performance metrics from distributed trace to your custom function. Importing core library from corporate artifactory While you may use github as a repository to test drive your applications, you should build and publish the mercury-composable library to your enterprise \"npm\" artifactory. Please consult your DevSecOps colleagues for pipeline setup procedure. It would vary from one organization to another. If you publish the mercury-composable to your own artifactory as another package name, you can point it to your corporate artifactory in the package.json of your application like this: \"scripts\": { \"clean\": \"node clean.js && node placeholder.js\", \"preload\": \"node preloader.js\", \"prebuild\": \"npm run lint\", \"build\": \"npm run preload && tsc -p tsconfig.json && node copy-resource-files.js\", \"build:watch\": \"tsc -w -p tsconfig.json\", \"lint\": \"eslint . --fix\", \"test\": \"vitest run\", \"test:watch\": \"vitest\" }, \"dependencies\": { \"mercury-composable\": \"npm:actual-published-package-name\" } In the above example, it assumes the actual package name that is published from mercury-composable core library is \"actual-published-package-name\", the package name \"mercury-composable\" becomes an alias so that you can keep the import statements that point to \"mercury-composable\" unchanged. Once you have updated the package.json file in the \"examples\" folder, you may run \"npm run build\". This verifies that the example application can import from the newly published mercury-composable core library in your own artifactory. Note : To publish the library to your enterprise npm artifactory, you may need to remove the package-lock.json and perform a npm install using your enterprise npm registry. This would ensure a new package-lock.json is generated based on your artifactory requirement. Chapter-4 Home Chapter-6 Event Script Syntax Table of Contents Event over HTTP","title":"Chapter-5"},{"location":"guides/CHAPTER-5/#build-test-and-deploy","text":"The first step in writing an application is to create an entry point for your application.","title":"Build, Test and Deploy"},{"location":"guides/CHAPTER-5/#main-application","text":"A minimalist main application template is shown as follows: import { ComposableLoader } from './preload/preload.js'; async function main() { // This assumes you have a \".env\" environment variable file. If not, remove this line. process.loadEnvFile(); // Load composable functions into memory and automatically starts your application modules await ComposableLoader.initialize(); } // run the application main(); In your application.yml configuration file, you would configure autostart modules like this: modules.autostart: - 'main.app' In the above example, the \"main.app\" is defined in the main-application.ts in the composable-example project. It looks like this: export class MainApp implements Composable { @preload('main.app') initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // put your start up business logic here log.info(\"Application started\"); // release this function to guarantee that it is executed only once Platform.getInstance().release('main.app'); // return value is ignored because start up code runs asynchronously return true; } } You can also build and run the application from command line like this: cd sandbox/composable-nodejs-example npm install npm run build npm run test Since all functions are connected using the in-memory event bus, you can test any function by sending events from a unit test module.","title":"Main application"},{"location":"guides/CHAPTER-5/#writing-your-functions","text":"Please follow the step-by-step learning guide in Chapter-1 to write your own functions. You can then configure new REST endpoints to use your new functions.","title":"Writing your functions"},{"location":"guides/CHAPTER-5/#http-forwarding","text":"In Chapter-3 , we have presented the configuration syntax for the \"rest.yaml\" REST automation definition file. Please review the sample rest.yaml file in the lambda-example project. You may notice that it has an entry for HTTP forwarding. The following entry in the sample rest.yaml file illustrates an HTTP forwarding endpoint. In HTTP forwarding, you can replace the \"service\" route name with a direct HTTP target host. You can do \"URL rewrite\" to change the URL path to the target endpoint path. In the below example, /api/v1/* will be mapped to /api/* in the target endpoint. - service: \"http://127.0.0.1:${rest.server.port}\" trust_all_cert: true methods: ['GET', 'PUT', 'POST'] url: \"/api/v1/*\" url_rewrite: ['/api/v1', '/api'] timeout: 20 cors: cors_1 headers: header_1 tracing: true","title":"HTTP forwarding"},{"location":"guides/CHAPTER-5/#sending-http-request-event-to-more-than-one-service","text":"One feature in REST automation \"rest.yaml\" configuration is that you can configure more than one function in the \"service\" section. In the following example, there are two function route names (\"hello.world\" and \"hello.copy\"). The first one \"hello.world\" is the primary service provider. The second one \"hello.copy\" will receive a copy of the incoming event. This feature allows you to write new version of a function without disruption to current functionality. Once you are happy with the new version of function, you can route the endpoint directly to the new version by updating the \"rest.yaml\" configuration file. - service: [\"hello.world\", \"hello.copy\"]","title":"Sending HTTP request event to more than one service"},{"location":"guides/CHAPTER-5/#writing-a-unit-test","text":"In unit tests, we want to tell the system to use the \"tests/resources\" folder to override the \"src/resources\" folder so that we can adjust the configuration to test different scenarios. This can be done by using the ComposableLoader's initialize method in the BeforeAll section like this: import { ComposableLoader } from '../test/preload/preload.ts'; describe('End-to-end tests', () => { beforeAll(async () => { await ComposableLoader.initialize(8305, true); }); // your unit test here afterAll(async () => { await Platform.getInstance().stop(); // give console.log a moment to finish await util.sleep(2000); log.info(\"End-to-end tests completed\"); }); it('can do health check', async () => { const po = new PostOffice(); const req = new EventEnvelope().setTo('demo.health').setHeader('type', 'health'); const result = await po.request(req, 2000); expect(result).toBeTruthy(); expect(result.getBody()).toEqual({\"status\": \"demo.service is running fine\"}); }); } In the above example, we set the server port for REST automation to 8305 and set the \"unit test\" parameter to true. Please refer to the e2e.test.ts and service.test.ts test suites as examples. Note : You must select a unique server port number for each test class because the test engine (vitest) will instantiate a new Javascript V8 engine for each test class.","title":"Writing a unit test"},{"location":"guides/CHAPTER-5/#composable-loader","text":"Since version 4.3.3, the build script \"npm run build\" will generate two \"preloader.ts\" files, one in the src folder and the other in the test folder. For unit tests, please import the preloader.ts file under the \"test\" folder because it can initialize Composable functions in your unit tests.","title":"Composable loader"},{"location":"guides/CHAPTER-5/#pseudo-annotation","text":"The TypeScript annotation preload only works in the main application under the src folder. To support writing composable functions for unit test purpose, the class scanner supports a concept called \"pseudo annotation\". This can be done by commenting out the \"preload annotation\" like this: import { Composable, EventEnvelope } from \"mercury-composable\"; export class SimpleTestTask implements Composable { static readonly routeName = 'simple.test.task' // @preload(SimpleTestTask.routeName, 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { return evt.setHeader('type', 'simple-test'); } } The class scanner will recognize the commented-out \"preload\" annotation and register the composable function accordingly. However, functions with commented-out \"preload\" annotation are only visible in unit tests under the \"test\" folder. They are not active when declared in the main application under the \"src\" folder. The ability to write composable functions in unit tests allows the developer to use composable design pattern consistently in the main application and unit tests.","title":"Pseudo annotation"},{"location":"guides/CHAPTER-5/#unit-test-limitation","text":"Although the preload annotation is not supported in TypeScript classes in the \"test\" folder, you can use \"pseudo annotation\" described earlier when writing composable functions that are used in unit tests. Similar to its Java counterpart, the configuration management system will use configuration files in composable libraries and those configuration files in \"src/resources\" folder if the requested configuration file does not exist in the \"tests/resources\" folder. However, the application.yml file must be presented in the \"tests/resources\" folder when running unit tests. The system does not scan the \"src/resources\" or composable libraries for this base configuration file.","title":"Unit test limitation"},{"location":"guides/CHAPTER-5/#convenient-utility-classes","text":"The Utility and MultiLevelMap classes are convenient tools for unit tests. The MultiLevelMap supports reading an element using the convenient \"dot and bracket\" format. For example, given a map like this: { \"body\": { \"time\": \"2023-03-27T18:10:34.234Z\", \"hello\": [1, 2, 3] } } Example Command Result 1 map.getElement(\"body.time\") 2023-03-27T18:10:34.234Z 2 map.getElement(\"body.hello[2]\") 3","title":"Convenient utility classes"},{"location":"guides/CHAPTER-5/#event-flow-mocking-framework","text":"We recommend using Event Script to write Composable application for highest level of decoupling. Event Script supports sophisticated event choreography by configuration. In Event Script, you have a event flow configuration and a few Composable functions in an application. Composable functions are self-contained with zero dependencies with other composable functions. You can invoke an event flow from an event flow adapter. The most common flow adapter is the \"HTTP flow adapter\" and it is available as a built-in module in the event-script-engine module in the system. You can associate many REST endpoints to the HTTP flow adapter. Since function routes for each composable function is defined in a event flow configuration and the same function route may be used for more than one task in the flow, the system provides a mock helper class called \"EventScriptMock\" to let your unit tests to override a task's function routes during test. In the following unit test example for a \"pipeline\" test, we created a mock function \"my.mock.function\" to override the \"no.op\" function that is associated with the first task \"echo.one\" in a pipeline. The original \"no.op\" function is an echo function. The mocked function increments a counter in addition to just echoing the input payload. In this fashion, the unit test can count the number of iteration of a pipeline to validate the looping feature of a pipeline. The unit test programmatically registers the mock function and override an existing function route with the new route for the mock function. it('can do for-loop in pipeline', async () => { const po = new PostOffice(); const req1 = new AsyncHttpRequest().setMethod('GET') .setTargetHost(baseUrl).setUrl('/api/for-loop/test-user') .setQueryParameter('seq', '100') .setHeader('accept', 'application/json'); // the iterationCount will be incremented by \"my.mock.function\" iterationCount = 0; var mock = new EventScriptMock(\"for-loop-test\"); var previousRoute = mock.getFunctionRoute('echo.one'); var currentRoute = mock.assignFunctionRoute('echo.one', 'my.mock.function').getFunctionRoute('echo.one'); expect(previousRoute).toBe('no.op'); expect(currentRoute).toBe('my.mock.function'); const reqEvent = new EventEnvelope().setTo(ASYNC_HTTP_CLIENT).setBody(req1.toMap()); const result = await po.request(reqEvent); expect(result.getStatus()).toBe(200); expect(result.getBody() instanceof Object); const map = new MultiLevelMap(result.getBody() as object); expect(result.getHeader('content-type')).toBe('application/json'); expect(map.getElement(\"data.sequence\")).toBe(100); expect(map.getElement(\"data.user\")).toBe('test-user'); expect(map.getElement(\"n\")).toBe(3); expect(iterationCount).toBe(3); }); When the event flow finishes, you will see an \"end-of-flow\" log like this. It shows that the function route for the \"echo.one\" task has been changed to \"my.mock.function\". This end-of-flow log is useful during application development and tests so that the developer knows exactly which function has been executed. Flow for-loop-test (0afcf555fc4141f4a16393422e468dc9) completed. Run 11 tasks in 28 ms. [ sequential.one, echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.four(no.op) ]","title":"Event Flow mocking framework"},{"location":"guides/CHAPTER-5/#deployment","text":"The npm run build command will generate an executable javascript bundle in the \"dist\" folder. Your pipeline can deploy the bundle in a Docker instance accordingly. Composable application is designed to be deployable using Kubernetes or serverless.","title":"Deployment"},{"location":"guides/CHAPTER-5/#distributed-tracing","text":"The system has a built-in distributed tracing feature. You can enable tracing for any REST endpoint by adding \"tracing=true\" in the endpoint definition in the \"rest.yaml\" configuration file. You may also upload performance metrics from the distributed tracing data to your favorite telemetry system dashboard. To do that, please implement a custom metrics function with the route name distributed.trace.forwarder . The input to the function will be a JSON of key-values like this: trace={path=/api/upload/demo, service=hello.upload, success=true, origin=2023032731e2a5eeae8f4da09f3d9ac6b55fb0a4, exec_time=77.462, start=2023-03-27T19:38:30.061Z, from=http.request, id=12345, round_trip=132.296, status=200} The system will detect if distributed.trace.forwarder is available. If yes, it will forward performance metrics from distributed trace to your custom function.","title":"Distributed tracing"},{"location":"guides/CHAPTER-5/#importing-core-library-from-corporate-artifactory","text":"While you may use github as a repository to test drive your applications, you should build and publish the mercury-composable library to your enterprise \"npm\" artifactory. Please consult your DevSecOps colleagues for pipeline setup procedure. It would vary from one organization to another. If you publish the mercury-composable to your own artifactory as another package name, you can point it to your corporate artifactory in the package.json of your application like this: \"scripts\": { \"clean\": \"node clean.js && node placeholder.js\", \"preload\": \"node preloader.js\", \"prebuild\": \"npm run lint\", \"build\": \"npm run preload && tsc -p tsconfig.json && node copy-resource-files.js\", \"build:watch\": \"tsc -w -p tsconfig.json\", \"lint\": \"eslint . --fix\", \"test\": \"vitest run\", \"test:watch\": \"vitest\" }, \"dependencies\": { \"mercury-composable\": \"npm:actual-published-package-name\" } In the above example, it assumes the actual package name that is published from mercury-composable core library is \"actual-published-package-name\", the package name \"mercury-composable\" becomes an alias so that you can keep the import statements that point to \"mercury-composable\" unchanged. Once you have updated the package.json file in the \"examples\" folder, you may run \"npm run build\". This verifies that the example application can import from the newly published mercury-composable core library in your own artifactory. Note : To publish the library to your enterprise npm artifactory, you may need to remove the package-lock.json and perform a npm install using your enterprise npm registry. This would ensure a new package-lock.json is generated based on your artifactory requirement. Chapter-4 Home Chapter-6 Event Script Syntax Table of Contents Event over HTTP","title":"Importing core library from corporate artifactory"},{"location":"guides/CHAPTER-6/","text":"Event over HTTP The in-memory event system allows functions to communicate with each other in the same application memory space. In composable architecture, applications are modular components in a network. Some transactions may require the services of more than one application. \"Event over HTTP\" extends the event system beyond a single application. The Event API service ( event.api.service ) is a built-in function in the system. The Event API endpoint To enable \"Event over HTTP\", you must first turn on the REST automation engine with the following parameters in the application.properties file: server.port: 8085 rest.automation: true and then check if the following entry is configured in the \"rest.yaml\" endpoint definition file. If not, update \"rest.yaml\" accordingly. The \"timeout\" value is set to 60 seconds to fit common use cases. - service: [ \"event.api.service\" ] methods: [ 'POST' ] url: \"/api/event\" timeout: 60s tracing: true This will expose the Event API endpoint at port 8085 and URL \"/api/event\". In kubernetes, The Event API endpoint of each application is reachable through internal DNS and there is no need to create \"ingress\" for this purpose. Event-over-HTTP configuration You can enable Event-over-HTTP configuration by adding this parameter in application.yml: # # Optional event-over-http target maps # yaml.event.over.http: classpath:/event-over-http.yaml and then create the configuration file \"event-over-http.yaml\" like this: event: http: - route: 'hello.world' target: 'http://127.0.0.1:${another.app.port}/api/event' # optional security headers headers: authorization: 'demo' In the above example, the route hello.world will be rerouted to the target URLs. If additional authentication is required for the peer's \"/api/event\" endpoint, you may add a set of security headers in each route. When you send asynchronous event or make a RPC call to \"hello.world\" service, it will be forwarded to the peer's \"event-over-HTTP\" endpoint ( /api/event ) accordingly. If \"hello.world\" is a task in an event flow, the event manager will make the \"Event over HTTP\" to the target service. You may also add environment variable or base configuration references to the application.yaml file, such as \"another.app.port\" in this example. Note : The target function must declare itself as PUBLIC in the preload annotation. Otherwise, you will get a HTTP-403 exception. Advantages The Event API exposes all public functions of an application instance to the network using a single REST endpoint. The advantages of Event API includes: Convenient - you do not need to write or configure individual endpoint for each public service Efficient - events are transported in binary format from one application to another Secure - you can protect the Event API endpoint with an authentication service The following configuration adds authentication service to the Event API endpoint: - service: [ \"event.api.service\" ] methods: [ 'POST' ] url: \"/api/event\" timeout: 60s authentication: \"v1.api.auth\" tracing: true This enforces every incoming request to the Event API endpoint to be authenticated by the \"v1.api.auth\" service before passing to the Event API service. You can plug in your own authentication service such as OAuth 2.0 \"bearer token\" validation. Please refer to Chapter-3 - REST automation for details. Chapter-5 Home Chapter-7 Build, test and deploy Table of Contents API overview","title":"Chapter-6"},{"location":"guides/CHAPTER-6/#event-over-http","text":"The in-memory event system allows functions to communicate with each other in the same application memory space. In composable architecture, applications are modular components in a network. Some transactions may require the services of more than one application. \"Event over HTTP\" extends the event system beyond a single application. The Event API service ( event.api.service ) is a built-in function in the system.","title":"Event over HTTP"},{"location":"guides/CHAPTER-6/#the-event-api-endpoint","text":"To enable \"Event over HTTP\", you must first turn on the REST automation engine with the following parameters in the application.properties file: server.port: 8085 rest.automation: true and then check if the following entry is configured in the \"rest.yaml\" endpoint definition file. If not, update \"rest.yaml\" accordingly. The \"timeout\" value is set to 60 seconds to fit common use cases. - service: [ \"event.api.service\" ] methods: [ 'POST' ] url: \"/api/event\" timeout: 60s tracing: true This will expose the Event API endpoint at port 8085 and URL \"/api/event\". In kubernetes, The Event API endpoint of each application is reachable through internal DNS and there is no need to create \"ingress\" for this purpose.","title":"The Event API endpoint"},{"location":"guides/CHAPTER-6/#event-over-http-configuration","text":"You can enable Event-over-HTTP configuration by adding this parameter in application.yml: # # Optional event-over-http target maps # yaml.event.over.http: classpath:/event-over-http.yaml and then create the configuration file \"event-over-http.yaml\" like this: event: http: - route: 'hello.world' target: 'http://127.0.0.1:${another.app.port}/api/event' # optional security headers headers: authorization: 'demo' In the above example, the route hello.world will be rerouted to the target URLs. If additional authentication is required for the peer's \"/api/event\" endpoint, you may add a set of security headers in each route. When you send asynchronous event or make a RPC call to \"hello.world\" service, it will be forwarded to the peer's \"event-over-HTTP\" endpoint ( /api/event ) accordingly. If \"hello.world\" is a task in an event flow, the event manager will make the \"Event over HTTP\" to the target service. You may also add environment variable or base configuration references to the application.yaml file, such as \"another.app.port\" in this example. Note : The target function must declare itself as PUBLIC in the preload annotation. Otherwise, you will get a HTTP-403 exception.","title":"Event-over-HTTP configuration"},{"location":"guides/CHAPTER-6/#advantages","text":"The Event API exposes all public functions of an application instance to the network using a single REST endpoint. The advantages of Event API includes: Convenient - you do not need to write or configure individual endpoint for each public service Efficient - events are transported in binary format from one application to another Secure - you can protect the Event API endpoint with an authentication service The following configuration adds authentication service to the Event API endpoint: - service: [ \"event.api.service\" ] methods: [ 'POST' ] url: \"/api/event\" timeout: 60s authentication: \"v1.api.auth\" tracing: true This enforces every incoming request to the Event API endpoint to be authenticated by the \"v1.api.auth\" service before passing to the Event API service. You can plug in your own authentication service such as OAuth 2.0 \"bearer token\" validation. Please refer to Chapter-3 - REST automation for details. Chapter-5 Home Chapter-7 Build, test and deploy Table of Contents API overview","title":"Advantages"},{"location":"guides/CHAPTER-7/","text":"API overview Main application Each application has an entry point. You may implement the main entry point like this: import { Logger, Platform, RestAutomation } from 'mercury-composable'; import { ComposableLoader } from './preload/preload.js'; const log = Logger.getInstance(); async function main() { // Load composable functions into memory and initialize configuration management ComposableLoader.initialize(); const platform = Platform.getInstance(); platform.runForever(); log.info('Composable application started'); } // run the application main(); In this example, the ComposableLoader will initialize the configuration management system, the REST automation system, and register user composable functions into the event system. The default location of the system files is the \"src/resources\" folder. File / bundle Purpose application.yml Base configuration file is assumed to be under the \"src/resources\" folder rest.yaml REST endpoint configuration file is assumed to be under the \"src/resources\" folder HTML bundle HTML/CSS/JS files, if any, can be placed under the \"src/resources/public\" folder To tell the system to use a different application.yml, you can use this following statement before running the ComposableLoader.initialize() command. // resourcePath should be a fully qualified file path to the application's \"resources\" folder. const appConfig = AppConfig.getInstance(resourcePath); log.info(`Base configuration ${appConfig.getId()}`); You may override the file path for REST endpoint configuration and HTML bundle with the following: yaml.rest.automation: 'classpath:/rest.yaml' static.html.folder: 'classpath:/public' The application can be stopped with Control-C in interactive mode or the Kill command at the kernel level by a container management system such as Kubernetes. Event envelope A composable application is a collection of functions that communicate with each other in events. Each event is transported by an event envelope. Let's examine the envelope. There are 3 elements in an event envelope: Element Type Purpose 1 metadata Includes unique ID, target function name, reply address correlation ID, status, exception, trace ID and path 2 headers User defined key-value pairs 3 body Event payload (primitive or JSON object) Headers and body are optional, but you must provide at least one of them. Custom exception using AppException To reject an incoming request, you can throw an AppException like this: throw new AppException(400, \"My custom error message\"); As a best practice, we recommend using error codes that are compatible with HTTP status codes. Defining a user function in TypeScript You can write a function like this: import { preload, Composable, EventEnvelope, AsyncHttpRequest, Logger } from 'mercury-composable'; const log = Logger.getInstance(); export class DemoAuth implements Composable { @preload('v1.api.auth', 5) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { const req = new AsyncHttpRequest(evt.getBody() as object); const method = req.getMethod(); const url = req.getUrl(); log.info(`${method} ${url} authenticated`); // this is a demo so we approve all requests return true; } } You can define route name, instances, isPublic and interceptor in the preload annotation. The default values are instances=1, isPublic=false and interceptor=false. In the example, the number of instances is set to 5. You can set the number of instances from 1 to 500. The above example is a demo \"API authentication\" function. The event body is an AsyncHttpRequest object from the user because the \"rest.yaml\" routes the HTTP request to the function via its unique \"route name\". Inspect event metadata There are some reserved metadata for route name (\"my_route\"), trace ID (\"my_trace_id\") and trace path (\"my_trace_path\") in the event's headers. They do not exist in the incoming event envelope. The system automatically insert them as read-only metadata. You may inspect other event metadata such as the replyTo address and correlation ID. Note that the \"replyTo\" address is optional. It only exists when the caller is making an RPC request or callback to your function. If the caller sends an asynchronous drop-n-forget request, the \"replyTo\" value is null. Platform API You can obtain a singleton instance of the Platform object like this: const platform = Platform.getInstance(); Register a function We recommend using the ComposableLoader to search and load your functions. In some use cases where you want to create and destroy functions on demand, you can register them programmatically. For example, platform.register(HELLO_BFF_SERVICE, new HelloBff()); What is a public function? A public function is visible by any application instances in the same network. When a function is declared as \"public\", the function is reachable through the Event-over-HTTP API REST endpoint. A private function is invisible outside the memory space of the application instance that it resides. This allows application to encapsulate business logic according to domain boundary. You can assemble closely related functions as a composable application that can be deployed independently. Release a function In some use cases, you want to release a function on-demand when it is no longer required. platform.release(\"another.function\"); The above API will unload the function from memory and release it from the \"event loop\". Obtain the unique application instance ID When an application instance starts, a unique ID is generated. const originId = po.getId(); PostOffice API You can obtain an instance of the PostOffice from the input EventEnvelope of your function. const po = new PostOffice(evt); The PostOffice is the event emitter that you can use to send asynchronous events or to make RPC requests. The constructor uses the metadata in the \"headers\" argument to create a trackable instance of the event emitter. For end-to-end traceability, please use the PostOffice instance to make requests to a composable library. It maintains the same traceId and tracePath in the traceability graph. If your handleEvent method calls another method in your class, you should pass this PostOffice instance so that any event calls from the other method can propagate the tracing information. For Unit Tests, since a test does not start with the handleEvent of a LambdaFunction, you can use the following to create a PostOffice with your own traceId. The \"myRoute\" is the caller's route name. In this case, you can set it to \"unit.test\". // create a PostOffice instance in a Unit Test const po = new PostOffice(new Sender(myRoute, traceId, tracePath)); Check if a function is available You can check if a function with the named route has been deployed. if (po.exists(\"another.function\")) { // do something } Obtain the class instance of a function Since a composable function is executed as an anonymous function, the this reference is undefined inside the functional scope and thus no longer relevant to the class scope. To invoke other methods in the same class holding the composable function, the \"getMyClass()\" API can be used. async handleEvent(evt: EventEnvelope) { const po = new PostOffice(evt); const self = po.getMyClass() as HelloWorldService; // business logic here const len = await self.downloadFile(request.getStreamRoute(), request.getFileName()); } In the above example, HelloWorldService is the Composable class and the downloadFile is a non-static method in the same class. Note that you must use the event headers to instantiate the PostOffice object. Retrieve routing metadata of my function The following code segment demonstrates that you can retrieve the function's route name, worker number, optional traceId and tracePath. async handleEvent(evt: EventEnvelope) { const po = new PostOffice(evt); const route = po.getMyRoute(); const workerNumber = po.getMyInstance(); const traceId = po.getMyTraceId(); const tracePath = po.getMyTracePath(); // processing logic here } Send an asynchronous event to a function You can send an asynchronous event like this. // example-1 const event = new EventEnvelope().setTo('hello.world').setBody('test message'); po.send(event); // example-2 po.sendLater(event, 5000); Example-1 sends the text string \"test message\" to the target service named \"hello.world\". Example-2 schedules an event to be delivered 5 seconds later. Make a RPC call You can make RPC call like this: // example-1 const event = new EventEnvelope().setTo('hello.world').setBody('test message'); // the response is a result event const result = await po.request(event, 5000); // example-2 const result = await po.remoteRequest(event, 'http://peer/api/event'); // API signatures request(event: EventEnvelope, timeout = 60000): Promise<EventEnvelope> remoteRequest(event: EventEnvelope, endpoint: string, securityHeaders: object = {}, rpc=true, timeout = 60000): Promise<EventEnvelope> Example-1 makes a RPC call with a 5-second timeout to \"hello.world\". Example-2 makes an \"event over HTTP\" RPC call to \"hello.world\" in another application instance. \"Event over HTTP\" is an important topic. Please refer to Chapter 6 for more details. Make a fork-n-join parallel RPC You can make fork-n-join parallel request like this: // example const event1 = new EventEnvelope().setTo('hello.world.1').setBody('test message one'); const event2 = new EventEnvelope().setTo('hello.world.2').setBody('test message two'); const events = [event1, event2]; // the response is a list of result events const response = await po.parallelRequest(events, 5000); // API signature parallelRequest(events: Array<EventEnvelope>, timeout = 60000): Promise<Array<EventEnvelope>> Example-1 makes a RPC call with a 5-second timeout to \"hello.world\". Example-2 makes an \"event over HTTP\" RPC call to \"hello.world\" in another application instance. \"Event over HTTP\" is an important topic. Please refer to Chapter 6 for more details. Retrieve trace ID and path If you want to know the route name and optional trace ID and path, you can inspect the incoming event headers. const po = new PostOffice(evt); const myRoute = po.getMyRoute(); const traceId = po.getMyTraceId(); const tracePath = po.getMyTracePath(); const myInstance = po.getMyInstance(); Trace annotation You can add a small number of annotations if the event to your function has tracing enabled. Annotated value can be a text string, a JSON object of key-values or a list of text strings. async handleEvent(evt: EventEnvelope) { // business logic to handle the incoming event // ... // annotate the event evt.annotate(\"hello\", \"world\"); Annotations of key-values, if any, will be recorded in the trace and they are not accessible by another function. The annotated key-values will be shown in the trace like this: \"annotations\": {\"hello\": \"world\"} Note : Don't annotate sensitive information or secrets such as PII, PHI, PCI data because the trace is visible in the application log. It may also be forwarded to a centralized telemetry dashboard for visualization and analytics. Configuration API Your function can access the main application configuration management system like this: const config = AppConfig.getInstance(); // the value can be string, a primitive or a JSON object const value = config.get('my.parameter'); // the value can be read as a string const text = config.getProperty('my.parameter'); The system uses the standard dot-bracket format for a parameter name. e.g. \"hello.world\", \"some.key[2]\" You can also override the main application configuration using the set method. Additional configuration files can be added with the ConfigReader API like this: const myConfig = new ConfigReader(filePath); where filePath can use the classpath:/ or file:/ prefix. The configuration system supports environment variable or reference to the main application configuration using the dollar-bracket syntax ${reference:default_value} . e.g. \"some.key=${MY_ENV_VARIABLE}\", \"some.key=${my.key}\" Override configuration parameters at run-time You can override any configuration parameter from the command line when starting your application. node my-app.js -Dsome.key=some_value -Danother.key=another_value You can point your application to use a different base configuration file like this: node my-app.js -C/opt/config/application.yml The -C command line argument tells the system to use the configuration file in \"/opt/config/application.yml\". Exercise: try this command \"node hello-world.js -Dlog.format=json\" to start the demo app This will tell the Logger system to use JSON format instead of plain text output. The log output may look like this: { \"time\": \"2023-06-10 09:51:20.884\", \"level\": \"INFO\", \"message\": \"Event system started - 9f5c99c4d21a42cfb0115cfbaf533820\", \"module\": \"platform.js:441\" } { \"time\": \"2023-06-10 09:51:21.037\", \"level\": \"INFO\", \"message\": \"REST automation service started on port 8085\", \"module\": \"rest-automation.js:226\" } Logger The system includes a built-in logger that can log in either text or json format. The default log format is \"text\". You can override the value in the \"src/resources/application.yml\" config file. The following example sets the log format to \"json\". log.format: json Alternatively you can also override it at run-time using the \"-D\" parameter like this: node my-app.js -Dlog.format=json The logger supports line-numbering. When you run your executable javascript main program, the line number for each log message is derived from the \".js\" file compiled from the \".ts\" files. If you want to show the line number in the source \".ts\" file for easy debug, you may test your application using \"nodemon\". For simplicity, the logger is implemented without any additional library dependencies. Class scanner TypeScript supports annotations in class, method and input parameter when experimentalDecorators is turned on in tsconfig.json configuration file. However, annotation features for class, method and input parameter work differently. To support automatic scanning of Composable functions in your source project and compiled code in library packages, the system provides class scanners TypeScriptClassScanner and JavaScriptClassScanner for TypeScript and compiled Javascript code respectively. You can review the \"preloader.js\" build script in the \"examples\" folder to see how the system generates the ComposableLoader (preload.ts) source code in the \"examples/src/preload\" folder. The two class scanners extract annotation's parameters so that your application can do interesting thing based on the annotations. This is similar to the concept of annotated key-values in the Java programming language. For example, you can do something like this to scan the \"preload\" annotated methods of TypeScript files in the source folder: const root = getCurrentFolder(); const src = root + 'src'; const scanner = new TypeScriptClassScanner(src, 'preload'); const result = await scanner.scan(); // extract of the result set is shown below { \"classes\": { \"HelloConcurrent\": \"../services/hello-concurrent.js\", \"HelloWorld\": \"../services/hello-world.js\", }, \"parents\": { \"HelloConcurrent\": { \"extends\": [], \"implements\": [ \"Composable\" ] }, \"HelloWorld\": { \"extends\": [], \"implements\": [ \"Composable\" ] } }, \"parameters\": { \"HelloConcurrent\": [ \"HelloConcurrent.routeName\", \"10\" ], \"HelloWorld\": [ \"HelloWorld.routeName\", \"10\", \"false\" ] }, \"methods\": { \"DemoHealthCheck\": \"initialize\", \"HelloConcurrent\": \"initialize\", \"HelloWorld\": \"initialize\", } } const map = new MultiLevelMap(result); // the MultiLevelMap allows you to retrieve key-value using the dot-bracket format. // e.g. you can validate the method name and parameters // 'initialize' == map.getElement(`methods.${cls}`) && map.exists(`parameters.${cls}`) To scan compiled JavaScript files in library packages, configure application.yaml with the web.component.scan parameter. Use a comma separated list when scanning more than one package. web.component.scan: 'my-package-one, my-package-two' You can then use the JavaScriptClassScanner like this: const scanner = new JavaScriptClassScanner(parent, target, 'preload'); const result = await scanner.scan(); // sample result set below { \"classes\": { \"NoOp\": \"../node_modules/mercury-composable/dist/services/no-op.js\" }, \"parameters\": { \"NoOp\": [ \"'no.op'\", \"10\" ] }, \"methods\": { \"NoOp\": \"initialize\" } } Note : For simplicity, the scanners support method annotations only. Class and input parameter annotations are not handled. Export of composable functions You can write composable modules as libraries for other composable application to use. The build script preload.js will use the TypeScriptClassScanner to scan composable functions from the source code of your application and use the JavaScriptClassCanner to scan composable modules from your libraries. When writing a composable library, please ensure that the composable functions in your library must export the composable modules in the index.ts file. Without this setup, the user application may fail to build. For the application that uses your composable libraries, it must include the package name in the web.component.scan parameter in the application.yml configuration file. Minimalist API design For configuration based Event Choreography, please refer to Chapter-4 for more details. You can build powerful composable application without a lot of APIs. \"Less\" is always better in composable methodology. We do not recommend \"event orchestration by code\" because it would lead to tight coupling of software modules. Co-existence with other development frameworks Mercury libraries are designed to co-exist with your favorite frameworks and tools. Inside a class implementing a composable function, you can use any coding style and frameworks as you like, including sequential, object-oriented and reactive programming styles. Mercury has a built-in lightweight non-blocking HTTP server based on Express, but you can also use other application server framework with it. Template application for quick start You can use the composable-example project as a template to start writing your own applications. Source code update frequency This project is licensed under the Apache 2.0 open sources license. We will update the public codebase after it passes regression tests and meets stability and performance benchmarks in our production systems. Mercury Composable is developed as an engine for you to build the latest cloud native applications. Composable technology is evolving rapidly. We would exercise best effort to keep the essential internals and core APIs stable. Please browse the latest Developer Guide, release notes and Javadoc for any breaking API changes. Technical support For enterprise clients, technical support is available. Please contact your Accenture representative for details. Chapter-6 Home Chapter-8 Event over HTTP Table of Contents Custom Flow Adapter","title":"Chapter-7"},{"location":"guides/CHAPTER-7/#api-overview","text":"","title":"API overview"},{"location":"guides/CHAPTER-7/#main-application","text":"Each application has an entry point. You may implement the main entry point like this: import { Logger, Platform, RestAutomation } from 'mercury-composable'; import { ComposableLoader } from './preload/preload.js'; const log = Logger.getInstance(); async function main() { // Load composable functions into memory and initialize configuration management ComposableLoader.initialize(); const platform = Platform.getInstance(); platform.runForever(); log.info('Composable application started'); } // run the application main(); In this example, the ComposableLoader will initialize the configuration management system, the REST automation system, and register user composable functions into the event system. The default location of the system files is the \"src/resources\" folder. File / bundle Purpose application.yml Base configuration file is assumed to be under the \"src/resources\" folder rest.yaml REST endpoint configuration file is assumed to be under the \"src/resources\" folder HTML bundle HTML/CSS/JS files, if any, can be placed under the \"src/resources/public\" folder To tell the system to use a different application.yml, you can use this following statement before running the ComposableLoader.initialize() command. // resourcePath should be a fully qualified file path to the application's \"resources\" folder. const appConfig = AppConfig.getInstance(resourcePath); log.info(`Base configuration ${appConfig.getId()}`); You may override the file path for REST endpoint configuration and HTML bundle with the following: yaml.rest.automation: 'classpath:/rest.yaml' static.html.folder: 'classpath:/public' The application can be stopped with Control-C in interactive mode or the Kill command at the kernel level by a container management system such as Kubernetes.","title":"Main application"},{"location":"guides/CHAPTER-7/#event-envelope","text":"A composable application is a collection of functions that communicate with each other in events. Each event is transported by an event envelope. Let's examine the envelope. There are 3 elements in an event envelope: Element Type Purpose 1 metadata Includes unique ID, target function name, reply address correlation ID, status, exception, trace ID and path 2 headers User defined key-value pairs 3 body Event payload (primitive or JSON object) Headers and body are optional, but you must provide at least one of them.","title":"Event envelope"},{"location":"guides/CHAPTER-7/#custom-exception-using-appexception","text":"To reject an incoming request, you can throw an AppException like this: throw new AppException(400, \"My custom error message\"); As a best practice, we recommend using error codes that are compatible with HTTP status codes.","title":"Custom exception using AppException"},{"location":"guides/CHAPTER-7/#defining-a-user-function-in-typescript","text":"You can write a function like this: import { preload, Composable, EventEnvelope, AsyncHttpRequest, Logger } from 'mercury-composable'; const log = Logger.getInstance(); export class DemoAuth implements Composable { @preload('v1.api.auth', 5) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { const req = new AsyncHttpRequest(evt.getBody() as object); const method = req.getMethod(); const url = req.getUrl(); log.info(`${method} ${url} authenticated`); // this is a demo so we approve all requests return true; } } You can define route name, instances, isPublic and interceptor in the preload annotation. The default values are instances=1, isPublic=false and interceptor=false. In the example, the number of instances is set to 5. You can set the number of instances from 1 to 500. The above example is a demo \"API authentication\" function. The event body is an AsyncHttpRequest object from the user because the \"rest.yaml\" routes the HTTP request to the function via its unique \"route name\".","title":"Defining a user function in TypeScript"},{"location":"guides/CHAPTER-7/#inspect-event-metadata","text":"There are some reserved metadata for route name (\"my_route\"), trace ID (\"my_trace_id\") and trace path (\"my_trace_path\") in the event's headers. They do not exist in the incoming event envelope. The system automatically insert them as read-only metadata. You may inspect other event metadata such as the replyTo address and correlation ID. Note that the \"replyTo\" address is optional. It only exists when the caller is making an RPC request or callback to your function. If the caller sends an asynchronous drop-n-forget request, the \"replyTo\" value is null.","title":"Inspect event metadata"},{"location":"guides/CHAPTER-7/#platform-api","text":"You can obtain a singleton instance of the Platform object like this: const platform = Platform.getInstance();","title":"Platform API"},{"location":"guides/CHAPTER-7/#register-a-function","text":"We recommend using the ComposableLoader to search and load your functions. In some use cases where you want to create and destroy functions on demand, you can register them programmatically. For example, platform.register(HELLO_BFF_SERVICE, new HelloBff());","title":"Register a function"},{"location":"guides/CHAPTER-7/#what-is-a-public-function","text":"A public function is visible by any application instances in the same network. When a function is declared as \"public\", the function is reachable through the Event-over-HTTP API REST endpoint. A private function is invisible outside the memory space of the application instance that it resides. This allows application to encapsulate business logic according to domain boundary. You can assemble closely related functions as a composable application that can be deployed independently.","title":"What is a public function?"},{"location":"guides/CHAPTER-7/#release-a-function","text":"In some use cases, you want to release a function on-demand when it is no longer required. platform.release(\"another.function\"); The above API will unload the function from memory and release it from the \"event loop\".","title":"Release a function"},{"location":"guides/CHAPTER-7/#obtain-the-unique-application-instance-id","text":"When an application instance starts, a unique ID is generated. const originId = po.getId();","title":"Obtain the unique application instance ID"},{"location":"guides/CHAPTER-7/#postoffice-api","text":"You can obtain an instance of the PostOffice from the input EventEnvelope of your function. const po = new PostOffice(evt); The PostOffice is the event emitter that you can use to send asynchronous events or to make RPC requests. The constructor uses the metadata in the \"headers\" argument to create a trackable instance of the event emitter. For end-to-end traceability, please use the PostOffice instance to make requests to a composable library. It maintains the same traceId and tracePath in the traceability graph. If your handleEvent method calls another method in your class, you should pass this PostOffice instance so that any event calls from the other method can propagate the tracing information. For Unit Tests, since a test does not start with the handleEvent of a LambdaFunction, you can use the following to create a PostOffice with your own traceId. The \"myRoute\" is the caller's route name. In this case, you can set it to \"unit.test\". // create a PostOffice instance in a Unit Test const po = new PostOffice(new Sender(myRoute, traceId, tracePath));","title":"PostOffice API"},{"location":"guides/CHAPTER-7/#check-if-a-function-is-available","text":"You can check if a function with the named route has been deployed. if (po.exists(\"another.function\")) { // do something }","title":"Check if a function is available"},{"location":"guides/CHAPTER-7/#obtain-the-class-instance-of-a-function","text":"Since a composable function is executed as an anonymous function, the this reference is undefined inside the functional scope and thus no longer relevant to the class scope. To invoke other methods in the same class holding the composable function, the \"getMyClass()\" API can be used. async handleEvent(evt: EventEnvelope) { const po = new PostOffice(evt); const self = po.getMyClass() as HelloWorldService; // business logic here const len = await self.downloadFile(request.getStreamRoute(), request.getFileName()); } In the above example, HelloWorldService is the Composable class and the downloadFile is a non-static method in the same class. Note that you must use the event headers to instantiate the PostOffice object.","title":"Obtain the class instance of a function"},{"location":"guides/CHAPTER-7/#retrieve-routing-metadata-of-my-function","text":"The following code segment demonstrates that you can retrieve the function's route name, worker number, optional traceId and tracePath. async handleEvent(evt: EventEnvelope) { const po = new PostOffice(evt); const route = po.getMyRoute(); const workerNumber = po.getMyInstance(); const traceId = po.getMyTraceId(); const tracePath = po.getMyTracePath(); // processing logic here }","title":"Retrieve routing metadata of my function"},{"location":"guides/CHAPTER-7/#send-an-asynchronous-event-to-a-function","text":"You can send an asynchronous event like this. // example-1 const event = new EventEnvelope().setTo('hello.world').setBody('test message'); po.send(event); // example-2 po.sendLater(event, 5000); Example-1 sends the text string \"test message\" to the target service named \"hello.world\". Example-2 schedules an event to be delivered 5 seconds later.","title":"Send an asynchronous event to a function"},{"location":"guides/CHAPTER-7/#make-a-rpc-call","text":"You can make RPC call like this: // example-1 const event = new EventEnvelope().setTo('hello.world').setBody('test message'); // the response is a result event const result = await po.request(event, 5000); // example-2 const result = await po.remoteRequest(event, 'http://peer/api/event'); // API signatures request(event: EventEnvelope, timeout = 60000): Promise<EventEnvelope> remoteRequest(event: EventEnvelope, endpoint: string, securityHeaders: object = {}, rpc=true, timeout = 60000): Promise<EventEnvelope> Example-1 makes a RPC call with a 5-second timeout to \"hello.world\". Example-2 makes an \"event over HTTP\" RPC call to \"hello.world\" in another application instance. \"Event over HTTP\" is an important topic. Please refer to Chapter 6 for more details.","title":"Make a RPC call"},{"location":"guides/CHAPTER-7/#make-a-fork-n-join-parallel-rpc","text":"You can make fork-n-join parallel request like this: // example const event1 = new EventEnvelope().setTo('hello.world.1').setBody('test message one'); const event2 = new EventEnvelope().setTo('hello.world.2').setBody('test message two'); const events = [event1, event2]; // the response is a list of result events const response = await po.parallelRequest(events, 5000); // API signature parallelRequest(events: Array<EventEnvelope>, timeout = 60000): Promise<Array<EventEnvelope>> Example-1 makes a RPC call with a 5-second timeout to \"hello.world\". Example-2 makes an \"event over HTTP\" RPC call to \"hello.world\" in another application instance. \"Event over HTTP\" is an important topic. Please refer to Chapter 6 for more details.","title":"Make a fork-n-join parallel RPC"},{"location":"guides/CHAPTER-7/#retrieve-trace-id-and-path","text":"If you want to know the route name and optional trace ID and path, you can inspect the incoming event headers. const po = new PostOffice(evt); const myRoute = po.getMyRoute(); const traceId = po.getMyTraceId(); const tracePath = po.getMyTracePath(); const myInstance = po.getMyInstance();","title":"Retrieve trace ID and path"},{"location":"guides/CHAPTER-7/#trace-annotation","text":"You can add a small number of annotations if the event to your function has tracing enabled. Annotated value can be a text string, a JSON object of key-values or a list of text strings. async handleEvent(evt: EventEnvelope) { // business logic to handle the incoming event // ... // annotate the event evt.annotate(\"hello\", \"world\"); Annotations of key-values, if any, will be recorded in the trace and they are not accessible by another function. The annotated key-values will be shown in the trace like this: \"annotations\": {\"hello\": \"world\"} Note : Don't annotate sensitive information or secrets such as PII, PHI, PCI data because the trace is visible in the application log. It may also be forwarded to a centralized telemetry dashboard for visualization and analytics.","title":"Trace annotation"},{"location":"guides/CHAPTER-7/#configuration-api","text":"Your function can access the main application configuration management system like this: const config = AppConfig.getInstance(); // the value can be string, a primitive or a JSON object const value = config.get('my.parameter'); // the value can be read as a string const text = config.getProperty('my.parameter'); The system uses the standard dot-bracket format for a parameter name. e.g. \"hello.world\", \"some.key[2]\" You can also override the main application configuration using the set method. Additional configuration files can be added with the ConfigReader API like this: const myConfig = new ConfigReader(filePath); where filePath can use the classpath:/ or file:/ prefix. The configuration system supports environment variable or reference to the main application configuration using the dollar-bracket syntax ${reference:default_value} . e.g. \"some.key=${MY_ENV_VARIABLE}\", \"some.key=${my.key}\"","title":"Configuration API"},{"location":"guides/CHAPTER-7/#override-configuration-parameters-at-run-time","text":"You can override any configuration parameter from the command line when starting your application. node my-app.js -Dsome.key=some_value -Danother.key=another_value You can point your application to use a different base configuration file like this: node my-app.js -C/opt/config/application.yml The -C command line argument tells the system to use the configuration file in \"/opt/config/application.yml\". Exercise: try this command \"node hello-world.js -Dlog.format=json\" to start the demo app This will tell the Logger system to use JSON format instead of plain text output. The log output may look like this: { \"time\": \"2023-06-10 09:51:20.884\", \"level\": \"INFO\", \"message\": \"Event system started - 9f5c99c4d21a42cfb0115cfbaf533820\", \"module\": \"platform.js:441\" } { \"time\": \"2023-06-10 09:51:21.037\", \"level\": \"INFO\", \"message\": \"REST automation service started on port 8085\", \"module\": \"rest-automation.js:226\" }","title":"Override configuration parameters at run-time"},{"location":"guides/CHAPTER-7/#logger","text":"The system includes a built-in logger that can log in either text or json format. The default log format is \"text\". You can override the value in the \"src/resources/application.yml\" config file. The following example sets the log format to \"json\". log.format: json Alternatively you can also override it at run-time using the \"-D\" parameter like this: node my-app.js -Dlog.format=json The logger supports line-numbering. When you run your executable javascript main program, the line number for each log message is derived from the \".js\" file compiled from the \".ts\" files. If you want to show the line number in the source \".ts\" file for easy debug, you may test your application using \"nodemon\". For simplicity, the logger is implemented without any additional library dependencies.","title":"Logger"},{"location":"guides/CHAPTER-7/#class-scanner","text":"TypeScript supports annotations in class, method and input parameter when experimentalDecorators is turned on in tsconfig.json configuration file. However, annotation features for class, method and input parameter work differently. To support automatic scanning of Composable functions in your source project and compiled code in library packages, the system provides class scanners TypeScriptClassScanner and JavaScriptClassScanner for TypeScript and compiled Javascript code respectively. You can review the \"preloader.js\" build script in the \"examples\" folder to see how the system generates the ComposableLoader (preload.ts) source code in the \"examples/src/preload\" folder. The two class scanners extract annotation's parameters so that your application can do interesting thing based on the annotations. This is similar to the concept of annotated key-values in the Java programming language. For example, you can do something like this to scan the \"preload\" annotated methods of TypeScript files in the source folder: const root = getCurrentFolder(); const src = root + 'src'; const scanner = new TypeScriptClassScanner(src, 'preload'); const result = await scanner.scan(); // extract of the result set is shown below { \"classes\": { \"HelloConcurrent\": \"../services/hello-concurrent.js\", \"HelloWorld\": \"../services/hello-world.js\", }, \"parents\": { \"HelloConcurrent\": { \"extends\": [], \"implements\": [ \"Composable\" ] }, \"HelloWorld\": { \"extends\": [], \"implements\": [ \"Composable\" ] } }, \"parameters\": { \"HelloConcurrent\": [ \"HelloConcurrent.routeName\", \"10\" ], \"HelloWorld\": [ \"HelloWorld.routeName\", \"10\", \"false\" ] }, \"methods\": { \"DemoHealthCheck\": \"initialize\", \"HelloConcurrent\": \"initialize\", \"HelloWorld\": \"initialize\", } } const map = new MultiLevelMap(result); // the MultiLevelMap allows you to retrieve key-value using the dot-bracket format. // e.g. you can validate the method name and parameters // 'initialize' == map.getElement(`methods.${cls}`) && map.exists(`parameters.${cls}`) To scan compiled JavaScript files in library packages, configure application.yaml with the web.component.scan parameter. Use a comma separated list when scanning more than one package. web.component.scan: 'my-package-one, my-package-two' You can then use the JavaScriptClassScanner like this: const scanner = new JavaScriptClassScanner(parent, target, 'preload'); const result = await scanner.scan(); // sample result set below { \"classes\": { \"NoOp\": \"../node_modules/mercury-composable/dist/services/no-op.js\" }, \"parameters\": { \"NoOp\": [ \"'no.op'\", \"10\" ] }, \"methods\": { \"NoOp\": \"initialize\" } } Note : For simplicity, the scanners support method annotations only. Class and input parameter annotations are not handled.","title":"Class scanner"},{"location":"guides/CHAPTER-7/#export-of-composable-functions","text":"You can write composable modules as libraries for other composable application to use. The build script preload.js will use the TypeScriptClassScanner to scan composable functions from the source code of your application and use the JavaScriptClassCanner to scan composable modules from your libraries. When writing a composable library, please ensure that the composable functions in your library must export the composable modules in the index.ts file. Without this setup, the user application may fail to build. For the application that uses your composable libraries, it must include the package name in the web.component.scan parameter in the application.yml configuration file.","title":"Export of composable functions"},{"location":"guides/CHAPTER-7/#minimalist-api-design","text":"For configuration based Event Choreography, please refer to Chapter-4 for more details. You can build powerful composable application without a lot of APIs. \"Less\" is always better in composable methodology. We do not recommend \"event orchestration by code\" because it would lead to tight coupling of software modules.","title":"Minimalist API design"},{"location":"guides/CHAPTER-7/#co-existence-with-other-development-frameworks","text":"Mercury libraries are designed to co-exist with your favorite frameworks and tools. Inside a class implementing a composable function, you can use any coding style and frameworks as you like, including sequential, object-oriented and reactive programming styles. Mercury has a built-in lightweight non-blocking HTTP server based on Express, but you can also use other application server framework with it.","title":"Co-existence with other development frameworks"},{"location":"guides/CHAPTER-7/#template-application-for-quick-start","text":"You can use the composable-example project as a template to start writing your own applications.","title":"Template application for quick start"},{"location":"guides/CHAPTER-7/#source-code-update-frequency","text":"This project is licensed under the Apache 2.0 open sources license. We will update the public codebase after it passes regression tests and meets stability and performance benchmarks in our production systems. Mercury Composable is developed as an engine for you to build the latest cloud native applications. Composable technology is evolving rapidly. We would exercise best effort to keep the essential internals and core APIs stable. Please browse the latest Developer Guide, release notes and Javadoc for any breaking API changes.","title":"Source code update frequency"},{"location":"guides/CHAPTER-7/#technical-support","text":"For enterprise clients, technical support is available. Please contact your Accenture representative for details. Chapter-6 Home Chapter-8 Event over HTTP Table of Contents Custom Flow Adapter","title":"Technical support"},{"location":"guides/CHAPTER-8/","text":"Writing custom Flow Adapters What is a Flow Adapter? Figure 1 - Event Flow Diagram As shown in Figure 1, a transaction flow diagram has a start point and an end point. Each processing step is called a \"task\". Some tasks would do calculation and some tasks would make decision to pass to the next steps. Each task is a composable function that is independent of each other. The system uses a user defined event \"flow\" configuration to connect the tasks together to handle a certain use case, query or transaction. A \"Flow Adapter\" provides a gateway between the external world and the internal world of a flow connecting the tasks. Built-in HTTP Flow Adapter The system has a built-in \"HTTP Flow Adapter\" that converts a REST request into an event that is passed to the first task of a flow. When the flow finishes, the result of the last task will be converted as a REST response to the HTTP Flow Adapter for onward delivery to the caller. Minimalist Kafka Flow Adapter A minimalist Kafka Flow Adapter application is available in Composable-example Please clone the above example repository and review the subproject \"minimalist-kafka-adapter\" for details. The sample Kafka Flow Adapter is written as a demo application for you to visualize how a Kafka Flow Adapter works. You may update the build script, remove the example \"profile\" demo classes and compile the project as a reusable composable library. Configurable setup of consumers and producer You can define the \"topic to task/flow\" mapping in the kafka-adapter.yaml file. For example, the sample kafka-adapter.yaml in the \"tests\" folder tells the system to route inbound messages from the Kafka topic \"hello.world\" to the flow \"get-profile-kafka\" and from the topic \"hello.notice\" to the task \"simple.topic.listener\". Note that a flow has the \"flow://\" protocol prefix. If you need to publish messages to Kafka topics, turn on the producer feature with the \"producer.enabled\" parameter. consumer: - topic: 'hello.world' target: 'flow://get-profile-kafka' group: 'group-100' tracing: true - topic: 'hello.notice' target: 'simple.topic.listener' group: 'group-100' tracing: true producer.enabled: true Kafka emulator The system can emulate a Kafka server in unit tests. Just turn on the \"emulate.kafka\" parameter in application.yml if you need it. # # To use Kafka emulator instead of a Kafka cluster or standalone Kafka server, # set emulate.kafka to true # emulate.kafka: true The Kafka emulator is provided as a convenient feature for unit tests. It is not designed as a replacement of a Kafka broker. The emulation offers a single partition per topic and the offset is monotonically increasing for illustration purpose only. Topics are automatically created. Kafka helper scripts In the \"helpers\" folder, there are three JavaScript programs you may use when trying the minimalist Kafka Flow Adapter as a main application. Please follow the README file in the minimalist-kafka-adapter subproject to test it. Use of worker thread technology The minimalist Kafka Adapter uses the Node's worker thread feature to encapsulate the KafkaJS library. This functional isolation runs the KafkaJS library in a sandbox memory space separated from the Kafka Flow Adapter itself. Since Kafka is resource intensive, this functional isolation is an architectural best practice. The composable function \"kafka.adapter\" encapsulates a Kafka Worker that runs in a separate \"worker thread\". The Kafka consumers and producer are served by two composable functions using the routes kafka.adapter and kafka.notification respectively. Consumer wrapper : import { Composable, EventEnvelope, preload } from 'mercury-composable'; import { KafkaWorker } from '../workers/kafka-worker.js'; export class KafkaAdapter implements Composable { // Composable worker is configured as an interceptor so its responses are ignored. // You can check if the incoming request has a \"replyTo\" and then send a response accordingly. @preload('kafka.adapter', 10, true, true) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // deferred startup until triggered by autostart or your own setup task if ('start' == evt.getHeader('type')) { KafkaWorker.workerBridge(); } // sending the original event to the worker to preserve metadata for tracing and correlation KafkaWorker.sendEventToWorker(evt); return null; } } Producer wrapper : import { Composable, EventEnvelope, AppException, preload, PostOffice } from 'mercury-composable'; /** * This composable function sends outbound messages from the caller to any Kafka topics */ export class KafkaNotification implements Composable { @preload('kafka.notification', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { if (evt.getHeader('topic') && evt.getBody() instanceof Object) { const body = evt.getBody() as object; if ('content' in body) { // convert traceId into x-trace-id header if (evt.getTraceId()) { evt.setHeader('x-trace-id', evt.getTraceId()); } // ask 'kafka.adapter' to send a message to a Kafka topic asynchronously const req = new EventEnvelope().setTo('kafka.adapter').setBody(evt.getBody()).setHeaders(evt.getHeaders()); // use PostOffice without tracking to reduce observability noise when forwarding request to 'kafka.adapter' const po = new PostOffice(); await po.send(req); return {'message': 'Event sent', 'topic': evt.getHeader('topic'), 'time': new Date()}; } } throw new AppException(400, 'Input must contain topic in headers and content in body'); } } Note : The 'kafka.notification' function in turn uses the 'kafka.adapter' function to publish Kafka messages. Sample code The minimalist Kafka Flow Adapter can be used as a template to write your own \"Flow Adapters\". Some flow adapters do not need \"worker thread\" technology for functional isolation if you have control of the underlying dependencies. For those libraries that are complex and you have no control over its source code, \"worker thread\" technology is a good choice. Chapter-7 Home Appendix-I API Overview Table of Contents Appendix-I","title":"Chapter-8"},{"location":"guides/CHAPTER-8/#writing-custom-flow-adapters","text":"","title":"Writing custom Flow Adapters"},{"location":"guides/CHAPTER-8/#what-is-a-flow-adapter","text":"Figure 1 - Event Flow Diagram As shown in Figure 1, a transaction flow diagram has a start point and an end point. Each processing step is called a \"task\". Some tasks would do calculation and some tasks would make decision to pass to the next steps. Each task is a composable function that is independent of each other. The system uses a user defined event \"flow\" configuration to connect the tasks together to handle a certain use case, query or transaction. A \"Flow Adapter\" provides a gateway between the external world and the internal world of a flow connecting the tasks.","title":"What is a Flow Adapter?"},{"location":"guides/CHAPTER-8/#built-in-http-flow-adapter","text":"The system has a built-in \"HTTP Flow Adapter\" that converts a REST request into an event that is passed to the first task of a flow. When the flow finishes, the result of the last task will be converted as a REST response to the HTTP Flow Adapter for onward delivery to the caller.","title":"Built-in HTTP Flow Adapter"},{"location":"guides/CHAPTER-8/#minimalist-kafka-flow-adapter","text":"A minimalist Kafka Flow Adapter application is available in Composable-example Please clone the above example repository and review the subproject \"minimalist-kafka-adapter\" for details. The sample Kafka Flow Adapter is written as a demo application for you to visualize how a Kafka Flow Adapter works. You may update the build script, remove the example \"profile\" demo classes and compile the project as a reusable composable library. Configurable setup of consumers and producer You can define the \"topic to task/flow\" mapping in the kafka-adapter.yaml file. For example, the sample kafka-adapter.yaml in the \"tests\" folder tells the system to route inbound messages from the Kafka topic \"hello.world\" to the flow \"get-profile-kafka\" and from the topic \"hello.notice\" to the task \"simple.topic.listener\". Note that a flow has the \"flow://\" protocol prefix. If you need to publish messages to Kafka topics, turn on the producer feature with the \"producer.enabled\" parameter. consumer: - topic: 'hello.world' target: 'flow://get-profile-kafka' group: 'group-100' tracing: true - topic: 'hello.notice' target: 'simple.topic.listener' group: 'group-100' tracing: true producer.enabled: true Kafka emulator The system can emulate a Kafka server in unit tests. Just turn on the \"emulate.kafka\" parameter in application.yml if you need it. # # To use Kafka emulator instead of a Kafka cluster or standalone Kafka server, # set emulate.kafka to true # emulate.kafka: true The Kafka emulator is provided as a convenient feature for unit tests. It is not designed as a replacement of a Kafka broker. The emulation offers a single partition per topic and the offset is monotonically increasing for illustration purpose only. Topics are automatically created. Kafka helper scripts In the \"helpers\" folder, there are three JavaScript programs you may use when trying the minimalist Kafka Flow Adapter as a main application. Please follow the README file in the minimalist-kafka-adapter subproject to test it.","title":"Minimalist Kafka Flow Adapter"},{"location":"guides/CHAPTER-8/#use-of-worker-thread-technology","text":"The minimalist Kafka Adapter uses the Node's worker thread feature to encapsulate the KafkaJS library. This functional isolation runs the KafkaJS library in a sandbox memory space separated from the Kafka Flow Adapter itself. Since Kafka is resource intensive, this functional isolation is an architectural best practice. The composable function \"kafka.adapter\" encapsulates a Kafka Worker that runs in a separate \"worker thread\". The Kafka consumers and producer are served by two composable functions using the routes kafka.adapter and kafka.notification respectively. Consumer wrapper : import { Composable, EventEnvelope, preload } from 'mercury-composable'; import { KafkaWorker } from '../workers/kafka-worker.js'; export class KafkaAdapter implements Composable { // Composable worker is configured as an interceptor so its responses are ignored. // You can check if the incoming request has a \"replyTo\" and then send a response accordingly. @preload('kafka.adapter', 10, true, true) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // deferred startup until triggered by autostart or your own setup task if ('start' == evt.getHeader('type')) { KafkaWorker.workerBridge(); } // sending the original event to the worker to preserve metadata for tracing and correlation KafkaWorker.sendEventToWorker(evt); return null; } } Producer wrapper : import { Composable, EventEnvelope, AppException, preload, PostOffice } from 'mercury-composable'; /** * This composable function sends outbound messages from the caller to any Kafka topics */ export class KafkaNotification implements Composable { @preload('kafka.notification', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { if (evt.getHeader('topic') && evt.getBody() instanceof Object) { const body = evt.getBody() as object; if ('content' in body) { // convert traceId into x-trace-id header if (evt.getTraceId()) { evt.setHeader('x-trace-id', evt.getTraceId()); } // ask 'kafka.adapter' to send a message to a Kafka topic asynchronously const req = new EventEnvelope().setTo('kafka.adapter').setBody(evt.getBody()).setHeaders(evt.getHeaders()); // use PostOffice without tracking to reduce observability noise when forwarding request to 'kafka.adapter' const po = new PostOffice(); await po.send(req); return {'message': 'Event sent', 'topic': evt.getHeader('topic'), 'time': new Date()}; } } throw new AppException(400, 'Input must contain topic in headers and content in body'); } } Note : The 'kafka.notification' function in turn uses the 'kafka.adapter' function to publish Kafka messages.","title":"Use of worker thread technology"},{"location":"guides/CHAPTER-8/#sample-code","text":"The minimalist Kafka Flow Adapter can be used as a template to write your own \"Flow Adapters\". Some flow adapters do not need \"worker thread\" technology for functional isolation if you have control of the underlying dependencies. For those libraries that are complex and you have no control over its source code, \"worker thread\" technology is a good choice. Chapter-7 Home Appendix-I API Overview Table of Contents Appendix-I","title":"Sample code"},{"location":"guides/METHODOLOGY/","text":"Background The high level concept of composable architecture was advocated by Gartner in 2022. At the platform level, composable architecture refers to loosely coupled platform services, utilities, and business applications. With modular design, you can assemble platform components and applications to create new use cases or to adjust for ever-changing business environment and requirements. Domain driven design (DDD), Command Query Responsibility Segregation (CQRS) and Microservices patterns are the popular tools that architects use to build composable architecture. You may deploy application in container, serverless or other means. At the application level, composable application means that an application is assembled from modular software components or functions that are self-contained and pluggable. You can mix-n-match functions to form new applications. You can retire outdated functions without adverse side effect to a production system. Multiple versions of a function can exist, and you can decide how to route user requests to different versions of a function. Applications would be easier to design, develop, maintain, deploy, and scale. In 2023, Accenture extended its event-driven development framework, codename \"Mercury\", to become the first implementation of a Composable framework to realize the goal of composable architecture and application design. Before we take a deep dive of the mercury-composable framework, let's review Composable Methodology first. Methodology The Composable Methodology takes a different approach in software development that empowers and aligns product owners, business analysts, technology architects and software engineers. Historically, there is a disconnect between product features and application design because product is user and business focused but application design is technology oriented. Product owners have almost no direct control over the quality of the user applications that are supposed to address business requirements. There is a communication barrier between the two domains in most projects. It requires a lot of iterations to get things right. The lack of direct connection between business domain and technical domain also leads to higher technical debts that are not just limited to imperfect coding. Composable methodology addresses this fundamental issue by connecting the two domains seamlessly. Before developers write a line of code, we start a project from product design. The output from a product design is a business transaction event flow diagram, ideally from a tactical Event Storming workshop or a more relaxed white boarding discussion among product owners, business domain experts and technology architects. Figure 1 - Event Flow Diagram As shown in Figure 1, a transaction flow diagram has a start point and an end point. Each processing step is called a \"task\". Some tasks would do calculation and some tasks would make decision to pass to the next steps. Note : Task types include decision, response, end, sequential, parallel, fork-n-join, sink and pipeline. The product team designs a transaction flow to address a business use case and presents it as a diagram. Naturally, this is how a human designer thinks. We don't want to prematurely adjust the business requirements to any infrastructural limitation. Events at the center of the modern enterprise In Domain Driven Design, it is well accepted that business transactions and their intermediate objects can be represented as \"events\". An event is a holder of a business object that may be a transaction request, a transaction response, a data object or any intermediate data representation. Generally, a data object is a structure of key-values. For simplicity, we will call a transaction flow diagram as \"Event Flow Diagram\" from now on. First principle - \"input-process-output\" The first principle of composable application design is that each task (also called \"function\") is self-contained and its input and output are immutable. Self-containment means that the task or function does not need to know the world outside its functional scope, thus making the function as pure as \"input-process-output\". i.e. given some input, the task or function will carry out some business logic to generate some output according to functional specification. Immutability is an important guardrail for clean code design. A function cannot change the business object outside its functional scope, thus eliminating the chance of unintended side effects of data interference. Note : In composable design, task and function can be used interchangeably. In an event flow diagram, we call each step a \"task\". In application design, we call each step a \"function\". Second principle - zero to one dependency The second principle is that each function should have zero dependency with other user functions. To connect to the world outside its functional scope, a function may have one and only one dependency with a platform or infrastructure component function. In composable design, a library can be packaged as a reusable composable function. A function consumes a platform component or library by sending an event to the component instead of tight coupling with a direct method call. Decoupling between functions and components promotes non-blocking, asynchronous and parallel operation that yields higher performance and throughput. Third principle - platform abstraction Since composable functions are self-contained and independent, they can be reused and repackaged into different applications to serve different purposes. Therefore, composable functions are, by definition, plug-n-play. The platform and infrastructure layers are encapsulated as adapters , gateways and wrappers . Figure 1 illustrates this architectural principle by connecting an event flow to the user through an event flow adapter. For example, a \"HTTP Flow Adapter\" serves both inbound request and outbound response. A \"Kafka Flow Adapter\" uses one topic for inbound and another topic for outbound events. Fourth principle - event choreography Without direct coupling, a composable development framework must support \"Event Choreography\" so that we can connect the various user composable functions, platform components and libraries together and route the events according to an event flow diagram for each use case. Application development Once we have drawn the event flow diagrams for different use cases, we can create user stories for each composable function and assign the development of each function to a developer or a pair of developers if using pair-programming. Composable methodology embraces \"Test Driven Development (TDD)\". Since each function is self-contained, it is TDD friendly because the developer does not need to deal with external dependencies for unit tests. It is the first principle of \"input-process-output\". This methodology allows us to scale our application development resources much better than traditional approach because developers do not need to know details of dependencies, thus avoiding hard code and reducing technical debts. For integration tests, it is easy to mock platform, infrastructure, database and HTTP resources by assigning mock functions to some tasks in an event flow. This greatly simplifies integration mocking. Seamless transition from product to application design Event flow diagram describes the product and each composable function contains the specific processing logic. Data attributes in an event flowing from one function to another provide the clarity for product designers and application engineers to communicate effectively. Application packaging and deployment Composable functions are independent and isolated. We can package related functions for an event flow in a single executable. A senior developer or architect can decide how to package related functions to reduce memory footprint and to scale efficiently and horizontally. Smaller memory footprint By design, composable functions are granular in nature. They are registered to an \"event loop\" on-demand. i.e. a function is executed when an event arrives. Without multiple levels of tight coupling, each piece of user code consumes memory efficiently. Memory is released to the system as soon as the function finishes execution. Composable framework To realize the composable design principles, a low-latency in-memory event system is available at the core of the Mercury-Composable framework. Figure 2 - Composable Framework As illustrated in Figure 2, event choreography for an event flow is described as an \"Event Flow Configuration\". An \"Event Manager\" is implemented as part of a low-latency in-memory event system. Composable functions (aka \"task\" in an event flow) ride on the in-memory event system so that the event manager can invoke them by sending events. Since composable functions are self-contained, the event manager performs input/output data mapping to and from the functional scope of a composable function. This flexible data mapping allows developers to write more generic code that responds to different input dataset, thus promoting software reusability. An in-memory state machine will be created for each execution of a transaction or \"event flow\". It is used for holding transaction \"states\" and temporary data objects. Sometimes a transaction may be suspended and restarted with different event flows. The in-memory state machine can be extended to an external datastore so that transaction states can be monitored and recovered operationally. Note : In Java, we combine the use of native Java 21 virtual thread management with Eclipse Vertx event bus. In Node.js, we use the EventEmitter event bus from the standard library. Event Envelope We use a standard \"Event Envelope\" to transport an event over the in-memory event bus. An event envelope contains three parts: (1) body, (2) headers and (3) metadata. Event body is used to transport a business object. In Java, it may be a PoJo or a HashMap. In node.js, it is a JSON object. Headers can be used to carry additional parameters to tell the user composable function what to do. Examples for metadata include performance metrics, status, exception, optional tracing information, and correlation ID. Language neutral The event flow configuration syntax and event envelope serialization scheme are standardized for polyglot deployment. The event flow configuration rides on YAML and event envelope serialization uses binary JSON (\"MsgPack\"). For higher serialization efficiency, the intermediate format is using \"key-value\" maps. JSON string is only used at the inbound and outbound flow adapters. Functional isolation Each function must implement the Composable interface. In Java, it is called \"TypedLambdaFunction\". In Node.js, it is called \"Composable\". The composable interface enforces a single \"handleEvent\" method where the function can access the event's headers and body. By design, each composable function is invoked by events. It is running parallel with other user functions. This non-blocking asynchronous execution architecture delivers higher performance than traditional coding approach with direct method invocation. While each function is executed in an event-driven and reactive manner, the user application code inside each composable function may use any coding style including object-oriented design, functional or reactive. Functional isolation means that you can use any open source or commercial library or framework in your user function without concerns about thread safety or unintended side effect. In Java, a composable function may look like this: @PreLoad(route = \"my.first.function\", instances = 10) public class MyFirstFunction implements TypedLambdaFunction<MyPoJo, AnotherPoJo> { @Override public AnotherPojo handleEvent(Map<String, String> headers, MyPoJo input, int instance) { // your business logic here return result; } } In Node.js, it may look like this: export class MyFirstFunction implements Composable { @preload('my.first.function', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // your business logic here return result; } A composable function is declared with a \"route name\" using the \"preload\" annotation. While Java and Node.js have different syntax for annotation, they use a similar declarative approach. Note : the \"instance\" count for each composable function controls execution concurrency in a single application. It can be used with horizontal scaling to optimize use of computing resources. Home Chapter-1 Table of Contents Introduction","title":"Methodology"},{"location":"guides/METHODOLOGY/#background","text":"The high level concept of composable architecture was advocated by Gartner in 2022. At the platform level, composable architecture refers to loosely coupled platform services, utilities, and business applications. With modular design, you can assemble platform components and applications to create new use cases or to adjust for ever-changing business environment and requirements. Domain driven design (DDD), Command Query Responsibility Segregation (CQRS) and Microservices patterns are the popular tools that architects use to build composable architecture. You may deploy application in container, serverless or other means. At the application level, composable application means that an application is assembled from modular software components or functions that are self-contained and pluggable. You can mix-n-match functions to form new applications. You can retire outdated functions without adverse side effect to a production system. Multiple versions of a function can exist, and you can decide how to route user requests to different versions of a function. Applications would be easier to design, develop, maintain, deploy, and scale. In 2023, Accenture extended its event-driven development framework, codename \"Mercury\", to become the first implementation of a Composable framework to realize the goal of composable architecture and application design. Before we take a deep dive of the mercury-composable framework, let's review Composable Methodology first.","title":"Background"},{"location":"guides/METHODOLOGY/#methodology","text":"The Composable Methodology takes a different approach in software development that empowers and aligns product owners, business analysts, technology architects and software engineers. Historically, there is a disconnect between product features and application design because product is user and business focused but application design is technology oriented. Product owners have almost no direct control over the quality of the user applications that are supposed to address business requirements. There is a communication barrier between the two domains in most projects. It requires a lot of iterations to get things right. The lack of direct connection between business domain and technical domain also leads to higher technical debts that are not just limited to imperfect coding. Composable methodology addresses this fundamental issue by connecting the two domains seamlessly. Before developers write a line of code, we start a project from product design. The output from a product design is a business transaction event flow diagram, ideally from a tactical Event Storming workshop or a more relaxed white boarding discussion among product owners, business domain experts and technology architects. Figure 1 - Event Flow Diagram As shown in Figure 1, a transaction flow diagram has a start point and an end point. Each processing step is called a \"task\". Some tasks would do calculation and some tasks would make decision to pass to the next steps. Note : Task types include decision, response, end, sequential, parallel, fork-n-join, sink and pipeline. The product team designs a transaction flow to address a business use case and presents it as a diagram. Naturally, this is how a human designer thinks. We don't want to prematurely adjust the business requirements to any infrastructural limitation.","title":"Methodology"},{"location":"guides/METHODOLOGY/#events-at-the-center-of-the-modern-enterprise","text":"In Domain Driven Design, it is well accepted that business transactions and their intermediate objects can be represented as \"events\". An event is a holder of a business object that may be a transaction request, a transaction response, a data object or any intermediate data representation. Generally, a data object is a structure of key-values. For simplicity, we will call a transaction flow diagram as \"Event Flow Diagram\" from now on.","title":"Events at the center of the modern enterprise"},{"location":"guides/METHODOLOGY/#first-principle-input-process-output","text":"The first principle of composable application design is that each task (also called \"function\") is self-contained and its input and output are immutable. Self-containment means that the task or function does not need to know the world outside its functional scope, thus making the function as pure as \"input-process-output\". i.e. given some input, the task or function will carry out some business logic to generate some output according to functional specification. Immutability is an important guardrail for clean code design. A function cannot change the business object outside its functional scope, thus eliminating the chance of unintended side effects of data interference. Note : In composable design, task and function can be used interchangeably. In an event flow diagram, we call each step a \"task\". In application design, we call each step a \"function\".","title":"First principle - \"input-process-output\""},{"location":"guides/METHODOLOGY/#second-principle-zero-to-one-dependency","text":"The second principle is that each function should have zero dependency with other user functions. To connect to the world outside its functional scope, a function may have one and only one dependency with a platform or infrastructure component function. In composable design, a library can be packaged as a reusable composable function. A function consumes a platform component or library by sending an event to the component instead of tight coupling with a direct method call. Decoupling between functions and components promotes non-blocking, asynchronous and parallel operation that yields higher performance and throughput.","title":"Second principle - zero to one dependency"},{"location":"guides/METHODOLOGY/#third-principle-platform-abstraction","text":"Since composable functions are self-contained and independent, they can be reused and repackaged into different applications to serve different purposes. Therefore, composable functions are, by definition, plug-n-play. The platform and infrastructure layers are encapsulated as adapters , gateways and wrappers . Figure 1 illustrates this architectural principle by connecting an event flow to the user through an event flow adapter. For example, a \"HTTP Flow Adapter\" serves both inbound request and outbound response. A \"Kafka Flow Adapter\" uses one topic for inbound and another topic for outbound events.","title":"Third principle - platform abstraction"},{"location":"guides/METHODOLOGY/#fourth-principle-event-choreography","text":"Without direct coupling, a composable development framework must support \"Event Choreography\" so that we can connect the various user composable functions, platform components and libraries together and route the events according to an event flow diagram for each use case.","title":"Fourth principle - event choreography"},{"location":"guides/METHODOLOGY/#application-development","text":"Once we have drawn the event flow diagrams for different use cases, we can create user stories for each composable function and assign the development of each function to a developer or a pair of developers if using pair-programming. Composable methodology embraces \"Test Driven Development (TDD)\". Since each function is self-contained, it is TDD friendly because the developer does not need to deal with external dependencies for unit tests. It is the first principle of \"input-process-output\". This methodology allows us to scale our application development resources much better than traditional approach because developers do not need to know details of dependencies, thus avoiding hard code and reducing technical debts. For integration tests, it is easy to mock platform, infrastructure, database and HTTP resources by assigning mock functions to some tasks in an event flow. This greatly simplifies integration mocking.","title":"Application development"},{"location":"guides/METHODOLOGY/#seamless-transition-from-product-to-application-design","text":"Event flow diagram describes the product and each composable function contains the specific processing logic. Data attributes in an event flowing from one function to another provide the clarity for product designers and application engineers to communicate effectively.","title":"Seamless transition from product to application design"},{"location":"guides/METHODOLOGY/#application-packaging-and-deployment","text":"Composable functions are independent and isolated. We can package related functions for an event flow in a single executable. A senior developer or architect can decide how to package related functions to reduce memory footprint and to scale efficiently and horizontally.","title":"Application packaging and deployment"},{"location":"guides/METHODOLOGY/#smaller-memory-footprint","text":"By design, composable functions are granular in nature. They are registered to an \"event loop\" on-demand. i.e. a function is executed when an event arrives. Without multiple levels of tight coupling, each piece of user code consumes memory efficiently. Memory is released to the system as soon as the function finishes execution.","title":"Smaller memory footprint"},{"location":"guides/METHODOLOGY/#composable-framework","text":"To realize the composable design principles, a low-latency in-memory event system is available at the core of the Mercury-Composable framework. Figure 2 - Composable Framework As illustrated in Figure 2, event choreography for an event flow is described as an \"Event Flow Configuration\". An \"Event Manager\" is implemented as part of a low-latency in-memory event system. Composable functions (aka \"task\" in an event flow) ride on the in-memory event system so that the event manager can invoke them by sending events. Since composable functions are self-contained, the event manager performs input/output data mapping to and from the functional scope of a composable function. This flexible data mapping allows developers to write more generic code that responds to different input dataset, thus promoting software reusability. An in-memory state machine will be created for each execution of a transaction or \"event flow\". It is used for holding transaction \"states\" and temporary data objects. Sometimes a transaction may be suspended and restarted with different event flows. The in-memory state machine can be extended to an external datastore so that transaction states can be monitored and recovered operationally. Note : In Java, we combine the use of native Java 21 virtual thread management with Eclipse Vertx event bus. In Node.js, we use the EventEmitter event bus from the standard library.","title":"Composable framework"},{"location":"guides/METHODOLOGY/#event-envelope","text":"We use a standard \"Event Envelope\" to transport an event over the in-memory event bus. An event envelope contains three parts: (1) body, (2) headers and (3) metadata. Event body is used to transport a business object. In Java, it may be a PoJo or a HashMap. In node.js, it is a JSON object. Headers can be used to carry additional parameters to tell the user composable function what to do. Examples for metadata include performance metrics, status, exception, optional tracing information, and correlation ID.","title":"Event Envelope"},{"location":"guides/METHODOLOGY/#language-neutral","text":"The event flow configuration syntax and event envelope serialization scheme are standardized for polyglot deployment. The event flow configuration rides on YAML and event envelope serialization uses binary JSON (\"MsgPack\"). For higher serialization efficiency, the intermediate format is using \"key-value\" maps. JSON string is only used at the inbound and outbound flow adapters.","title":"Language neutral"},{"location":"guides/METHODOLOGY/#functional-isolation","text":"Each function must implement the Composable interface. In Java, it is called \"TypedLambdaFunction\". In Node.js, it is called \"Composable\". The composable interface enforces a single \"handleEvent\" method where the function can access the event's headers and body. By design, each composable function is invoked by events. It is running parallel with other user functions. This non-blocking asynchronous execution architecture delivers higher performance than traditional coding approach with direct method invocation. While each function is executed in an event-driven and reactive manner, the user application code inside each composable function may use any coding style including object-oriented design, functional or reactive. Functional isolation means that you can use any open source or commercial library or framework in your user function without concerns about thread safety or unintended side effect. In Java, a composable function may look like this: @PreLoad(route = \"my.first.function\", instances = 10) public class MyFirstFunction implements TypedLambdaFunction<MyPoJo, AnotherPoJo> { @Override public AnotherPojo handleEvent(Map<String, String> headers, MyPoJo input, int instance) { // your business logic here return result; } } In Node.js, it may look like this: export class MyFirstFunction implements Composable { @preload('my.first.function', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // your business logic here return result; } A composable function is declared with a \"route name\" using the \"preload\" annotation. While Java and Node.js have different syntax for annotation, they use a similar declarative approach. Note : the \"instance\" count for each composable function controls execution concurrency in a single application. It can be used with horizontal scaling to optimize use of computing resources. Home Chapter-1 Table of Contents Introduction","title":"Functional isolation"},{"location":"guides/TABLE-OF-CONTENTS/","text":"Developer's Guide Mercury version 3 is a toolkit for writing composable applications. Methodology Chapter 1 - Getting Started Chapter 2 - Function Execution Strategies Chapter 3 - REST Automation Chapter 4 - Event Script Syntax Chapter 5 - Build, Test and Deploy Chapter 6 - Event over HTTP Chapter 7 - API Overview Appendix I - application configuration Appendix II - Actuators, HTTP client and More","title":"Contents"},{"location":"guides/TABLE-OF-CONTENTS/#developers-guide","text":"Mercury version 3 is a toolkit for writing composable applications. Methodology Chapter 1 - Getting Started Chapter 2 - Function Execution Strategies Chapter 3 - REST Automation Chapter 4 - Event Script Syntax Chapter 5 - Build, Test and Deploy Chapter 6 - Event over HTTP Chapter 7 - API Overview Appendix I - application configuration Appendix II - Actuators, HTTP client and More","title":"Developer's Guide"}]}